{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hadoop_install.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harnalashok/hadoop/blob/main/hadoop_spark_install_on_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4q64Z8IYb8fr"
      },
      "source": [
        "# Last amended: 30th March, 2021\n",
        "# Myfolder: github/hadoop\n",
        "# Objective:\n",
        "#            i)  Install hadoop on colab\n",
        "#                (current version is 3.2.2)\n",
        "#            ii) Experiments with hadoop\n",
        "#           iii) Install spark on colab\n",
        "#            iv) Access hadoop file from spark\n",
        "#             v) Install koalas on colab\n",
        "#\n",
        "#\n",
        "# Java 8 install: https://stackoverflow.com/a/58191107\n",
        "# Hadoop install: https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html\n",
        "# Spark install:  https://stackoverflow.com/a/64183749\n",
        "#                 https://www.analyticsvidhya.com/blog/2020/11/a-must-read-guide-on-how-to-work-with-pyspark-on-google-colab-for-data-scientists/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDmuRsOCchey"
      },
      "source": [
        "## Install hadoop\n",
        "If it takes too long, it means, it is awaiting input from you regarding overwriting ssh keys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-v7l17LclsR"
      },
      "source": [
        "### Define functions\n",
        "No downloads. Just function definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBVUrlkidyaE"
      },
      "source": [
        "# 1.0 How to set environment variable\n",
        "import os  \n",
        "import time  "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfFBQ0FLcolo"
      },
      "source": [
        "#### ssh_install()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgS9HNCyR7C0"
      },
      "source": [
        "# 2.0 Function to install ssh client and sshd (Server)\n",
        "def ssh_install():\n",
        "  print(\"\\n--1. Download and install ssh server----\\n\")\n",
        "  ! sudo apt-get remove openssh-client openssh-server\n",
        "  ! sudo apt install openssh-client openssh-server\n",
        "  \n",
        "  print(\"\\n--2. Restart ssh server----\\n\")\n",
        "  ! service ssh restart"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJNKSScPcsDS"
      },
      "source": [
        "#### Java install"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsFu84PSR9jR"
      },
      "source": [
        "# 3.0 Function to download and install java 8\n",
        "def install_java():\n",
        "  ! rm -rf /usr/java\n",
        "\n",
        "  print(\"\\n--Download and install Java 8----\\n\")\n",
        "  !apt-get install -y openjdk-8-jdk-headless -qq > /dev/null        # install openjdk\n",
        "  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"     # set environment variable\n",
        "\n",
        "  !update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java\n",
        "  !update-alternatives --set javac /usr/lib/jvm/java-8-openjdk-amd64/bin/javac\n",
        "  \n",
        "  !mkdir -p /usr/java\n",
        "  ! ln -s \"/usr/lib/jvm/java-8-openjdk-amd64\"  \"/usr/java\"\n",
        "  ! mv \"/usr/java/java-8-openjdk-amd64\"  \"/usr/java/latest\"\n",
        "  \n",
        "  !java -version       #check java version\n",
        "  !javac -version"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuZPLoqrcvtp"
      },
      "source": [
        "#### hadoop install"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dg2fSk_ylMbr"
      },
      "source": [
        "# 4.0 Function to download and install hadoop\n",
        "def hadoop_install():\n",
        "  print(\"\\n--5. Download hadoop tar.gz----\\n\")\n",
        "  ! wget -c https://mirrors.estointernet.in/apache/hadoop/common/hadoop-3.2.2/hadoop-3.2.2.tar.gz\n",
        "\n",
        "  print(\"\\n--6. Transfer downloaded content and unzip tar.gz----\\n\")\n",
        "  !  mv /content/hadoop*   /opt/\n",
        "  ! tar -xzf /opt/hadoop-3.2.2.tar.gz  --directory /opt/\n",
        "\n",
        "  print(\"\\n--7. Create hadoop folder----\\n\")\n",
        "  ! rm -r /app/hadoop/tmp\n",
        "  ! mkdir  -p   /app/hadoop/tmp\n",
        "  \n",
        "  print(\"\\n--8. Check folder for files----\\n\")\n",
        "  ! ls -la /opt"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iPTRV2qc0Pg"
      },
      "source": [
        "#### hadoop config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SChb4dXsurlC"
      },
      "source": [
        "# 5.0 Function for setting hadoop configuration\n",
        "def hadoop_config():\n",
        "  print(\"\\n--Begin Configuring hadoop---\\n\")\n",
        "  print(\"\\n=============================\\n\")\n",
        "  print(\"\\n--9. core-site.xml----\\n\")\n",
        "  ! cat  /opt/hadoop-3.2.2/etc/hadoop/core-site.xml\n",
        "\n",
        "  print(\"\\n--10. Amend core-site.xml----\\n\")\n",
        "  !  echo  '<?xml version=\"1.0\" encoding=\"UTF-8\"?>' >  /opt/hadoop-3.2.2/etc/hadoop/core-site.xml\n",
        "  !  echo  '<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>' >>  /opt/hadoop-3.2.2/etc/hadoop/core-site.xml\n",
        "  !  echo  ' <configuration>' >>  /opt/hadoop-3.2.2/etc/hadoop/core-site.xml\n",
        "  !  echo  '    <property>' >>  /opt/hadoop-3.2.2/etc/hadoop/core-site.xml\n",
        "  !  echo  '        <name>fs.defaultFS</name>' >>  /opt/hadoop-3.2.2/etc/hadoop/core-site.xml\n",
        "  !  echo  '        <value>hdfs://localhost:9000</value>' >>  /opt/hadoop-3.2.2/etc/hadoop/core-site.xml\n",
        "  !  echo  '    </property>' >>  /opt/hadoop-3.2.2/etc/hadoop/core-site.xml\n",
        "  !  echo  '    <property>' >>  /opt/hadoop-3.2.2/etc/hadoop/core-site.xml\n",
        "  !  echo  '          <name>hadoop.tmp.dir</name>' >>  /opt/hadoop-3.2.2/etc/hadoop/core-site.xml\n",
        "  !  echo  '          <value>/app/hadoop/tmp</value>' >>  /opt/hadoop-3.2.2/etc/hadoop/core-site.xml\n",
        "  !  echo  '       <description>A base for other temporary directories.</description>' >>  /opt/hadoop-3.2.2/etc/hadoop/core-site.xml\n",
        "  !  echo  '     </property>' >>  /opt/hadoop-3.2.2/etc/hadoop/core-site.xml\n",
        "  # Added following regarding safemode from here:\n",
        "  # https://stackoverflow.com/a/33800253\n",
        "  !  echo  '     <property>'    >> /opt/hadoop-3.2.2/etc/hadoop/core-site.xml\n",
        "  !  echo  '      <name>dfs.safemode.threshold.pct</name>'  >> /opt/hadoop-3.2.2/etc/hadoop/core-site.xml\n",
        "  !  echo  '    <value>0</value>'  >> /opt/hadoop-3.2.2/etc/hadoop/core-site.xml\n",
        "  !  echo  '  </property>'  >> /opt/hadoop-3.2.2/etc/hadoop/core-site.xml\n",
        "  !  echo  '  </configuration>' >>  /opt/hadoop-3.2.2/etc/hadoop/core-site.xml\n",
        "\n",
        "  print(\"\\n--11. Amended core-site.xml----\\n\")\n",
        "  ! cat  /opt/hadoop-3.2.2/etc/hadoop/core-site.xml\n",
        "\n",
        "  print(\"\\n--12. yarn-site.xml----\\n\")\n",
        "  !cat /opt/hadoop-3.2.2/etc/hadoop/yarn-site.xml\n",
        "\n",
        "  !echo '<?xml version=\"1.0\" encoding=\"UTF-8\"?>' > /opt/hadoop-3.2.2/etc/hadoop/yarn-site.xml\n",
        "  !echo '<configuration>' >> /opt/hadoop-3.2.2/etc/hadoop/yarn-site.xml\n",
        "  !echo '    <property>' >> /opt/hadoop-3.2.2/etc/hadoop/yarn-site.xml\n",
        "  !echo '        <name>yarn.nodemanager.aux-services</name>' >> /opt/hadoop-3.2.2/etc/hadoop/yarn-site.xml\n",
        "  !echo '        <value>mapreduce_shuffle</value>' >> /opt/hadoop-3.2.2/etc/hadoop/yarn-site.xml\n",
        "  !echo '    </property>' >> /opt/hadoop-3.2.2/etc/hadoop/yarn-site.xml\n",
        "  !echo '    <property>'  >> /opt/hadoop-3.2.2/etc/hadoop/yarn-site.xml\n",
        "  !echo '       <name>yarn.nodemanager.vmem-check-enabled</name>'  >> /opt/hadoop-3.2.2/etc/hadoop/yarn-site.xml\n",
        "  !echo '       <value>false</value>'  >> /opt/hadoop-3.2.2/etc/hadoop/yarn-site.xml\n",
        "  !echo '    </property>'  >> /opt/hadoop-3.2.2/etc/hadoop/yarn-site.xml\n",
        "  !echo ' </configuration>'  >> /opt/hadoop-3.2.2/etc/hadoop/yarn-site.xml\n",
        "  \n",
        "  print(\"\\n--13. Amended yarn-site.xml----\\n\")\n",
        "  !cat /opt/hadoop-3.2.2/etc/hadoop/yarn-site.xml\n",
        "\n",
        "  print(\"\\n--14. mapred-site.xml----\\n\")\n",
        "  !cat  /opt/hadoop-3.2.2/etc/hadoop/mapred-site.xml\n",
        "\n",
        "  print(\"\\n--15. Amend mapred-site.xml----\\n\")\n",
        "  !echo '<?xml version=\"1.0\"?>'  > /opt/hadoop-3.2.2/etc/hadoop/mapred-site.xml\n",
        "  !echo '<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>'  >> /opt/hadoop-3.2.2/etc/hadoop/mapred-site.xml\n",
        "  !echo '<configuration>'  >> /opt/hadoop-3.2.2/etc/hadoop/mapred-site.xml\n",
        "  !echo '    <property>'  >> /opt/hadoop-3.2.2/etc/hadoop/mapred-site.xml\n",
        "  !echo '       <name>mapreduce.framework.name</name>'  >> /opt/hadoop-3.2.2/etc/hadoop/mapred-site.xml\n",
        "  !echo '        <value>yarn</value>'  >> /opt/hadoop-3.2.2/etc/hadoop/mapred-site.xml\n",
        "  !echo '    </property>'  >> /opt/hadoop-3.2.2/etc/hadoop/mapred-site.xml\n",
        "  !echo '    <property>'  >> /opt/hadoop-3.2.2/etc/hadoop/mapred-site.xml\n",
        "  !echo '        <name>yarn.app.mapreduce.am.env</name>'  >> /opt/hadoop-3.2.2/etc/hadoop/mapred-site.xml\n",
        "  !echo '        <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>'  >> /opt/hadoop-3.2.2/etc/hadoop/mapred-site.xml\n",
        "  !echo '    </property>'  >> /opt/hadoop-3.2.2/etc/hadoop/mapred-site.xml\n",
        "  !echo '    <property>'  >> /opt/hadoop-3.2.2/etc/hadoop/mapred-site.xml\n",
        "  !echo '       <name>mapreduce.map.env</name>'  >> /opt/hadoop-3.2.2/etc/hadoop/mapred-site.xml\n",
        "  !echo '       <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>'  >> /opt/hadoop-3.2.2/etc/hadoop/mapred-site.xml\n",
        "  !echo '    </property>'  >> /opt/hadoop-3.2.2/etc/hadoop/mapred-site.xml\n",
        "  !echo '    <property>'  >> /opt/hadoop-3.2.2/etc/hadoop/mapred-site.xml\n",
        "  !echo '      <name>mapreduce.reduce.env</name>'  >> /opt/hadoop-3.2.2/etc/hadoop/mapred-site.xml\n",
        "  !echo '      <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>'  >> /opt/hadoop-3.2.2/etc/hadoop/mapred-site.xml\n",
        "  !echo '   </property>'  >> /opt/hadoop-3.2.2/etc/hadoop/mapred-site.xml\n",
        "  !echo '</configuration>'  >> /opt/hadoop-3.2.2/etc/hadoop/mapred-site.xml\n",
        "\n",
        "  print(\"\\n--16, Amended mapred-site.xml----\\n\")\n",
        "  !cat  /opt/hadoop-3.2.2/etc/hadoop/mapred-site.xml\n",
        "\n",
        "  print(\"\\n---17. hdfs-site.xml----\\n\")\n",
        "  !cat  /opt/hadoop-3.2.2/etc/hadoop/hdfs-site.xml\n",
        "  \n",
        "  print(\"\\n---18. Amend hdfs-site.xml----\\n\")\n",
        "  !echo  '<?xml version=\"1.0\" encoding=\"UTF-8\"?> '   > /opt/hadoop-3.2.2/etc/hadoop/hdfs-site.xml\n",
        "  !echo  '<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>' >> /opt/hadoop-3.2.2/etc/hadoop/hdfs-site.xml\n",
        "  !echo  '<configuration>'  >> /opt/hadoop-3.2.2/etc/hadoop/hdfs-site.xml\n",
        "  !echo  '    <property>'  >> /opt/hadoop-3.2.2/etc/hadoop/hdfs-site.xml\n",
        "  !echo  '        <name>dfs.replication</name>'  >> /opt/hadoop-3.2.2/etc/hadoop/hdfs-site.xml\n",
        "  !echo  '        <value>1</value>'  >> /opt/hadoop-3.2.2/etc/hadoop/hdfs-site.xml\n",
        "  !echo  '    </property>'  >> /opt/hadoop-3.2.2/etc/hadoop/hdfs-site.xml\n",
        "  !echo  '   <property>'   >> /opt/hadoop-3.2.2/etc/hadoop/hdfs-site.xml\n",
        "  !echo  '        <name>dfs.block.size</name>'  >> /opt/hadoop-3.2.2/etc/hadoop/hdfs-site.xml\n",
        "  !echo  '        <value>16777216</value>'  >> /opt/hadoop-3.2.2/etc/hadoop/hdfs-site.xml\n",
        "  !echo  '        <description>Block size</description>'  >> /opt/hadoop-3.2.2/etc/hadoop/hdfs-site.xml\n",
        "  !echo  '  </property>'  >> /opt/hadoop-3.2.2/etc/hadoop/hdfs-site.xml\n",
        "  !echo  '</configuration>'  >> /opt/hadoop-3.2.2/etc/hadoop/hdfs-site.xml\n",
        "\n",
        "  print(\"\\n---19. Amended hdfs-site.xml----\\n\")\n",
        "  !cat  /opt/hadoop-3.2.2/etc/hadoop/hdfs-site.xml\n",
        "\n",
        "  print(\"\\n---20. hadoop-env.sh----\\n\")\n",
        "  # https://stackoverflow.com/a/53140448\n",
        "  !cat /opt/hadoop-3.2.2/etc/hadoop/hadoop-env.sh\n",
        "  ! echo 'export JAVA_HOME=\"/usr/lib/jvm/java-8-openjdk-amd64\"' >> /opt/hadoop-3.2.2/etc/hadoop/hadoop-env.sh\n",
        "  ! echo 'export HDFS_NAMENODE_USER=\"root\"'  >> /opt/hadoop-3.2.2/etc/hadoop/hadoop-env.sh\n",
        "  ! echo 'export HDFS_DATANODE_USER=\"root\"'  >> /opt/hadoop-3.2.2/etc/hadoop/hadoop-env.sh\n",
        "  ! echo 'export HDFS_SECONDARYNAMENODE_USER=\"root\"'  >> /opt/hadoop-3.2.2/etc/hadoop/hadoop-env.sh\n",
        "  ! echo 'export YARN_RESOURCEMANAGER_USER=\"root\"'  >> /opt/hadoop-3.2.2/etc/hadoop/hadoop-env.sh\n",
        "  ! echo 'export YARN_NODEMANAGER_USER=\"root\"'  >> /opt/hadoop-3.2.2/etc/hadoop/hadoop-env.sh\n",
        "  \n",
        "  print(\"\\n---21. Amended hadoop-env.sh----\\n\")\n",
        "  !cat /opt/hadoop-3.2.2/etc/hadoop/hadoop-env.sh\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y159J8TDc3wS"
      },
      "source": [
        "#### ssh keys"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOFbfw7n0Pps"
      },
      "source": [
        "# 6.0 Function tp setup ssh passphrase\n",
        "def set_keys():\n",
        "  print(\"\\n---22. Generate SSH keys----\\n\")\n",
        "  ! cd ~ ; pwd \n",
        "  ! cd ~ ; ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa\n",
        "  ! cd ~ ; cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\n",
        "  ! cd ~ ; chmod 0600 ~/.ssh/authorized_keys\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M2kWg3dc6FT"
      },
      "source": [
        "#### Set environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRSn9XAV4rsR"
      },
      "source": [
        "# 7.0 Function to set up environmental variables\n",
        "def set_env():\n",
        "  print(\"\\n---23. Set Environment variables----\\n\")\n",
        "  # 'export' command does not work in colab\n",
        "  # https://stackoverflow.com/a/57240319\n",
        "  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"     #set environment variable\n",
        "  os.environ[\"JRE_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64/jre\"   \n",
        "  os.environ[\"HADOOP_HOME\"] = \"/opt/hadoop-3.2.2\"\n",
        "  os.environ[\"HADOOP_CONF_DIR\"] = \"/opt/hadoop-3.2.2/etc/hadoop\" \n",
        "  os.environ[\"LD_LIBRARY_PATH\"] += \":/opt/hadoop-3.2.2/lib/native\"\n",
        "  os.environ[\"PATH\"] += \":/opt/hadoop-3.2.2/bin:/opt/hadoop-3.2.2/sbin\""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WII-UNCzc9qJ"
      },
      "source": [
        "#### Install all function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oh1Mi0rHFpkU"
      },
      "source": [
        "# 8.0 Function to call all functions\n",
        "def install_hadoop():\n",
        "  print(\"\\n--Install java----\\n\")\n",
        "  ssh_install()\n",
        "  install_java()  \n",
        "  hadoop_install()\n",
        "  hadoop_config()\n",
        "  set_keys()\n",
        "  set_env()\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHJyHMhUdCRZ"
      },
      "source": [
        "### Begin install\n",
        "Start downloading, install and configure. Takes around 2 minutes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77YQikvsJiTm"
      },
      "source": [
        "# 9.0 Start installation\n",
        "start = time.time()\n",
        "install_hadoop()\n",
        "end = time.time()\n",
        "print(\"\\n---Time taken----\\n\")\n",
        "print((end- start)/60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MW3SwbcLdGaY"
      },
      "source": [
        "### Format hadoop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhLO-fzv3EpE"
      },
      "source": [
        "# 10.0 Format hadoop\n",
        "print(\"\\n---24. Format namenode----\\n\")\n",
        "!hdfs namenode  -format"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3iMrO3ldPsS"
      },
      "source": [
        "## Start and test hadoop\n",
        "If namenode is in safemode, use the command:   \n",
        "`!hdfs dfsadmin -safemode leave`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvfQ6c1KeqIh"
      },
      "source": [
        "#### Start hadoop\n",
        "If start fails with 'Connection refused', run `ssh_install()` once again"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlT89iZz_aeh",
        "outputId": "8b87e392-c634-4030-97e5-0e6c033c3a90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 11.0 Start namenode\n",
        "#      If this fails, run\n",
        "#       ssh_install() below\n",
        "#        and start hadoop again:\n",
        "\n",
        "print(\"\\n---25. Start namenode----\\n\")\n",
        "! start-dfs.sh"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "---25. Start namenode----\n",
            "\n",
            "Starting namenodes on [localhost]\n",
            "localhost: Warning: Permanently added 'localhost' (ECDSA) to the list of known hosts.\n",
            "Starting datanodes\n",
            "Starting secondary namenodes [b44e9ceadc5e]\n",
            "b44e9ceadc5e: Warning: Permanently added 'b44e9ceadc5e,172.28.0.2' (ECDSA) to the list of known hosts.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScVvdOPiuAaI"
      },
      "source": [
        "#ssh_install()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azR2g9vkes4K"
      },
      "source": [
        "#### Start yarn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8Qx_vc9PHZj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10709d40-53cb-4c87-c024-ba5381c4661c"
      },
      "source": [
        "# 11.1 Start yarn\n",
        "! start-yarn.sh"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting resourcemanager\n",
            "Starting nodemanagers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UI-wzZD6ccBM"
      },
      "source": [
        "If `start-dfs.sh` fails, issue the following three commands, one after another:<br>  \n",
        "`! sudo apt-get remove openssh-client openssh-server`<br>\n",
        "`! sudo apt-get install openssh-client openssh-server`<br>\n",
        "`! service ssh restart`<br>\n",
        "\n",
        "And then try to start hadoop again, as: `start-dfs.sh`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJZLpxOYev7y"
      },
      "source": [
        "#### Test hadoop\n",
        "IF in safe mode, leave safe mode as:<br>\n",
        "`!hdfs dfsadmin -safemode leave`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMt89uLNCUVC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2e7778a-c516-4fcd-d0df-8f7a123ff13c"
      },
      "source": [
        "# 11.1\n",
        "print(\"\\n---26. Make folders in hadoop----\\n\")\n",
        "! hdfs dfs -mkdir /user\n",
        "! hdfs dfs -mkdir /user/ashok"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "---26. Make folders in hadoop----\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ez0FPw3CtOi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3971ebdb-5b23-4912-99a6-b3c16e1091e1"
      },
      "source": [
        "# 11.2 Run hadoop commands\n",
        "! hdfs dfs -ls /\n",
        "! hdfs dfs -ls /user"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1 items\n",
            "drwxr-xr-x   - root supergroup          0 2021-03-30 11:28 /user\n",
            "Found 1 items\n",
            "drwxr-xr-x   - root supergroup          0 2021-03-30 11:28 /user/ashok\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-fSbrEWH643"
      },
      "source": [
        "# 11.3 Stopping hadoop\n",
        "#      Gives some errors\n",
        "#      But hadoop stops\n",
        "#!stop-dfs.sh"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwBD74dadYfz"
      },
      "source": [
        "Run the `ssh_install()` again if hadoop fails to start with `start-dfs.sh` and then try to start hadoop again."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBFVWWuafLBL"
      },
      "source": [
        "## Install spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyuLCFRJsvW3"
      },
      "source": [
        "### Define functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kk3BP0OfYpT1"
      },
      "source": [
        "`findspark`: PySpark isn't on `sys.path` by default, but that doesn't mean it can't be used as a regular library. You can address this by either symlinking pyspark into your site-packages, or adding `pyspark` to `sys.path` at runtime. `findspark` does the latter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opLGqtPRfM_5"
      },
      "source": [
        "# 1.0 Function to download and unzip spark\n",
        "def spark_koalas_install():\n",
        "  print(\"\\n--1.1 Install findspark----\\n\")\n",
        "  !pip install -q findspark\n",
        "\n",
        "  print(\"\\n--1.2 Install databricks Koalas----\\n\")\n",
        "  !pip install koalas\n",
        "\n",
        "  print(\"\\n--1.3 Download Apache tar.gz----\\n\")\n",
        "  ! wget -c https://mirrors.estointernet.in/apache/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "\n",
        "  print(\"\\n--1.4 Transfer downloaded content and unzip tar.gz----\\n\")\n",
        "  !  mv /content/spark*   /opt/\n",
        "  ! tar -xzf /opt/spark-3.1.1-bin-hadoop3.2.tgz  --directory /opt/\n",
        "\n",
        "  print(\"\\n--1.5 Check folder for files----\\n\")\n",
        "  ! ls -la /opt\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebXfvQ1qiQHz"
      },
      "source": [
        "# 1.1 Function to set environment\n",
        "def set_spark_env():\n",
        "  print(\"\\n---2. Set Environment variables----\\n\")\n",
        "  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\" \n",
        "  os.environ[\"JRE_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64/jre\" \n",
        "  os.environ[\"SPARK_HOME\"] = \"/opt/spark-3.1.1-bin-hadoop3.2\"     \n",
        "  os.environ[\"LD_LIBRARY_PATH\"] += \":/opt/spark-3.1.1-bin-hadoop3.2/lib/native\"\n",
        "  os.environ[\"PATH\"] += \":/opt/spark-3.1.1-bin-hadoop3.2/bin:/opt/spark-3.1.1-bin-hadoop3.2/sbin\"\n",
        "  print(\"\\n---2.1. Check Environment variables----\\n\")\n",
        "  # Check\n",
        "  ! echo $PATH\n",
        "  ! echo $LD_LIBRARY_PATH"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUsZggDHj13U"
      },
      "source": [
        "# 1.2 Function to configure spark \n",
        "def spark_conf():\n",
        "  print(\"\\n---3. Configure spark to access hadoop----\\n\")\n",
        "  !mv /opt/spark-3.1.1-bin-hadoop3.2/conf/spark-env.sh.template  /opt/spark-3.1.1-bin-hadoop3.2/conf/spark-env.sh\n",
        "  !echo \"HADOOP_CONF_DIR=/opt/hadoop-3.2.2/etc/hadoop/\" >> /opt/spark-3.1.1-bin-hadoop3.2/conf/spark-env.sh\n",
        "  print(\"\\n---3.1 Check ----\\n\")\n",
        "  #!cat /opt/spark-3.1.1-bin-hadoop3.2/conf/spark-env.sh"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7kLbAFLszN3"
      },
      "source": [
        "### Install spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_eaLhtPktHJ"
      },
      "source": [
        "# 2.0 Call all the three functions\n",
        "def install_spark():\n",
        "  spark_koalas_install()\n",
        "  set_spark_env()\n",
        "  spark_conf()\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emaHs1XxRt5z",
        "outputId": "49cb774a-1472-4b97-9784-781d19ff38c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 2.1 \n",
        "install_spark()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "--1.1 Install findspark----\n",
            "\n",
            "\n",
            "--1.2 Install databricks Koalas----\n",
            "\n",
            "Collecting koalas\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/de/87c016a3e5055251ed117c86eb3b0de2381518c7acae54e115711ff30ceb/koalas-1.7.0-py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 5.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<1.20.0,>=1.14 in /usr/local/lib/python3.7/dist-packages (from koalas) (1.19.5)\n",
            "Requirement already satisfied: pyarrow>=0.10 in /usr/local/lib/python3.7/dist-packages (from koalas) (3.0.0)\n",
            "Requirement already satisfied: pandas<1.2.0,>=0.23.2 in /usr/local/lib/python3.7/dist-packages (from koalas) (1.1.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas<1.2.0,>=0.23.2->koalas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas<1.2.0,>=0.23.2->koalas) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas<1.2.0,>=0.23.2->koalas) (1.15.0)\n",
            "Installing collected packages: koalas\n",
            "Successfully installed koalas-1.7.0\n",
            "\n",
            "--1.3 Download Apache tar.gz----\n",
            "\n",
            "--2021-03-30 11:29:04--  https://mirrors.estointernet.in/apache/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
            "Resolving mirrors.estointernet.in (mirrors.estointernet.in)... 43.255.166.254, 2403:8940:3:1::f\n",
            "Connecting to mirrors.estointernet.in (mirrors.estointernet.in)|43.255.166.254|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228721937 (218M) [application/octet-stream]\n",
            "Saving to: ‘spark-3.1.1-bin-hadoop3.2.tgz’\n",
            "\n",
            "spark-3.1.1-bin-had 100%[===================>] 218.13M  11.9MB/s    in 22s     \n",
            "\n",
            "2021-03-30 11:29:27 (9.91 MB/s) - ‘spark-3.1.1-bin-hadoop3.2.tgz’ saved [228721937/228721937]\n",
            "\n",
            "\n",
            "--1.4 Transfer downloaded content and unzip tar.gz----\n",
            "\n",
            "\n",
            "--1.5 Check folder for files----\n",
            "\n",
            "total 609576\n",
            "drwxr-xr-x  1 root root      4096 Mar 30 11:29 .\n",
            "drwxr-xr-x  1 root root      4096 Mar 30 11:26 ..\n",
            "drwxr-xr-x  1 root root      4096 Mar 18 13:31 google\n",
            "drwxr-xr-x 10 1000 1000      4096 Mar 30 11:26 hadoop-3.2.2\n",
            "-rw-r--r--  1 root root 395448622 Jan 13 18:48 hadoop-3.2.2.tar.gz\n",
            "drwxr-xr-x  4 root root      4096 Mar 18 13:25 nvidia\n",
            "drwxr-xr-x 13 1000 1000      4096 Feb 22 02:11 spark-3.1.1-bin-hadoop3.2\n",
            "-rw-r--r--  1 root root 228721937 Feb 22 02:45 spark-3.1.1-bin-hadoop3.2.tgz\n",
            "\n",
            "---2. Set Environment variables----\n",
            "\n",
            "\n",
            "---2.1. Check Environment variables----\n",
            "\n",
            "/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin:/opt/bin:/opt/hadoop-3.2.2/bin:/opt/hadoop-3.2.2/sbin:/opt/spark-3.1.1-bin-hadoop3.2/bin:/opt/spark-3.1.1-bin-hadoop3.2/sbin\n",
            "/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/opt/hadoop-3.2.2/lib/native:/opt/spark-3.1.1-bin-hadoop3.2/lib/native\n",
            "\n",
            "---3. Configure spark to access hadoop----\n",
            "\n",
            "\n",
            "---3.1 Check ----\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6XYg1IGs17n"
      },
      "source": [
        "## Test spark\n",
        "Hadoop should have been started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZwLRxQWZryJ"
      },
      "source": [
        "Call some libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_1RXFGClfCy",
        "outputId": "db4670b6-53cf-4cda-e585-a1f200c000aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 3.0 Just call some libraries to test\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 3.1 Get spark in sys.path\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "# 3.2 Call other spark libraries\n",
        "#     Just to test\n",
        "from pyspark.sql import SparkSession\n",
        "import databricks.koalas as ks\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. Koalas will set it for you but it does not work if there is a Spark context already launched.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISNdxVmMmUVz"
      },
      "source": [
        "# 3.1 Build spark session\n",
        "spark = SparkSession. \\\n",
        "                    builder. \\\n",
        "                    master(\"local[*]\"). \\\n",
        "                    getOrCreate()\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVlRiGQJmk58"
      },
      "source": [
        "# 4.0 Pandas DataFrame\n",
        "pdf = pd.DataFrame({\n",
        "        'x1': ['a','a','b','b', 'b', 'c', 'd','d'],\n",
        "        'x2': ['apple', 'orange', 'orange','orange', 'peach', 'peach','apple','orange'],\n",
        "        'x3': [1, 1, 2, 2, 2, 4, 1, 2],\n",
        "        'x4': [2.4, 2.5, 3.5, 1.4, 2.1,1.5, 3.0, 2.0],\n",
        "        'y1': [1, 0, 1, 0, 0, 1, 1, 0],\n",
        "        'y2': ['yes', 'no', 'no', 'yes', 'yes', 'yes', 'no', 'yes']\n",
        "    })\n",
        "\n",
        "# 4.1\n",
        "pdf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3pSQ7sDmnjt"
      },
      "source": [
        "# 4.2 Transform to Spark DataFrame\n",
        "df = spark.createDataFrame(pdf)\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzVBbgLkmsIR"
      },
      "source": [
        "# 4.3 Create a csv file \n",
        "#     and tranfer it to hdfs\n",
        "!echo \"a,b,c,d\"   > /content/airports.csv\n",
        "!echo \"5,4,6,7\"   >> /content/airports.csv\n",
        "!echo \"2,3,4,5\"   >> /content/airports.csv\n",
        "!echo \"8,9,0,1\"   >> /content/airports.csv\n",
        "!echo \"2,3,4,1\"   >> /content/airports.csv\n",
        "!echo \"1,2,2,1\"   >> /content/airports.csv\n",
        "!echo \"0,1,2,6\"   >> /content/airports.csv\n",
        "!echo \"9,3,1,8\"   >> /content/airports.csv\n",
        "!ls -la /content\n",
        "\n",
        "# 4.4\n",
        "!hdfs dfs -rm -f /user/ashok/airports.csv\n",
        "!hdfs dfs -put /content/airports.csv  /user/ashok/\n",
        "!hdfs dfs -ls /user/ashok"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fxfT0ysnmEK"
      },
      "source": [
        "# 5.0 Read file directly from hadoop\n",
        "airports_df = spark.read.csv( \n",
        "                              \"/user/ashok/airports.csv\",\n",
        "                              inferSchema = True,\n",
        "                              header = True\n",
        "                             )\n",
        "\n",
        "# 5.1 Show file\n",
        "airports_df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MihVRZhdxI4"
      },
      "source": [
        "## Test Koalas\n",
        "Hadoop should have been started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOGpE59BeHvk"
      },
      "source": [
        "Create a koalas dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPETd5DKtRem"
      },
      "source": [
        "# 6.0\n",
        "# If namenode is in safemode, first use:\n",
        "# hdfs dfsadmin -safemode leave\n",
        "kdf = ks.DataFrame(\n",
        "                   {\n",
        "                       'a': [1, 2, 3, 4, 5, 6],\n",
        "                       'b': [100, 200, 300, 400, 500, 600],\n",
        "                       'c': [\"one\", \"two\", \"three\", \"four\", \"five\", \"six\"]\n",
        "                    },\n",
        "                    index=[10, 20, 30, 40, 50, 60]\n",
        "                   )\n",
        "\n",
        "# 6.1 And show\n",
        "kdf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67nrVj9ctYwU"
      },
      "source": [
        "# 6.2 Pandas DataFrame\n",
        "pdf = pd.DataFrame({'x':range(3), 'y':['a','b','b'], 'z':['a','b','b']})\n",
        "\n",
        "# 6.2.1 Transform to koalas DataFrame\n",
        "df = ks.from_pandas(pdf)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2U6CsdIftpEf"
      },
      "source": [
        "# 6.3 Rename koalas dataframe columns\n",
        "df.columns = ['x', 'y', 'z1']\n",
        "\n",
        "# 6.4 Do some operations on koalas DF, in place:\n",
        "df['x2'] = df.x * df.x\n",
        "\n",
        "# 6.6 Finally show koalas df\n",
        "df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eovb-i45tt4Z"
      },
      "source": [
        "# 6.7 Read csv file from hadoop\n",
        "#     and create koalas df\n",
        "ks.read_csv(\"/user/ashok/airports.csv\").head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_033UMyX7HJ"
      },
      "source": [
        "###################"
      ],
      "execution_count": 32,
      "outputs": []
    }
  ]
}