{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hadoop_install.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harnalashok/hadoop/blob/main/hadoop_install_on_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4q64Z8IYb8fr"
      },
      "source": [
        "# Last amended: 29th March, 2021\n",
        "# Myfolder: github/hadoop\n",
        "# Objective:\n",
        "#            i)  Install hadoop on colab\n",
        "#                (current version is 3.2.2)\n",
        "#            ii) Experiments with hadoop\n",
        "#\n",
        "#\n",
        "# Java 8 install: https://stackoverflow.com/a/58191107\n",
        "# Hadoop install: https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html\n",
        "# Spark install:  https://stackoverflow.com/a/64183749"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3shMHHDCHw-"
      },
      "source": [
        "# 1.0 How to set environment variable\n",
        "import os  \n",
        "import time    "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgS9HNCyR7C0"
      },
      "source": [
        "# 2.0 Function to install ssh client and sshd (Server)\n",
        "def ssh_install():\n",
        "  print(\"\\n--1. Download and install ssh server----\\n\")\n",
        "  ! sudo apt-get remove openssh-client openssh-server\n",
        "  ! sudo apt install openssh-client openssh-server\n",
        "  \n",
        "  print(\"\\n--2. Restart ssh server----\\n\")\n",
        "  ! service ssh restart"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsFu84PSR9jR"
      },
      "source": [
        "# 3.0 Function to download and install java 8\n",
        "def install_java():\n",
        "  ! rm -rf /usr/java\n",
        "\n",
        "  print(\"\\n--Download and install Java 8----\\n\")\n",
        "  !apt-get install -y openjdk-8-jdk-headless -qq > /dev/null        # install openjdk\n",
        "  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"     # set environment variable\n",
        "\n",
        "  !update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java\n",
        "  !update-alternatives --set javac /usr/lib/jvm/java-8-openjdk-amd64/bin/javac\n",
        "  \n",
        "  !mkdir -p /usr/java\n",
        "  ! ln -s \"/usr/lib/jvm/java-8-openjdk-amd64\"  \"/usr/java\"\n",
        "  ! mv \"/usr/java/java-8-openjdk-amd64\"  \"/usr/java/latest\"\n",
        "  \n",
        "  !java -version       #check java version\n",
        "  !javac -version"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dg2fSk_ylMbr"
      },
      "source": [
        "# 4.0 Function to download and install hadoop\n",
        "def hadoop_install():\n",
        "  print(\"\\n--5. Download hadoop tar.gz----\\n\")\n",
        "  ! wget -c https://mirrors.estointernet.in/apache/hadoop/common/hadoop-3.2.2/hadoop-3.2.2.tar.gz\n",
        "\n",
        "  print(\"\\n--6. Transfer downloaded content and unzip tar.gz----\\n\")\n",
        "  !  mv /content/hadoop*   /opt/\n",
        "  ! tar -xzvf /opt/hadoop-3.2.2.tar.gz  --directory /opt/\n",
        "\n",
        "  print(\"\\n--7. Create hadoop folder----\\n\")\n",
        "  ! rm -r /app/hadoop/tmp\n",
        "  ! mkdir  -p   /app/hadoop/tmp\n",
        "  \n",
        "  print(\"\\n--8. Check folder for files----\\n\")\n",
        "  ! ls -la /opt"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SChb4dXsurlC"
      },
      "source": [
        "# 5.0 Function for setting hadoop configuration\n",
        "def hadoop_config():\n",
        "  print(\"\\n--Begin Configuring hadoop---\\n\")\n",
        "  print(\"\\n=============================\\n\")\n",
        "  print(\"\\n--9. core-site.xml----\\n\")\n",
        "  ! cat  /opt/hadoop-3.2.2/etc/hadoop/core-site.xml\n",
        "\n",
        "  print(\"\\n--10. Amend core-site.xml----\\n\")\n",
        "  !  echo  '<?xml version=\"1.0\" encoding=\"UTF-8\"?>' >  /opt/hadoop-3.2.2/etc/hadoop/core-site.xml\n",
        "  !  echo  '<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>' >>  /opt/hadoop-3.2.2/etc/hadoop/core-site.xml\n",
        "  !  echo  ' <configuration>' >>  /opt/hadoop-3.2.2/etc/hadoop/core-site.xml\n",
        "  !  echo  '    <property>' >>  /opt/hadoop-3.2.2/etc/hadoop/core-site.xml\n",
        "  !  echo  '        <name>fs.defaultFS</name>' >>  /opt/hadoop-3.2.2/etc/hadoop/core-site.xml\n",
        "  !  echo  '        <value>hdfs://localhost:9000</value>' >>  /opt/hadoop-3.2.2/etc/hadoop/core-site.xml\n",
        "  !  echo  '    </property>' >>  /opt/hadoop-3.2.2/etc/hadoop/core-site.xml\n",
        "  !  echo  '    <property>' >>  /opt/hadoop-3.2.2/etc/hadoop/core-site.xml\n",
        "  !  echo  '          <name>hadoop.tmp.dir</name>' >>  /opt/hadoop-3.2.2/etc/hadoop/core-site.xml\n",
        "  !  echo  '          <value>/app/hadoop/tmp</value>' >>  /opt/hadoop-3.2.2/etc/hadoop/core-site.xml\n",
        "  !  echo  '       <description>A base for other temporary directories.</description>' >>  /opt/hadoop-3.2.2/etc/hadoop/core-site.xml\n",
        "  !  echo  '     </property>' >>  /opt/hadoop-3.2.2/etc/hadoop/core-site.xml\n",
        "  !  echo  '  </configuration>' >>  /opt/hadoop-3.2.2/etc/hadoop/core-site.xml\n",
        "\n",
        "  print(\"\\n--11. Amended core-site.xml----\\n\")\n",
        "  ! cat  /opt/hadoop-3.2.2/etc/hadoop/core-site.xml\n",
        "\n",
        "  print(\"\\n--12. yarn-site.xml----\\n\")\n",
        "  !cat /opt/hadoop-3.2.2/etc/hadoop/yarn-site.xml\n",
        "\n",
        "  !echo '<?xml version=\"1.0\" encoding=\"UTF-8\"?>' > /opt/hadoop-3.2.2/etc/hadoop/yarn-site.xml\n",
        "  !echo '<configuration>' >> /opt/hadoop-3.2.2/etc/hadoop/yarn-site.xml\n",
        "  !echo '    <property>' >> /opt/hadoop-3.2.2/etc/hadoop/yarn-site.xml\n",
        "  !echo '        <name>yarn.nodemanager.aux-services</name>' >> /opt/hadoop-3.2.2/etc/hadoop/yarn-site.xml\n",
        "  !echo '        <value>mapreduce_shuffle</value>' >> /opt/hadoop-3.2.2/etc/hadoop/yarn-site.xml\n",
        "  !echo '    </property>' >> /opt/hadoop-3.2.2/etc/hadoop/yarn-site.xml\n",
        "  !echo '    <property>'  >> /opt/hadoop-3.2.2/etc/hadoop/yarn-site.xml\n",
        "  !echo '       <name>yarn.nodemanager.vmem-check-enabled</name>'  >> /opt/hadoop-3.2.2/etc/hadoop/yarn-site.xml\n",
        "  !echo '       <value>false</value>'  >> /opt/hadoop-3.2.2/etc/hadoop/yarn-site.xml\n",
        "  !echo '    </property>'  >> /opt/hadoop-3.2.2/etc/hadoop/yarn-site.xml\n",
        "  !echo ' </configuration>'  >> /opt/hadoop-3.2.2/etc/hadoop/yarn-site.xml\n",
        "  \n",
        "  print(\"\\n--13. Amended yarn-site.xml----\\n\")\n",
        "  !cat /opt/hadoop-3.2.2/etc/hadoop/yarn-site.xml\n",
        "\n",
        "  print(\"\\n--14. mapred-site.xml----\\n\")\n",
        "  !cat  /opt/hadoop-3.2.2/etc/hadoop/mapred-site.xml\n",
        "\n",
        "  print(\"\\n--15. Amend mapred-site.xml----\\n\")\n",
        "  !echo '<?xml version=\"1.0\"?>'  > /opt/hadoop-3.2.2/etc/hadoop/mapred-site.xml\n",
        "  !echo '<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>'  >> /opt/hadoop-3.2.2/etc/hadoop/mapred-site.xml\n",
        "  !echo '<configuration>'  >> /opt/hadoop-3.2.2/etc/hadoop/mapred-site.xml\n",
        "  !echo '    <property>'  >> /opt/hadoop-3.2.2/etc/hadoop/mapred-site.xml\n",
        "  !echo '       <name>mapreduce.framework.name</name>'  >> /opt/hadoop-3.2.2/etc/hadoop/mapred-site.xml\n",
        "  !echo '        <value>yarn</value>'  >> /opt/hadoop-3.2.2/etc/hadoop/mapred-site.xml\n",
        "  !echo '    </property>'  >> /opt/hadoop-3.2.2/etc/hadoop/mapred-site.xml\n",
        "  !echo '    <property>'  >> /opt/hadoop-3.2.2/etc/hadoop/mapred-site.xml\n",
        "  !echo '        <name>yarn.app.mapreduce.am.env</name>'  >> /opt/hadoop-3.2.2/etc/hadoop/mapred-site.xml\n",
        "  !echo '        <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>'  >> /opt/hadoop-3.2.2/etc/hadoop/mapred-site.xml\n",
        "  !echo '    </property>'  >> /opt/hadoop-3.2.2/etc/hadoop/mapred-site.xml\n",
        "  !echo '    <property>'  >> /opt/hadoop-3.2.2/etc/hadoop/mapred-site.xml\n",
        "  !echo '       <name>mapreduce.map.env</name>'  >> /opt/hadoop-3.2.2/etc/hadoop/mapred-site.xml\n",
        "  !echo '       <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>'  >> /opt/hadoop-3.2.2/etc/hadoop/mapred-site.xml\n",
        "  !echo '    </property>'  >> /opt/hadoop-3.2.2/etc/hadoop/mapred-site.xml\n",
        "  !echo '    <property>'  >> /opt/hadoop-3.2.2/etc/hadoop/mapred-site.xml\n",
        "  !echo '      <name>mapreduce.reduce.env</name>'  >> /opt/hadoop-3.2.2/etc/hadoop/mapred-site.xml\n",
        "  !echo '      <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>'  >> /opt/hadoop-3.2.2/etc/hadoop/mapred-site.xml\n",
        "  !echo '   </property>'  >> /opt/hadoop-3.2.2/etc/hadoop/mapred-site.xml\n",
        "  !echo '</configuration>'  >> /opt/hadoop-3.2.2/etc/hadoop/mapred-site.xml\n",
        "\n",
        "  print(\"\\n--16, Amended mapred-site.xml----\\n\")\n",
        "  !cat  /opt/hadoop-3.2.2/etc/hadoop/mapred-site.xml\n",
        "\n",
        "  print(\"\\n---17. hdfs-site.xml----\\n\")\n",
        "  !cat  /opt/hadoop-3.2.2/etc/hadoop/hdfs-site.xml\n",
        "  \n",
        "  print(\"\\n---18. Amend hdfs-site.xml----\\n\")\n",
        "  !echo  '<?xml version=\"1.0\" encoding=\"UTF-8\"?> '   > /opt/hadoop-3.2.2/etc/hadoop/hdfs-site.xml\n",
        "  !echo  '<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>' >> /opt/hadoop-3.2.2/etc/hadoop/hdfs-site.xml\n",
        "  !echo  '<configuration>'  >> /opt/hadoop-3.2.2/etc/hadoop/hdfs-site.xml\n",
        "  !echo  '    <property>'  >> /opt/hadoop-3.2.2/etc/hadoop/hdfs-site.xml\n",
        "  !echo  '        <name>dfs.replication</name>'  >> /opt/hadoop-3.2.2/etc/hadoop/hdfs-site.xml\n",
        "  !echo  '        <value>1</value>'  >> /opt/hadoop-3.2.2/etc/hadoop/hdfs-site.xml\n",
        "  !echo  '    </property>'  >> /opt/hadoop-3.2.2/etc/hadoop/hdfs-site.xml\n",
        "  !echo  '   <property>'   >> /opt/hadoop-3.2.2/etc/hadoop/hdfs-site.xml\n",
        "  !echo  '        <name>dfs.block.size</name>'  >> /opt/hadoop-3.2.2/etc/hadoop/hdfs-site.xml\n",
        "  !echo  '        <value>16777216</value>'  >> /opt/hadoop-3.2.2/etc/hadoop/hdfs-site.xml\n",
        "  !echo  '        <description>Block size</description>'  >> /opt/hadoop-3.2.2/etc/hadoop/hdfs-site.xml\n",
        "  !echo  '  </property>'  >> /opt/hadoop-3.2.2/etc/hadoop/hdfs-site.xml\n",
        "  !echo  '</configuration>'  >> /opt/hadoop-3.2.2/etc/hadoop/hdfs-site.xml\n",
        "\n",
        "  print(\"\\n---19. Amended hdfs-site.xml----\\n\")\n",
        "  !cat  /opt/hadoop-3.2.2/etc/hadoop/hdfs-site.xml\n",
        "\n",
        "  print(\"\\n---20. hadoop-env.sh----\\n\")\n",
        "  # https://stackoverflow.com/a/53140448\n",
        "  !cat /opt/hadoop-3.2.2/etc/hadoop/hadoop-env.sh\n",
        "  ! echo 'export JAVA_HOME=\"/usr/lib/jvm/java-8-openjdk-amd64\"' >> /opt/hadoop-3.2.2/etc/hadoop/hadoop-env.sh\n",
        "  ! echo 'export HDFS_NAMENODE_USER=\"root\"'  >> /opt/hadoop-3.2.2/etc/hadoop/hadoop-env.sh\n",
        "  ! echo 'export HDFS_DATANODE_USER=\"root\"'  >> /opt/hadoop-3.2.2/etc/hadoop/hadoop-env.sh\n",
        "  ! echo 'export HDFS_SECONDARYNAMENODE_USER=\"root\"'  >> /opt/hadoop-3.2.2/etc/hadoop/hadoop-env.sh\n",
        "  ! echo 'export YARN_RESOURCEMANAGER_USER=\"root\"'  >> /opt/hadoop-3.2.2/etc/hadoop/hadoop-env.sh\n",
        "  ! echo 'export YARN_NODEMANAGER_USER=\"root\"'  >> /opt/hadoop-3.2.2/etc/hadoop/hadoop-env.sh\n",
        "  \n",
        "  print(\"\\n---21. Amended hadoop-env.sh----\\n\")\n",
        "  !cat /opt/hadoop-3.2.2/etc/hadoop/hadoop-env.sh\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOFbfw7n0Pps"
      },
      "source": [
        "# 6.0 Function tp setup ssh passphrase\n",
        "def set_keys():\n",
        "  print(\"\\n---22. Generate SSH keys----\\n\")\n",
        "  ! cd ~ ; pwd \n",
        "  ! cd ~ ; ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa\n",
        "  ! cd ~ ; cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\n",
        "  ! cd ~ ; chmod 0600 ~/.ssh/authorized_keys\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRSn9XAV4rsR"
      },
      "source": [
        "# 7.0 Function to set up environmental variables\n",
        "def set_env():\n",
        "  print(\"\\n---23. Set Environment variables----\\n\")\n",
        "  # 'export' command does not work in colab\n",
        "  # https://stackoverflow.com/a/57240319\n",
        "  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"     #set environment variable\n",
        "  os.environ[\"JRE_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64/jre\"   \n",
        "  os.environ[\"HADOOP_HOME\"] = \"/opt/hadoop-3.2.2\"\n",
        "  os.environ[\"HADOOP_CONF_DIR\"] = \"/opt/hadoop-3.2.2/etc/hadoop\" \n",
        "  os.environ[\"LD_LIBRARY_PATH\"] += \":/opt/hadoop-3.2.2/lib/native\"\n",
        "  os.environ[\"PATH\"] += \":/opt/hadoop-3.2.2/bin:/opt/hadoop-3.2.2/sbin\""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oh1Mi0rHFpkU"
      },
      "source": [
        "# 8.0 Function to call all functions\n",
        "def install_hadoop():\n",
        "  print(\"\\n--Install java----\\n\")\n",
        "  ssh_install()\n",
        "  install_java()  \n",
        "  hadoop_install()\n",
        "  hadoop_config()\n",
        "  set_keys()\n",
        "  set_env()\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77YQikvsJiTm"
      },
      "source": [
        "# 9.0 Start installation\n",
        "start = time.time()\n",
        "install_hadoop()\n",
        "end = time.time()\n",
        "print(\"\\n---Time taken----\\n\")\n",
        "print((end- start)/60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhLO-fzv3EpE",
        "outputId": "7e5333d2-a065-4b56-c80f-ac2f03774b1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 10.0 Format hadoop\n",
        "print(\"\\n---24. Format namenode----\\n\")\n",
        "!hdfs namenode  -format"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "---24. Format namenode----\n",
            "\n",
            "WARNING: /opt/hadoop-3.2.2/logs does not exist. Creating.\n",
            "2021-03-29 13:35:09,057 INFO namenode.NameNode: STARTUP_MSG: \n",
            "/************************************************************\n",
            "STARTUP_MSG: Starting NameNode\n",
            "STARTUP_MSG:   host = f28419ec4842/172.28.0.2\n",
            "STARTUP_MSG:   args = [-format]\n",
            "STARTUP_MSG:   version = 3.2.2\n",
            "STARTUP_MSG:   classpath = /opt/hadoop-3.2.2/etc/hadoop:/opt/hadoop-3.2.2/share/hadoop/common/lib/curator-recipes-2.13.0.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/commons-io-2.5.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/httpclient-4.5.13.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/commons-net-3.6.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/jackson-core-2.9.10.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/kerb-server-1.0.1.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/accessors-smart-1.2.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/netty-3.10.6.Final.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/commons-beanutils-1.9.4.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/jetty-http-9.4.20.v20190813.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/jersey-server-1.19.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/kerb-client-1.0.1.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/failureaccess-1.0.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/checker-qual-2.5.2.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/curator-framework-2.13.0.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/commons-compress-1.19.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/jsr305-3.0.2.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/jersey-json-1.19.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/token-provider-1.0.1.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/commons-lang3-3.7.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/asm-5.0.4.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/stax2-api-3.1.4.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/jetty-util-9.4.20.v20190813.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/jetty-security-9.4.20.v20190813.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/jsch-0.1.55.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/dnsjava-2.1.7.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/javax.activation-api-1.2.0.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/commons-text-1.4.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/snappy-java-1.0.5.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/jetty-xml-9.4.20.v20190813.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/hadoop-auth-3.2.2.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/json-smart-2.3.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/commons-codec-1.11.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/guava-27.0-jre.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/re2j-1.1.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/jersey-core-1.19.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/jetty-server-9.4.20.v20190813.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/jetty-servlet-9.4.20.v20190813.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/avro-1.7.7.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/httpcore-4.4.13.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/kerby-config-1.0.1.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/kerb-common-1.0.1.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/curator-client-2.13.0.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/kerby-util-1.0.1.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/metrics-core-3.2.4.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/jersey-servlet-1.19.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/jackson-databind-2.9.10.4.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/zookeeper-3.4.13.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/jetty-webapp-9.4.20.v20190813.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/jetty-io-9.4.20.v20190813.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/hadoop-annotations-3.2.2.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/kerb-core-1.0.1.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/jackson-annotations-2.9.10.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/kerb-util-1.0.1.jar:/opt/hadoop-3.2.2/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/opt/hadoop-3.2.2/share/hadoop/common/hadoop-common-3.2.2-tests.jar:/opt/hadoop-3.2.2/share/hadoop/common/hadoop-kms-3.2.2.jar:/opt/hadoop-3.2.2/share/hadoop/common/hadoop-common-3.2.2.jar:/opt/hadoop-3.2.2/share/hadoop/common/hadoop-nfs-3.2.2.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/curator-recipes-2.13.0.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/commons-io-2.5.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/httpclient-4.5.13.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/commons-net-3.6.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/jackson-core-2.9.10.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/netty-3.10.6.Final.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/jettison-1.1.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/commons-beanutils-1.9.4.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/jetty-http-9.4.20.v20190813.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/gson-2.2.4.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/curator-framework-2.13.0.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/commons-compress-1.19.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/jsr305-3.0.2.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/commons-lang3-3.7.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/asm-5.0.4.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/jetty-util-9.4.20.v20190813.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/jetty-security-9.4.20.v20190813.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/paranamer-2.3.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/jsch-0.1.55.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/javax.activation-api-1.2.0.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/commons-text-1.4.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/okio-1.6.0.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/hadoop-auth-3.2.2.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/json-smart-2.3.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/re2j-1.1.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/jetty-server-9.4.20.v20190813.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/avro-1.7.7.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/httpcore-4.4.13.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/netty-all-4.1.48.Final.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/curator-client-2.13.0.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/jackson-databind-2.9.10.4.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/zookeeper-3.4.13.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/jetty-io-9.4.20.v20190813.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/hadoop-annotations-3.2.2.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/jackson-annotations-2.9.10.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/hadoop-hdfs-3.2.2-tests.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/hadoop-hdfs-3.2.2.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.2.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/hadoop-hdfs-client-3.2.2.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.2.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.2-tests.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.2.2.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.2-tests.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/hadoop-hdfs-client-3.2.2-tests.jar:/opt/hadoop-3.2.2/share/hadoop/hdfs/hadoop-hdfs-nfs-3.2.2.jar:/opt/hadoop-3.2.2/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop-3.2.2/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop-3.2.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.2-tests.jar:/opt/hadoop-3.2.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.2.jar:/opt/hadoop-3.2.2/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.2.2.jar:/opt/hadoop-3.2.2/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.2.2.jar:/opt/hadoop-3.2.2/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.2.2.jar:/opt/hadoop-3.2.2/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.2.2.jar:/opt/hadoop-3.2.2/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.2.2.jar:/opt/hadoop-3.2.2/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.2.2.jar:/opt/hadoop-3.2.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.2.jar:/opt/hadoop-3.2.2/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.2.2.jar:/opt/hadoop-3.2.2/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.2.2.jar:/opt/hadoop-3.2.2/share/hadoop/yarn:/opt/hadoop-3.2.2/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop-3.2.2/share/hadoop/yarn/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop-3.2.2/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop-3.2.2/share/hadoop/yarn/lib/objenesis-1.0.jar:/opt/hadoop-3.2.2/share/hadoop/yarn/lib/guice-4.0.jar:/opt/hadoop-3.2.2/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop-3.2.2/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.9.10.jar:/opt/hadoop-3.2.2/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/opt/hadoop-3.2.2/share/hadoop/yarn/lib/jackson-jaxrs-base-2.9.10.jar:/opt/hadoop-3.2.2/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/opt/hadoop-3.2.2/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/opt/hadoop-3.2.2/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/opt/hadoop-3.2.2/share/hadoop/yarn/lib/jersey-client-1.19.jar:/opt/hadoop-3.2.2/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/opt/hadoop-3.2.2/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop-3.2.2/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/opt/hadoop-3.2.2/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/opt/hadoop-3.2.2/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.9.10.jar:/opt/hadoop-3.2.2/share/hadoop/yarn/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop-3.2.2/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop-3.2.2/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/opt/hadoop-3.2.2/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/opt/hadoop-3.2.2/share/hadoop/yarn/hadoop-yarn-services-core-3.2.2.jar:/opt/hadoop-3.2.2/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.2.2.jar:/opt/hadoop-3.2.2/share/hadoop/yarn/hadoop-yarn-api-3.2.2.jar:/opt/hadoop-3.2.2/share/hadoop/yarn/hadoop-yarn-server-tests-3.2.2.jar:/opt/hadoop-3.2.2/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.2.2.jar:/opt/hadoop-3.2.2/share/hadoop/yarn/hadoop-yarn-submarine-3.2.2.jar:/opt/hadoop-3.2.2/share/hadoop/yarn/hadoop-yarn-server-common-3.2.2.jar:/opt/hadoop-3.2.2/share/hadoop/yarn/hadoop-yarn-services-api-3.2.2.jar:/opt/hadoop-3.2.2/share/hadoop/yarn/hadoop-yarn-common-3.2.2.jar:/opt/hadoop-3.2.2/share/hadoop/yarn/hadoop-yarn-server-router-3.2.2.jar:/opt/hadoop-3.2.2/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.2.2.jar:/opt/hadoop-3.2.2/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.2.2.jar:/opt/hadoop-3.2.2/share/hadoop/yarn/hadoop-yarn-registry-3.2.2.jar:/opt/hadoop-3.2.2/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.2.2.jar:/opt/hadoop-3.2.2/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.2.2.jar:/opt/hadoop-3.2.2/share/hadoop/yarn/hadoop-yarn-client-3.2.2.jar:/opt/hadoop-3.2.2/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.2.2.jar:/opt/hadoop-3.2.2/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.2.2.jar\n",
            "STARTUP_MSG:   build = Unknown -r 7a3bc90b05f257c8ace2f76d74264906f0f7a932; compiled by 'hexiaoqiao' on 2021-01-03T09:26Z\n",
            "STARTUP_MSG:   java = 1.8.0_282\n",
            "************************************************************/\n",
            "2021-03-29 13:35:09,127 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\n",
            "2021-03-29 13:35:09,265 INFO namenode.NameNode: createNameNode [-format]\n",
            "Formatting using clusterid: CID-ac126d77-fbbc-42ea-97bd-8c5ce6b0f3ae\n",
            "2021-03-29 13:35:09,883 INFO namenode.FSEditLog: Edit logging is async:true\n",
            "2021-03-29 13:35:09,922 INFO namenode.FSNamesystem: KeyProvider: null\n",
            "2021-03-29 13:35:09,925 INFO namenode.FSNamesystem: fsLock is fair: true\n",
            "2021-03-29 13:35:09,925 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\n",
            "2021-03-29 13:35:09,933 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\n",
            "2021-03-29 13:35:09,933 INFO namenode.FSNamesystem: supergroup          = supergroup\n",
            "2021-03-29 13:35:09,933 INFO namenode.FSNamesystem: isPermissionEnabled = true\n",
            "2021-03-29 13:35:09,933 INFO namenode.FSNamesystem: HA Enabled: false\n",
            "2021-03-29 13:35:09,990 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\n",
            "2021-03-29 13:35:10,000 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\n",
            "2021-03-29 13:35:10,000 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\n",
            "2021-03-29 13:35:10,004 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\n",
            "2021-03-29 13:35:10,005 INFO blockmanagement.BlockManager: The block deletion will start around 2021 Mar 29 13:35:10\n",
            "2021-03-29 13:35:10,006 INFO util.GSet: Computing capacity for map BlocksMap\n",
            "2021-03-29 13:35:10,006 INFO util.GSet: VM type       = 64-bit\n",
            "2021-03-29 13:35:10,007 INFO util.GSet: 2.0% max memory 2.8 GB = 57.9 MB\n",
            "2021-03-29 13:35:10,007 INFO util.GSet: capacity      = 2^23 = 8388608 entries\n",
            "2021-03-29 13:35:10,048 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled\n",
            "2021-03-29 13:35:10,048 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\n",
            "2021-03-29 13:35:10,054 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\n",
            "2021-03-29 13:35:10,054 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\n",
            "2021-03-29 13:35:10,054 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\n",
            "2021-03-29 13:35:10,054 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\n",
            "2021-03-29 13:35:10,055 INFO blockmanagement.BlockManager: defaultReplication         = 1\n",
            "2021-03-29 13:35:10,055 INFO blockmanagement.BlockManager: maxReplication             = 512\n",
            "2021-03-29 13:35:10,055 INFO blockmanagement.BlockManager: minReplication             = 1\n",
            "2021-03-29 13:35:10,055 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\n",
            "2021-03-29 13:35:10,055 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\n",
            "2021-03-29 13:35:10,056 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\n",
            "2021-03-29 13:35:10,056 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\n",
            "2021-03-29 13:35:10,082 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911\n",
            "2021-03-29 13:35:10,082 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215\n",
            "2021-03-29 13:35:10,083 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215\n",
            "2021-03-29 13:35:10,083 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215\n",
            "2021-03-29 13:35:10,097 INFO util.GSet: Computing capacity for map INodeMap\n",
            "2021-03-29 13:35:10,097 INFO util.GSet: VM type       = 64-bit\n",
            "2021-03-29 13:35:10,098 INFO util.GSet: 1.0% max memory 2.8 GB = 28.9 MB\n",
            "2021-03-29 13:35:10,098 INFO util.GSet: capacity      = 2^22 = 4194304 entries\n",
            "2021-03-29 13:35:10,100 INFO namenode.FSDirectory: ACLs enabled? false\n",
            "2021-03-29 13:35:10,100 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\n",
            "2021-03-29 13:35:10,100 INFO namenode.FSDirectory: XAttrs enabled? true\n",
            "2021-03-29 13:35:10,100 INFO namenode.NameNode: Caching file names occurring more than 10 times\n",
            "2021-03-29 13:35:10,106 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536\n",
            "2021-03-29 13:35:10,108 INFO snapshot.SnapshotManager: SkipList is disabled\n",
            "2021-03-29 13:35:10,114 INFO util.GSet: Computing capacity for map cachedBlocks\n",
            "2021-03-29 13:35:10,114 INFO util.GSet: VM type       = 64-bit\n",
            "2021-03-29 13:35:10,114 INFO util.GSet: 0.25% max memory 2.8 GB = 7.2 MB\n",
            "2021-03-29 13:35:10,115 INFO util.GSet: capacity      = 2^20 = 1048576 entries\n",
            "2021-03-29 13:35:10,123 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\n",
            "2021-03-29 13:35:10,124 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\n",
            "2021-03-29 13:35:10,124 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\n",
            "2021-03-29 13:35:10,129 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\n",
            "2021-03-29 13:35:10,129 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\n",
            "2021-03-29 13:35:10,131 INFO util.GSet: Computing capacity for map NameNodeRetryCache\n",
            "2021-03-29 13:35:10,131 INFO util.GSet: VM type       = 64-bit\n",
            "2021-03-29 13:35:10,132 INFO util.GSet: 0.029999999329447746% max memory 2.8 GB = 889.2 KB\n",
            "2021-03-29 13:35:10,132 INFO util.GSet: capacity      = 2^17 = 131072 entries\n",
            "2021-03-29 13:35:10,161 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1440474771-172.28.0.2-1617024910153\n",
            "2021-03-29 13:35:10,182 INFO common.Storage: Storage directory /app/hadoop/tmp/dfs/name has been successfully formatted.\n",
            "2021-03-29 13:35:10,218 INFO namenode.FSImageFormatProtobuf: Saving image file /app/hadoop/tmp/dfs/name/current/fsimage.ckpt_0000000000000000000 using no compression\n",
            "2021-03-29 13:35:10,373 INFO namenode.FSImageFormatProtobuf: Image file /app/hadoop/tmp/dfs/name/current/fsimage.ckpt_0000000000000000000 of size 399 bytes saved in 0 seconds .\n",
            "2021-03-29 13:35:10,390 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\n",
            "2021-03-29 13:35:10,396 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.\n",
            "2021-03-29 13:35:10,396 INFO namenode.NameNode: SHUTDOWN_MSG: \n",
            "/************************************************************\n",
            "SHUTDOWN_MSG: Shutting down NameNode at f28419ec4842/172.28.0.2\n",
            "************************************************************/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlT89iZz_aeh",
        "outputId": "c47b6511-04f9-40aa-e6ca-11ab63b74f16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 11.0 Start namenode\n",
        "#      If this fails, run\n",
        "#       ssh_install() below\n",
        "#        and start hadoop again:\n",
        "\n",
        "print(\"\\n---25. Start namenode----\\n\")\n",
        "! start-dfs.sh"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "---25. Start namenode----\n",
            "\n",
            "Starting namenodes on [localhost]\n",
            "localhost: Warning: Permanently added 'localhost' (ECDSA) to the list of known hosts.\n",
            "Starting datanodes\n",
            "Starting secondary namenodes [f28419ec4842]\n",
            "f28419ec4842: Warning: Permanently added 'f28419ec4842,172.28.0.2' (ECDSA) to the list of known hosts.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-8I9JsaHSOe"
      },
      "source": [
        "# ssh_install()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UI-wzZD6ccBM"
      },
      "source": [
        "If `start-dfs.sh` fails, issue the following three commands, one after another:<br>  \n",
        "`! sudo apt-get remove openssh-client openssh-server`<br>\n",
        "`! sudo apt-get install openssh-client openssh-server`<br>\n",
        "`! service ssh restart`<br>\n",
        "\n",
        "And then try to start hadoop again, as: `start-dfs.sh`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMt89uLNCUVC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05ac8fb3-5f0e-4b46-ed85-cf1641a9f371"
      },
      "source": [
        "print(\"\\n---26. Make folders in hadoop----\\n\")\n",
        "! hdfs dfs -mkdir /user\n",
        "! hdfs dfs -mkdir /user/ashok"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "---26. Make folders in hadoop----\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ez0FPw3CtOi",
        "outputId": "a83a38eb-a258-441d-cb74-e38cb9eeed13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Run hadoop commands\n",
        "! hdfs dfs -ls /\n",
        "! hdfs dfs -ls /user"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1 items\n",
            "drwxr-xr-x   - root supergroup          0 2021-03-29 13:35 /user\n",
            "Found 1 items\n",
            "drwxr-xr-x   - root supergroup          0 2021-03-29 13:35 /user/ashok\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-fSbrEWH643"
      },
      "source": [
        "#!stop-dfs.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwBD74dadYfz"
      },
      "source": [
        "Run the following commands if hadoop fails to start with `start-dfs.sh` then try again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rkvPrXrZgey"
      },
      "source": [
        "################# I am done #############"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}