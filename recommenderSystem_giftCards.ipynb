{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "recommenderSystem_giftCards.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Y3vw_quGSgrW",
        "yC40UHIKPJEL",
        "w8cVWRNoPOS6",
        "mfFBQ0FLcolo",
        "IJNKSScPcsDS",
        "Y159J8TDc3wS",
        "7M2kWg3dc6FT",
        "MYRPnQhTRdOn",
        "k5Kb-o82T0oQ",
        "Obzrn0jTVslw",
        "L6XYg1IGs17n",
        "QxLQzJD4p81p"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harnalashok/hadoop/blob/main/recommenderSystem_giftCards.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4q64Z8IYb8fr"
      },
      "source": [
        "# Last amended: 15th October, 2021\n",
        "# Myfolder: github/hadoop\n",
        "# Objectives:\n",
        "#             i)  Develop ALS based recommender system\n",
        "#             ii) Tune grid parameters\n",
        "#\n",
        "# Refer:\n",
        "# https://spark.apache.org/docs/2.2.0/ml-collaborative-filtering.html\n",
        "# https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/2799933550853697/2823893187441173/2202577924924539/latest.html\n",
        "#\n",
        "# Data Source(s):\n",
        "# https://nijianmo.github.io/amazon/index.html\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuuFSunM_rL2"
      },
      "source": [
        "# Spark Reference API\n",
        "a. [Quickstart](https://spark.apache.org/docs/latest/api/python/getting_started/quickstart.html) <br>\n",
        "b. Dataframe [APIs list](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#dataframe-apis) at a glance<br>\n",
        "c. ALso look at useful [this source code](https://spark.apache.org/docs/latest/api/python/_modules/pyspark/sql/functions.html) of functions that has examples\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3vw_quGSgrW"
      },
      "source": [
        "# A. Full spark install"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yC40UHIKPJEL"
      },
      "source": [
        "### 1.0 Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBVUrlkidyaE"
      },
      "source": [
        "# 1.0 How to set environment variable\n",
        "import os  \n",
        "import time  "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8cVWRNoPOS6"
      },
      "source": [
        "## 2.0 Define some functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfFBQ0FLcolo"
      },
      "source": [
        "#### ssh_install()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgS9HNCyR7C0"
      },
      "source": [
        "# 2.0 Function to install ssh client and sshd (Server)\n",
        "def ssh_install():\n",
        "  print(\"\\n--1. Download and install ssh server----\\n\")\n",
        "  ! sudo apt-get remove openssh-client openssh-server\n",
        "  ! sudo apt install openssh-client openssh-server\n",
        "  \n",
        "  print(\"\\n--2. Restart ssh server----\\n\")\n",
        "  ! service ssh restart"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJNKSScPcsDS"
      },
      "source": [
        "#### Java install"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsFu84PSR9jR"
      },
      "source": [
        "# 3.0 Function to download and install java 8\n",
        "def install_java():\n",
        "  ! rm -rf /usr/java\n",
        "\n",
        "  print(\"\\n--Download and install Java 8----\\n\")\n",
        "  !apt-get install -y openjdk-8-jdk-headless -qq > /dev/null        # install openjdk\n",
        "  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"     # set environment variable\n",
        "\n",
        "  !update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java\n",
        "  !update-alternatives --set javac /usr/lib/jvm/java-8-openjdk-amd64/bin/javac\n",
        "  \n",
        "  !mkdir -p /usr/java\n",
        "  ! ln -s \"/usr/lib/jvm/java-8-openjdk-amd64\"  \"/usr/java\"\n",
        "  ! mv \"/usr/java/java-8-openjdk-amd64\"  \"/usr/java/latest\"\n",
        "  \n",
        "  !java -version       #check java version\n",
        "  !javac -version"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y159J8TDc3wS"
      },
      "source": [
        "#### setup ssh passphrase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOFbfw7n0Pps"
      },
      "source": [
        "# 6.0 Function tp setup ssh passphrase\n",
        "def set_keys():\n",
        "  print(\"\\n---22. Generate SSH keys----\\n\")\n",
        "  ! cd ~ ; pwd \n",
        "  ! cd ~ ; ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa\n",
        "  ! cd ~ ; cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\n",
        "  ! cd ~ ; chmod 0600 ~/.ssh/authorized_keys\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M2kWg3dc6FT"
      },
      "source": [
        "#### Set environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRSn9XAV4rsR"
      },
      "source": [
        "# 7.0 Function to set up environmental variables\n",
        "def set_env():\n",
        "  print(\"\\n---23. Set Environment variables----\\n\")\n",
        "  # 'export' command does not work in colab\n",
        "  # https://stackoverflow.com/a/57240319\n",
        "  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"     #set environment variable\n",
        "  os.environ[\"JRE_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64/jre\"   \n",
        "  "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WII-UNCzc9qJ"
      },
      "source": [
        "#### function to install prerequisites\n",
        "java and ssh<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oh1Mi0rHFpkU"
      },
      "source": [
        "# 8.0 Function to call all functions\n",
        "def install_components():\n",
        "  print(\"\\n--Install java----\\n\")\n",
        "  ssh_install()\n",
        "  install_java()  \n",
        "  #set_keys()\n",
        "  set_env()\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHJyHMhUdCRZ"
      },
      "source": [
        "## 3.0 Install components\n",
        "Start downloading, install and configure. Takes around 2 minutes<br>\n",
        "Your <u>input *'y'* is required </u>at one place while overwriting earlier ssh keys"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77YQikvsJiTm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5da1918d-7067-4dc5-c6fe-7f223b9f2e9f"
      },
      "source": [
        "# 9.0 Start installation\n",
        "start = time.time()\n",
        "install_components()\n",
        "end = time.time()\n",
        "print(\"\\n---Time taken----\\n\")\n",
        "print((end- start)/60)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--Install java----\n",
            "\n",
            "\n",
            "--1. Download and install ssh server----\n",
            "\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following packages were automatically installed and are no longer required:\n",
            "  ncurses-term python3-certifi python3-chardet python3-idna\n",
            "  python3-pkg-resources python3-requests python3-six python3-urllib3\n",
            "Use 'sudo apt autoremove' to remove them.\n",
            "The following packages will be REMOVED:\n",
            "  openssh-client openssh-server openssh-sftp-server ssh-import-id\n",
            "0 upgraded, 0 newly installed, 4 to remove and 37 not upgraded.\n",
            "After this operation, 5,240 kB disk space will be freed.\n",
            "(Reading database ... 158402 files and directories currently installed.)\n",
            "Removing openssh-server (1:7.6p1-4ubuntu0.5) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of stop.\n",
            "Removing ssh-import-id (5.7-0ubuntu1.1) ...\n",
            "Removing openssh-sftp-server (1:7.6p1-4ubuntu0.5) ...\n",
            "Removing openssh-client (1:7.6p1-4ubuntu0.5) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  openssh-sftp-server ssh-import-id\n",
            "Suggested packages:\n",
            "  keychain libpam-ssh monkeysphere ssh-askpass molly-guard rssh ufw\n",
            "The following NEW packages will be installed:\n",
            "  openssh-client openssh-server openssh-sftp-server ssh-import-id\n",
            "0 upgraded, 4 newly installed, 0 to remove and 37 not upgraded.\n",
            "Need to get 1,001 kB of archives.\n",
            "After this operation, 5,240 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 openssh-client amd64 1:7.6p1-4ubuntu0.5 [612 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 openssh-sftp-server amd64 1:7.6p1-4ubuntu0.5 [45.5 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 openssh-server amd64 1:7.6p1-4ubuntu0.5 [332 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 ssh-import-id all 5.7-0ubuntu1.1 [10.9 kB]\n",
            "Fetched 1,001 kB in 1s (1,115 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 4.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package openssh-client.\n",
            "(Reading database ... 158323 files and directories currently installed.)\n",
            "Preparing to unpack .../openssh-client_1%3a7.6p1-4ubuntu0.5_amd64.deb ...\n",
            "Unpacking openssh-client (1:7.6p1-4ubuntu0.5) ...\n",
            "Selecting previously unselected package openssh-sftp-server.\n",
            "Preparing to unpack .../openssh-sftp-server_1%3a7.6p1-4ubuntu0.5_amd64.deb ...\n",
            "Unpacking openssh-sftp-server (1:7.6p1-4ubuntu0.5) ...\n",
            "Selecting previously unselected package openssh-server.\n",
            "Preparing to unpack .../openssh-server_1%3a7.6p1-4ubuntu0.5_amd64.deb ...\n",
            "Unpacking openssh-server (1:7.6p1-4ubuntu0.5) ...\n",
            "Selecting previously unselected package ssh-import-id.\n",
            "Preparing to unpack .../ssh-import-id_5.7-0ubuntu1.1_all.deb ...\n",
            "Unpacking ssh-import-id (5.7-0ubuntu1.1) ...\n",
            "Setting up openssh-client (1:7.6p1-4ubuntu0.5) ...\n",
            "Setting up ssh-import-id (5.7-0ubuntu1.1) ...\n",
            "Setting up openssh-sftp-server (1:7.6p1-4ubuntu0.5) ...\n",
            "Setting up openssh-server (1:7.6p1-4ubuntu0.5) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of restart.\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for systemd (237-3ubuntu10.52) ...\n",
            "\n",
            "--2. Restart ssh server----\n",
            "\n",
            " * Restarting OpenBSD Secure Shell server sshd\n",
            "   ...done.\n",
            "\n",
            "--Download and install Java 8----\n",
            "\n",
            "openjdk version \"1.8.0_292\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_292-8u292-b10-0ubuntu1~18.04-b10)\n",
            "OpenJDK 64-Bit Server VM (build 25.292-b10, mixed mode)\n",
            "javac 1.8.0_292\n",
            "\n",
            "---23. Set Environment variables----\n",
            "\n",
            "\n",
            "---Time taken----\n",
            "\n",
            "0.21586336692174277\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBFVWWuafLBL"
      },
      "source": [
        "## 4.0 Install spark\n",
        "koalas will also be installed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyuLCFRJsvW3"
      },
      "source": [
        "### Define functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kk3BP0OfYpT1"
      },
      "source": [
        "`findspark`: PySpark isn't on `sys.path` by default, but that doesn't mean it can't be used as a regular library. You can address this by either symlinking pyspark into your site-packages, or adding `pyspark` to `sys.path` at runtime. `findspark` does the latter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opLGqtPRfM_5"
      },
      "source": [
        "# 1.0 Function to download and unzip spark\n",
        "def spark_koalas_install():\n",
        "  print(\"\\n--1.1 Install findspark----\\n\")\n",
        "  !pip install -q findspark\n",
        "\n",
        "  print(\"\\n--1.2 Install databricks Koalas----\\n\")\n",
        "  !pip install koalas\n",
        "  \n",
        "  # This download link NEEDS TO BE CHECKED AGAIN\n",
        "  print(\"\\n--1.3 Download Apache tar.gz----\\n\")\n",
        "  ! wget -c https://dlcdn.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz\n",
        "\n",
        "  print(\"\\n--1.4 Transfer downloaded content and unzip tar.gz----\\n\")\n",
        "  !  mv /content/spark*   /opt/\n",
        "  ! tar -xzf /opt/spark-3.1.2-bin-hadoop3.2.tgz  --directory /opt/\n",
        "\n",
        "  print(\"\\n--1.5 Check folder for files----\\n\")\n",
        "  ! ls -la /opt\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebXfvQ1qiQHz"
      },
      "source": [
        "# 1.1 Function to set environment\n",
        "def set_spark_env():\n",
        "  print(\"\\n---2. Set Environment variables----\\n\")\n",
        "  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\" \n",
        "  os.environ[\"JRE_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64/jre\" \n",
        "  os.environ[\"SPARK_HOME\"] = \"/opt/spark-3.1.2-bin-hadoop3.2\" \n",
        "  os.environ[\"SPARK_CONF_DIR\"] = \"/opt/spark-3.1.2-bin-hadoop3.2/conf\"     \n",
        "  os.environ[\"LD_LIBRARY_PATH\"] += \":/opt/spark-3.1.2-bin-hadoop3.2/lib/native\"\n",
        "  os.environ[\"PATH\"] += \":/opt/spark-3.1.2-bin-hadoop3.2/bin:/opt/spark-3.1.2-bin-hadoop3.2/sbin\"\n",
        "  print(\"\\n---2.1. Check Environment variables----\\n\")\n",
        "  # Check\n",
        "  ! echo $PATH\n",
        "  ! echo $LD_LIBRARY_PATH"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUsZggDHj13U"
      },
      "source": [
        "# 1.2 Function to configure spark \n",
        "def spark_conf():\n",
        "  print(\"\\n---3. Configure spark to access hadoop----\\n\")\n",
        "  !mv /opt/spark-3.1.2-bin-hadoop3.2/conf/spark-env.sh.template  /opt/spark-3.1.2-bin-hadoop3.2/conf/spark-env.sh\n",
        "  #!echo \"HADOOP_CONF_DIR=/opt/hadoop-3.2.2/etc/hadoop/\" >> /opt/spark-3.1.2-bin-hadoop3.2/conf/spark-env.sh\n",
        "  print(\"\\n---3.1 Check ----\\n\")\n",
        "  #!cat /opt/spark-3.1.1-bin-hadoop3.2/conf/spark-env.sh"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7kLbAFLszN3"
      },
      "source": [
        "### Install spark\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_eaLhtPktHJ"
      },
      "source": [
        "# 2.0 Call all the three functions\n",
        "def install_spark():\n",
        "  spark_koalas_install()\n",
        "  set_spark_env()\n",
        "  spark_conf()\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emaHs1XxRt5z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90f77510-2a06-4c9a-cc45-665176e42550"
      },
      "source": [
        "# 2.1 \n",
        "install_spark()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--1.1 Install findspark----\n",
            "\n",
            "\n",
            "--1.2 Install databricks Koalas----\n",
            "\n",
            "Requirement already satisfied: koalas in /usr/local/lib/python3.7/dist-packages (1.8.1)\n",
            "Requirement already satisfied: pyarrow>=0.10 in /usr/local/lib/python3.7/dist-packages (from koalas) (3.0.0)\n",
            "Requirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.7/dist-packages (from koalas) (1.19.5)\n",
            "Requirement already satisfied: pandas>=0.23.2 in /usr/local/lib/python3.7/dist-packages (from koalas) (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.2->koalas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.2->koalas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.23.2->koalas) (1.15.0)\n",
            "\n",
            "--1.3 Download Apache tar.gz----\n",
            "\n",
            "--2021-10-15 13:04:01--  https://dlcdn.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz\n",
            "Resolving dlcdn.apache.org (dlcdn.apache.org)... 151.101.2.132, 2a04:4e42::644\n",
            "Connecting to dlcdn.apache.org (dlcdn.apache.org)|151.101.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228834641 (218M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.1.2-bin-hadoop3.2.tgz’\n",
            "\n",
            "spark-3.1.2-bin-had 100%[===================>] 218.23M   142MB/s    in 1.5s    \n",
            "\n",
            "2021-10-15 13:04:03 (142 MB/s) - ‘spark-3.1.2-bin-hadoop3.2.tgz’ saved [228834641/228834641]\n",
            "\n",
            "\n",
            "--1.4 Transfer downloaded content and unzip tar.gz----\n",
            "\n",
            "\n",
            "--1.5 Check folder for files----\n",
            "\n",
            "total 223492\n",
            "drwxr-xr-x  1 root root      4096 Oct 15 13:04 .\n",
            "drwxr-xr-x  1 root root      4096 Oct 15 09:20 ..\n",
            "drwxr-xr-x  1 root root      4096 Oct  8 13:39 google\n",
            "drwxr-xr-x  4 root root      4096 Oct  8 13:31 nvidia\n",
            "drwxr-xr-x 13 1000 1000      4096 May 24 04:45 spark-3.1.2-bin-hadoop3.2\n",
            "-rw-r--r--  1 root root 228834641 May 24 05:01 spark-3.1.2-bin-hadoop3.2.tgz\n",
            "\n",
            "---2. Set Environment variables----\n",
            "\n",
            "\n",
            "---2.1. Check Environment variables----\n",
            "\n",
            "/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin:/opt/bin:/opt/spark-3.1.2-bin-hadoop3.2/bin:/opt/spark-3.1.2-bin-hadoop3.2/sbin\n",
            "/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/opt/spark-3.1.2-bin-hadoop3.2/lib/native\n",
            "\n",
            "---3. Configure spark to access hadoop----\n",
            "\n",
            "\n",
            "---3.1 Check ----\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYRPnQhTRdOn"
      },
      "source": [
        "# B. Call libraries\n",
        "We call some essential libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiO4shcjRjNO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5be295f0-4101-4e12-96a9-acd518030225"
      },
      "source": [
        "# 3.0 Just call some libraries to test\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import time \n",
        "\n",
        "# 3.1 Get spark in sys.path\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "# 3.2 Call other spark libraries\n",
        "#     Just to test\n",
        "from pyspark.sql import SparkSession\n",
        "import databricks.koalas as ks"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. Koalas will set it for you but it does not work if there is a Spark context already launched.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7lb_6xCwrZm"
      },
      "source": [
        "# 3.3\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\""
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCOPgKGO60_I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "e8bd08a1-d9ba-4c43-9224-89dd1e7d82f6"
      },
      "source": [
        "# 3.4 Increase cell width to display wide columnar output\n",
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>.container { width:100% !important; }</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHd3YFPHO1SV"
      },
      "source": [
        "# C. Build spark session\n",
        "You can modify spark driver/executor memory here<br>\n",
        "SparkSession name is 'spark'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5Kb-o82T0oQ"
      },
      "source": [
        "## Modifying spark configuraion\n",
        "Increase driver and executor memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3M6d5xfNJSwx",
        "outputId": "7964aa74-668f-425a-fe84-b48778a2432b"
      },
      "source": [
        "# 4.0 Check template file\n",
        "! cat /opt/spark-3.1.2-bin-hadoop3.2/conf/spark-defaults.conf.template"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#\n",
            "# Licensed to the Apache Software Foundation (ASF) under one or more\n",
            "# contributor license agreements.  See the NOTICE file distributed with\n",
            "# this work for additional information regarding copyright ownership.\n",
            "# The ASF licenses this file to You under the Apache License, Version 2.0\n",
            "# (the \"License\"); you may not use this file except in compliance with\n",
            "# the License.  You may obtain a copy of the License at\n",
            "#\n",
            "#    http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "#\n",
            "\n",
            "# Default system properties included when running spark-submit.\n",
            "# This is useful for setting default environmental settings.\n",
            "\n",
            "# Example:\n",
            "# spark.master                     spark://master:7077\n",
            "# spark.eventLog.enabled           true\n",
            "# spark.eventLog.dir               hdfs://namenode:8021/directory\n",
            "# spark.serializer                 org.apache.spark.serializer.KryoSerializer\n",
            "# spark.driver.memory              5g\n",
            "# spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers=\"one two three\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXEkJxzyNYn4"
      },
      "source": [
        "# 4.1 Create spark-defaults.conf \n",
        "! cp /opt/spark-3.1.2-bin-hadoop3.2/conf/spark-defaults.conf.template  /opt/spark-3.1.2-bin-hadoop3.2/conf/spark-defaults.conf\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZD3i2MXNacY"
      },
      "source": [
        "# 4.2 Amend properties\n",
        "! echo \"spark.driver.memory 6g\" >> /opt/spark-3.1.2-bin-hadoop3.2/conf/spark-defaults.conf\n",
        "! echo \"spark.executor.memory 3g\" >> /opt/spark-3.1.2-bin-hadoop3.2/conf/spark-defaults.conf"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtFbJ5UEObIg",
        "outputId": "590006ea-5894-4536-c352-f559f5d76ac3"
      },
      "source": [
        "# 4.3 Check now\n",
        "! cat /opt/spark-3.1.2-bin-hadoop3.2/conf/spark-defaults.conf"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#\n",
            "# Licensed to the Apache Software Foundation (ASF) under one or more\n",
            "# contributor license agreements.  See the NOTICE file distributed with\n",
            "# this work for additional information regarding copyright ownership.\n",
            "# The ASF licenses this file to You under the Apache License, Version 2.0\n",
            "# (the \"License\"); you may not use this file except in compliance with\n",
            "# the License.  You may obtain a copy of the License at\n",
            "#\n",
            "#    http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "#\n",
            "\n",
            "# Default system properties included when running spark-submit.\n",
            "# This is useful for setting default environmental settings.\n",
            "\n",
            "# Example:\n",
            "# spark.master                     spark://master:7077\n",
            "# spark.eventLog.enabled           true\n",
            "# spark.eventLog.dir               hdfs://namenode:8021/directory\n",
            "# spark.serializer                 org.apache.spark.serializer.KryoSerializer\n",
            "# spark.driver.memory              5g\n",
            "# spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers=\"one two three\"\n",
            "spark.driver.memory 6g\n",
            "spark.executor.memory 3g\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Obzrn0jTVslw"
      },
      "source": [
        "## Stop and start SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDGr6SWsOqox"
      },
      "source": [
        "# 5.0 Build spark session:\n",
        "#    Stop spark, if started\n",
        "\n",
        "if 'spark' in locals():\n",
        "  spark.stop()\n",
        "\n",
        "# 5.1 Now start spark\n",
        "spark = SparkSession. \\\n",
        "                    builder. \\\n",
        "                    master(\"local[*]\"). \\\n",
        "                    appName(\"myexpt\"). \\\n",
        "                    getOrCreate()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFOjGwWiaNZH",
        "outputId": "ceece912-b28d-4865-cedf-8acd870c05bb"
      },
      "source": [
        "sc = spark.sparkContext\n",
        "spark.sparkContext.getConf().getAll()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('spark.executor.memory', '3g'),\n",
              " ('spark.app.name', 'myexpt'),\n",
              " ('spark.executor.id', 'driver'),\n",
              " ('spark.sql.warehouse.dir', 'file:/content/spark-warehouse'),\n",
              " ('spark.app.startTime', '1634303391906'),\n",
              " ('spark.driver.memory', '6g'),\n",
              " ('spark.driver.port', '33101'),\n",
              " ('spark.rdd.compress', 'True'),\n",
              " ('spark.serializer.objectStreamReset', '100'),\n",
              " ('spark.master', 'local[*]'),\n",
              " ('spark.submit.pyFiles', ''),\n",
              " ('spark.submit.deployMode', 'client'),\n",
              " ('spark.driver.host', '90202ab9013d'),\n",
              " ('spark.app.id', 'local-1634303392022'),\n",
              " ('spark.ui.showConsoleProgress', 'true')]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jimCCywmPJmm",
        "outputId": "4ca14438-a6ef-43af-aee3-6c1430be6560"
      },
      "source": [
        "# 5.2\n",
        "print(spark.sparkContext._conf.getAll())"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('spark.executor.memory', '3g'), ('spark.app.name', 'myexpt'), ('spark.executor.id', 'driver'), ('spark.sql.warehouse.dir', 'file:/content/spark-warehouse'), ('spark.app.startTime', '1634303391906'), ('spark.driver.memory', '6g'), ('spark.driver.port', '33101'), ('spark.rdd.compress', 'True'), ('spark.serializer.objectStreamReset', '100'), ('spark.master', 'local[*]'), ('spark.submit.pyFiles', ''), ('spark.submit.deployMode', 'client'), ('spark.driver.host', '90202ab9013d'), ('spark.app.id', 'local-1634303392022'), ('spark.ui.showConsoleProgress', 'true')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6XYg1IGs17n"
      },
      "source": [
        "# D. Test spark\n",
        "Use existing *spark* session to test if spark is working\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVlRiGQJmk58"
      },
      "source": [
        "# 6.0 Pandas DataFrame\n",
        "pdf = pd.DataFrame({\n",
        "        'x1': ['a','a','b','b','', 'c', 'd','d'],\n",
        "        'x2': ['apple','', 'orange','orange', 'peach','','apple','orange'],\n",
        "        'x3': [1, 1, 2, 2, 2, 4, 1, 2],\n",
        "        'x4': [2.4, 2.5, 3.5, 1.4, 2.1,1.5, 3.0, 2.0],\n",
        "        'y1': [1, 0, 1, 0, 0, 1, 1, 0],\n",
        "        'y2': ['yes', 'no', 'no','','', 'yes','', 'yes']\n",
        "    })\n",
        "\n",
        "# 6.1\n",
        "pdf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3pSQ7sDmnjt"
      },
      "source": [
        "# 6.2 Transform to Spark DataFrame\n",
        "#     and print\n",
        "df = spark.createDataFrame(pdf)\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTjT6sQYbgz3"
      },
      "source": [
        "############"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxLQzJD4p81p"
      },
      "source": [
        "# E. Some useful functions\n",
        "Execute only if you need. Else, forget it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYwZ4OJfp_tp"
      },
      "source": [
        "# 7.0 Per column how many null values:\n",
        "from pyspark.sql.functions import isnan, when, count, col\n",
        "def null_values(data):\n",
        "  data.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in data.columns]).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8GzRFOIUKoj"
      },
      "source": [
        "# 8.0 Finding mode of a column\n",
        "# Refer StackOverflow: https://stackoverflow.com/a/58279672\n",
        "def mode(df,col):\n",
        "  df.groupby(\"col\").count().orderBy(\"count\", ascending=False).first()[0]\n",
        "\n",
        "# 9.0 Find mode of all columns\n",
        "def mode_cols(df):\n",
        "  [[i,df.groupby(i).count().orderBy(\"count\", ascending=False).first()[0]] for i in df.columns]  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbIOI7qpebYo"
      },
      "source": [
        "# 10.0 Value counts\n",
        "def value_counts(df):\n",
        "    for colm in df.columns:\n",
        "        df.groupby(colm).count().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQYbaYIBW-BS"
      },
      "source": [
        "# 11.0 Map a string to another\n",
        "# See here: https://stackoverflow.com/a/55026324/3282777\n",
        "# Code not tested\n",
        "# Map a column 'colname' in dataframe 'df'\n",
        "# as follows:\n",
        "#\n",
        "# map_dict= {\n",
        "#            'A': '1',\n",
        "#            'B': '2'\n",
        "#           }\n",
        "\n",
        "def mapping(df,map_dict, colname):\n",
        "  df2 = df.replace(to_replace=map_dict, subset=['yourColName'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zn3xhm6LdXvK"
      },
      "source": [
        "# F. Your experiments\n",
        "SparkSession is <i>'spark'</i>. Call all needed libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQ2e6elQoj5z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "400b47b5-2070-4011-93e9-ecec6598b5d5"
      },
      "source": [
        "# 1.0 Mount gdrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPV0-QZbeQtQ"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pyspark.sql.functions import col, explode\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark import SparkContext"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdGVJeDCeRNb"
      },
      "source": [
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\""
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m2KdpFVeUkh"
      },
      "source": [
        "ratings = spark.read.csv(\n",
        "                         \"/content/drive/MyDrive/Colab_data_files/recommenderSystems/gift_cards.csv\",\n",
        "                         header=True\n",
        "                         )"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xylQq7CevUF",
        "outputId": "76a2afeb-5048-4301-f45e-1ecd5a1c97ae"
      },
      "source": [
        "ratings.show(3)\n",
        "ratings.printSchema()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------+------+----------+\n",
            "|      item|        userid|rating| timestamp|\n",
            "+----------+--------------+------+----------+\n",
            "|B001GXRQW0| APV13CM0919JD|   1.0|1229644800|\n",
            "|B001GXRQW0|A3G8U1G1V082SN|   5.0|1229472000|\n",
            "|B001GXRQW0| A11T2Q0EVTUWP|   5.0|1229472000|\n",
            "+----------+--------------+------+----------+\n",
            "only showing top 3 rows\n",
            "\n",
            "root\n",
            " |-- item: string (nullable = true)\n",
            " |-- userid: string (nullable = true)\n",
            " |-- rating: string (nullable = true)\n",
            " |-- timestamp: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoJ9OY5uf3b2"
      },
      "source": [
        "st = StringIndexer(\n",
        "                    inputCols = ['item', 'userid'],\n",
        "                    outputCols = ['card', 'user']\n",
        "                   )\n",
        "model = st.fit(ratings)\n",
        "ratings = model.transform(ratings)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDZaPjTw7Yd9",
        "outputId": "7815eb9d-2688-48e2-9943-ad482982b62d"
      },
      "source": [
        "ratings.show()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----+------+\n",
            "|rating|card|  user|\n",
            "+------+----+------+\n",
            "|   1.0|  39|120184|\n",
            "|   5.0|  39| 87652|\n",
            "|   5.0|  39| 13165|\n",
            "|   5.0|  39|106567|\n",
            "|   1.0|  39| 77751|\n",
            "|   3.0|  39| 44380|\n",
            "|   1.0|  39| 42655|\n",
            "|   1.0|  39| 96529|\n",
            "|   5.0|  39| 21991|\n",
            "|   1.0|  39|114512|\n",
            "|   5.0|  39| 79408|\n",
            "|   5.0|  39|102618|\n",
            "|   4.0|  39| 15821|\n",
            "|   5.0|  39| 69061|\n",
            "|   4.0|  39|  6167|\n",
            "|   5.0|  39| 44270|\n",
            "|   5.0|  39| 38757|\n",
            "|   5.0|  39|113343|\n",
            "|   1.0|  39| 53351|\n",
            "|   5.0|  39|104423|\n",
            "+------+----+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AY8JmyXUe1o2"
      },
      "source": [
        "ratings = ratings.\\\n",
        "                   withColumn('card', col('card').cast('integer')).\\\n",
        "                   withColumn('user', col('user').cast('integer')).\\\n",
        "                   withColumn('rating', col('rating').cast('float')).\\\n",
        "                   drop('timestamp').drop('item').drop('userid')\n",
        "ratings.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGmqqIZ2e7I7"
      },
      "source": [
        "# Import the required functions\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmLd5HyJfAgB",
        "outputId": "6a293fdc-6473-4b08-969f-95509eb696f3"
      },
      "source": [
        "# Create test and train set\n",
        "(train, test) = ratings.randomSplit([0.8, 0.2], seed = 1234)\n",
        "\n",
        "print(train.count())\n",
        "print(test.count())"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "117935\n",
            "29259\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynFsGjKQDFSs"
      },
      "source": [
        "train = train.cache()"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRUmv3oYER1V"
      },
      "source": [
        "assert train.is_cached"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTAMwn5XtuaB"
      },
      "source": [
        "## Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CLIgt5XfCZ0",
        "outputId": "1342f1e0-2018-4c3d-fa22-14b1ff6b2a86"
      },
      "source": [
        "# Create ALS model\n",
        "als = ALS(\n",
        "            userCol=\"user\",\n",
        "            itemCol=\"card\",\n",
        "            ratingCol=\"rating\",\n",
        "            maxIter=5, \n",
        "            #regParam=0.01,\n",
        "            nonnegative = True,\n",
        "            implicitPrefs = False,\n",
        "            coldStartStrategy=\"drop\"\n",
        "          )\n",
        "\n",
        "# Confirm that a model called \"als\" was created\n",
        "type(als)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.ml.recommendation.ALS"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFCgAtyyfJS7"
      },
      "source": [
        "# Import the requisite items\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBV_zGtMazKx",
        "outputId": "12b855ba-6c87-46f6-fd28-a2a9ad75d18a"
      },
      "source": [
        "# Add hyperparameters and their respective values to param_grid\n",
        "# May try:  .addGrid(als.regParam, [.01, .1, .15]) \\\n",
        "param_grid = ParamGridBuilder() \\\n",
        "            .addGrid(als.rank, [4, 8, 12]) \\\n",
        "            .build()\n",
        "            #             .addGrid(als.maxIter, [5, 50, 100, 200]) \\\n",
        "print (\"Num models to be tested: \", len(param_grid))"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num models to be tested:  3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXLTA1O_fNmV"
      },
      "source": [
        "# Define evaluator as RMSE and print length of evaluator\n",
        "evaluator = RegressionEvaluator(\n",
        "                                 metricName=\"rmse\",\n",
        "                                 labelCol=\"rating\",\n",
        "                                 predictionCol=\"prediction\"\n",
        "                                ) \n"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLTrRW3GfPdA",
        "outputId": "876c870c-583f-42d8-f044-048a20ae3ddb"
      },
      "source": [
        "# Build cross validation using CrossValidator\n",
        "cv = CrossValidator(\n",
        "                     estimator=als,\n",
        "                     estimatorParamMaps=param_grid,\n",
        "                     evaluator=evaluator,\n",
        "                     numFolds=3\n",
        "                    )\n",
        "\n",
        "# Confirm cv was built\n",
        "print(cv)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CrossValidator_4c319b63ea00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VLvYZ1OfRbP",
        "outputId": "a8207ca6-9321-4602-a5b2-91ac84aa9f9e"
      },
      "source": [
        "%%time\n",
        "\n",
        "#Fit cross validator to the 'train' dataset\n",
        "\n",
        "model = cv.fit(train)\n",
        "#model = als.fit(train)     # 30seconds / 10 minutes"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3.72 s, sys: 571 ms, total: 4.29 s\n",
            "Wall time: 7min 10s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06zvMYdS_t-o",
        "outputId": "196478cc-905b-4fde-b7a1-454d32b65101"
      },
      "source": [
        "# View the predictions\n",
        "%%time\n",
        "test_predictions = model.transform(test)\n",
        "RMSE = evaluator.evaluate(test_predictions)\n",
        "print(RMSE)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.5538336352667395\n",
            "CPU times: user 274 ms, sys: 43.1 ms, total: 317 ms\n",
            "Wall time: 32.5 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1eilj5gfV_h"
      },
      "source": [
        "#Extract best model from the cv model above\n",
        "best_model = model.bestModel"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heMr3e9LfZBt",
        "outputId": "86d027c5-f449-4702-f3f9-228c0370af0f"
      },
      "source": [
        "\n",
        "# Print best_model\n",
        "print(type(best_model))\n",
        "\n",
        "# Complete the code below to extract the ALS model parameters\n",
        "print(\"**Best Model**\")\n",
        "\n",
        "# # Print \"Rank\"\n",
        "print(\"  Rank:\", best_model._java_obj.parent().getRank())\n",
        "\n",
        "# Print \"MaxIter\"\n",
        "print(\"  MaxIter:\", best_model._java_obj.parent().getMaxIter())\n",
        "\n",
        "# Print \"RegParam\"\n",
        "print(\"  RegParam:\", best_model._java_obj.parent().getRegParam())"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.ml.recommendation.ALSModel'>\n",
            "**Best Model**\n",
            "  Rank: 4\n",
            "  MaxIter: 5\n",
            "  RegParam: 0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laIg7Jnufb16",
        "outputId": "2397c13d-451a-403a-dfa4-a5e3776e2651"
      },
      "source": [
        "# View the predictions\n",
        "test_predictions = best_model.transform(test)\n",
        "RMSE = evaluator.evaluate(test_predictions)\n",
        "print(RMSE)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.5538336352667395\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LCqAZ2MfeMu",
        "outputId": "81455b3c-4b74-459c-c1b9-1bb64f559170"
      },
      "source": [
        "test_predictions.show()"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----+-----+----------+\n",
            "|rating|card| user|prediction|\n",
            "+------+----+-----+----------+\n",
            "|   5.0| 148| 6732| 1.9062009|\n",
            "|   5.0| 148| 5585| 3.1085262|\n",
            "|   5.0| 148|10053|  5.093481|\n",
            "|   5.0| 148| 6618| 1.9062009|\n",
            "|   5.0| 148| 9197| 3.8480022|\n",
            "|   5.0| 148|   23| 3.3468761|\n",
            "|   5.0| 148|    7|  5.146885|\n",
            "|   4.0| 148|  277| 5.1043305|\n",
            "|   5.0| 148|  524| 4.6295757|\n",
            "|   4.0| 148| 9201| 3.3762894|\n",
            "|   5.0| 148|  730|  4.504654|\n",
            "|   5.0| 148| 2981| 4.9238777|\n",
            "|   5.0| 148| 7544| 1.9062009|\n",
            "|   5.0| 148|10717|0.47338432|\n",
            "|   5.0| 148| 1309|  4.302889|\n",
            "|   1.0| 148|   89| 2.6841044|\n",
            "|   4.0| 463|10145| 10.234779|\n",
            "|   5.0| 496| 3631| 3.3246105|\n",
            "|   5.0| 243| 7361| 2.3075113|\n",
            "|   5.0| 392| 5327| 3.8420656|\n",
            "+------+----+-----+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNsj5NPVA5Lv",
        "outputId": "c6c379d2-77f7-4d03-d135-dd3b1218eacd"
      },
      "source": [
        "# Generate top 10 movie recommendations for each user\n",
        "userRecs = best_model.recommendForAllUsers(10)\n",
        "#userRecs = model.recommendForAllUsers(10)\n",
        "\n",
        "userRecs.show()"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+--------------------+\n",
            "|user|     recommendations|\n",
            "+----+--------------------+\n",
            "| 148|[{949, 15.823175}...|\n",
            "| 463|[{1141, 16.12802}...|\n",
            "| 471|[{1175, 15.156533...|\n",
            "| 496|[{949, 14.877338}...|\n",
            "| 833|[{1141, 15.091605...|\n",
            "|1088|[{1141, 15.598734...|\n",
            "|1238|[{1141, 16.194313...|\n",
            "|1342|[{1292, 15.278643...|\n",
            "|1580|[{1253, 14.972759...|\n",
            "|1591|[{1353, 15.475293...|\n",
            "|1645|[{1141, 15.24032}...|\n",
            "|1829|[{1224, 14.170139...|\n",
            "|1959|[{1175, 15.554655...|\n",
            "|2122|[{1141, 15.24032}...|\n",
            "|2142|[{1259, 15.30538}...|\n",
            "|2366|[{1175, 15.224998...|\n",
            "|2659|[{1141, 15.612871...|\n",
            "|2866|[{1029, 12.96638}...|\n",
            "|3175|[{1141, 15.773278...|\n",
            "|3749|[{1224, 17.205673...|\n",
            "+----+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOJOtPHDA29y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}