{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harnalashok/hadoop/blob/main/hadoop_spark_install_on_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4q64Z8IYb8fr"
      },
      "source": [
        "# Last amended: 30th March, 2021\n",
        "# Myfolder: github/hadoop\n",
        "# Objective:\n",
        "#            i)  Install hadoop on colab\n",
        "#                (current version is 3.3.5)\n",
        "#            ii) Experiments with hadoop\n",
        "#           iii) Install spark on colab\n",
        "#            iv) Access hadoop file from spark\n",
        "#             v) Install koalas on colab\n",
        "#\n",
        "#\n",
        "# Java 8 install: https://stackoverflow.com/a/58191107\n",
        "# Hadoop install: https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html\n",
        "# Spark install:  https://stackoverflow.com/a/64183749\n",
        "#                 https://www.analyticsvidhya.com/blog/2020/11/a-must-read-guide-on-how-to-work-with-pyspark-on-google-colab-for-data-scientists/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDmuRsOCchey"
      },
      "source": [
        "## Install hadoop\n",
        "If it takes too long, it means, it is awaiting input from you regarding overwriting ssh keys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-v7l17LclsR"
      },
      "source": [
        "### Define functions\n",
        "No downloads. Just function definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBVUrlkidyaE"
      },
      "source": [
        "# 1.0 How to set environment variable\n",
        "import os  \n",
        "import time  "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfFBQ0FLcolo"
      },
      "source": [
        "#### ssh_install()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgS9HNCyR7C0"
      },
      "source": [
        "# 2.0 Function to install ssh client and sshd (Server)\n",
        "def ssh_install():\n",
        "  print(\"\\n--1. Download and install ssh server----\\n\")\n",
        "  ! sudo apt-get remove openssh-client openssh-server\n",
        "  ! sudo apt install openssh-client openssh-server\n",
        "  \n",
        "  print(\"\\n--2. Restart ssh server----\\n\")\n",
        "  ! service ssh restart"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJNKSScPcsDS"
      },
      "source": [
        "#### Java install"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsFu84PSR9jR"
      },
      "source": [
        "# 3.0 Function to download and install java 8\n",
        "def install_java():\n",
        "  ! rm -rf /usr/java\n",
        "\n",
        "  print(\"\\n--Download and install Java 8----\\n\")\n",
        "  !apt-get install -y openjdk-8-jdk-headless -qq > /dev/null        # install openjdk\n",
        "  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"     # set environment variable\n",
        "\n",
        "  !update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java\n",
        "  !update-alternatives --set javac /usr/lib/jvm/java-8-openjdk-amd64/bin/javac\n",
        "  \n",
        "  !mkdir -p /usr/java\n",
        "  ! ln -s \"/usr/lib/jvm/java-8-openjdk-amd64\"  \"/usr/java\"\n",
        "  ! mv \"/usr/java/java-8-openjdk-amd64\"  \"/usr/java/latest\"\n",
        "  \n",
        "  !java -version       #check java version\n",
        "  !javac -version"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuZPLoqrcvtp"
      },
      "source": [
        "#### hadoop install"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dg2fSk_ylMbr"
      },
      "source": [
        "# 4.0 Function to download and install hadoop\n",
        "def hadoop_install():\n",
        "  print(\"\\n--5. Download hadoop tar.gz----\\n\")\n",
        "  ! wget -c https://dlcdn.apache.org/hadoop/common/hadoop-3.3.5/hadoop-3.3.5.tar.gz\n",
        "\n",
        "  print(\"\\n--6. Transfer downloaded content and unzip tar.gz----\\n\")\n",
        "  !  mv /content/hadoop*   /opt/\n",
        "  ! tar -xzf /opt/hadoop-3.3.5.tar.gz  --directory /opt/\n",
        "\n",
        "  print(\"\\n--7. Create hadoop folder----\\n\")\n",
        "  ! rm -r /app/hadoop/tmp\n",
        "  ! mkdir  -p   /app/hadoop/tmp\n",
        "  \n",
        "  print(\"\\n--8. Check folder for files----\\n\")\n",
        "  ! ls -la /opt"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iPTRV2qc0Pg"
      },
      "source": [
        "#### hadoop config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SChb4dXsurlC"
      },
      "source": [
        "# 5.0 Function for setting hadoop configuration\n",
        "def hadoop_config():\n",
        "  print(\"\\n--Begin Configuring hadoop---\\n\")\n",
        "  print(\"\\n=============================\\n\")\n",
        "  print(\"\\n--9. core-site.xml----\\n\")\n",
        "  ! cat  /opt/hadoop-3.3.5/etc/hadoop/core-site.xml\n",
        "\n",
        "  print(\"\\n--10. Amend core-site.xml----\\n\")\n",
        "  !  echo  '<?xml version=\"1.0\" encoding=\"UTF-8\"?>' >  /opt/hadoop-3.3.5/etc/hadoop/core-site.xml\n",
        "  !  echo  '<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>' >>  /opt/hadoop-3.3.5/etc/hadoop/core-site.xml\n",
        "  !  echo  ' <configuration>' >>  /opt/hadoop-3.3.5/etc/hadoop/core-site.xml\n",
        "  !  echo  '    <property>' >>  /opt/hadoop-3.3.5/etc/hadoop/core-site.xml\n",
        "  !  echo  '        <name>fs.defaultFS</name>' >>  /opt/hadoop-3.3.5/etc/hadoop/core-site.xml\n",
        "  !  echo  '        <value>hdfs://localhost:9000</value>' >>  /opt/hadoop-3.3.5/etc/hadoop/core-site.xml\n",
        "  !  echo  '    </property>' >>  /opt/hadoop-3.3.5/etc/hadoop/core-site.xml\n",
        "  !  echo  '    <property>' >>  /opt/hadoop-3.3.5/etc/hadoop/core-site.xml\n",
        "  !  echo  '          <name>hadoop.tmp.dir</name>' >>  /opt/hadoop-3.3.5/etc/hadoop/core-site.xml\n",
        "  !  echo  '          <value>/app/hadoop/tmp</value>' >>  /opt/hadoop-3.3.5/etc/hadoop/core-site.xml\n",
        "  !  echo  '       <description>A base for other temporary directories.</description>' >>  /opt/hadoop-3.3.5/etc/hadoop/core-site.xml\n",
        "  !  echo  '     </property>' >>  /opt/hadoop-3.3.5/etc/hadoop/core-site.xml\n",
        "  # Added following regarding safemode from here:\n",
        "  # https://stackoverflow.com/a/33800253\n",
        "  !  echo  '     <property>'    >> /opt/hadoop-3.3.5/etc/hadoop/core-site.xml\n",
        "  !  echo  '      <name>dfs.safemode.threshold.pct</name>'  >> /opt/hadoop-3.3.5/etc/hadoop/core-site.xml\n",
        "  !  echo  '    <value>0</value>'  >> /opt/hadoop-3.3.5/etc/hadoop/core-site.xml\n",
        "  !  echo  '  </property>'  >> /opt/hadoop-3.3.5/etc/hadoop/core-site.xml\n",
        "  !  echo  '  </configuration>' >>  /opt/hadoop-3.3.5/etc/hadoop/core-site.xml\n",
        "\n",
        "  print(\"\\n--11. Amended core-site.xml----\\n\")\n",
        "  ! cat  /opt/hadoop-3.3.5/etc/hadoop/core-site.xml\n",
        "\n",
        "  print(\"\\n--12. yarn-site.xml----\\n\")\n",
        "  !cat /opt/hadoop-3.3.5/etc/hadoop/yarn-site.xml\n",
        "\n",
        "  !echo '<?xml version=\"1.0\" encoding=\"UTF-8\"?>' > /opt/hadoop-3.3.5/etc/hadoop/yarn-site.xml\n",
        "  !echo '<configuration>' >> /opt/hadoop-3.3.5/etc/hadoop/yarn-site.xml\n",
        "  !echo '    <property>' >> /opt/hadoop-3.3.5/etc/hadoop/yarn-site.xml\n",
        "  !echo '        <name>yarn.nodemanager.aux-services</name>' >> /opt/hadoop-3.3.5/etc/hadoop/yarn-site.xml\n",
        "  !echo '        <value>mapreduce_shuffle</value>' >> /opt/hadoop-3.3.5/etc/hadoop/yarn-site.xml\n",
        "  !echo '    </property>' >> /opt/hadoop-3.3.5/etc/hadoop/yarn-site.xml\n",
        "  !echo '    <property>'  >> /opt/hadoop-3.3.5/etc/hadoop/yarn-site.xml\n",
        "  !echo '       <name>yarn.nodemanager.vmem-check-enabled</name>'  >> /opt/hadoop-3.3.5/etc/hadoop/yarn-site.xml\n",
        "  !echo '       <value>false</value>'  >> /opt/hadoop-3.3.5/etc/hadoop/yarn-site.xml\n",
        "  !echo '    </property>'  >> /opt/hadoop-3.3.5/etc/hadoop/yarn-site.xml\n",
        "  !echo ' </configuration>'  >> /opt/hadoop-3.3.5/etc/hadoop/yarn-site.xml\n",
        "  \n",
        "  print(\"\\n--13. Amended yarn-site.xml----\\n\")\n",
        "  !cat /opt/hadoop-3.3.5/etc/hadoop/yarn-site.xml\n",
        "\n",
        "  print(\"\\n--14. mapred-site.xml----\\n\")\n",
        "  !cat  /opt/hadoop-3.3.5/etc/hadoop/mapred-site.xml\n",
        "\n",
        "  print(\"\\n--15. Amend mapred-site.xml----\\n\")\n",
        "  !echo '<?xml version=\"1.0\"?>'  > /opt/hadoop-3.3.5/etc/hadoop/mapred-site.xml\n",
        "  !echo '<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>'  >> /opt/hadoop-3.3.5/etc/hadoop/mapred-site.xml\n",
        "  !echo '<configuration>'  >> /opt/hadoop-3.3.5/etc/hadoop/mapred-site.xml\n",
        "  !echo '    <property>'  >> /opt/hadoop-3.3.5/etc/hadoop/mapred-site.xml\n",
        "  !echo '       <name>mapreduce.framework.name</name>'  >> /opt/hadoop-3.3.5/etc/hadoop/mapred-site.xml\n",
        "  !echo '        <value>yarn</value>'  >> /opt/hadoop-3.3.5/etc/hadoop/mapred-site.xml\n",
        "  !echo '    </property>'  >> /opt/hadoop-3.3.5/etc/hadoop/mapred-site.xml\n",
        "  !echo '    <property>'  >> /opt/hadoop-3.3.5/etc/hadoop/mapred-site.xml\n",
        "  !echo '        <name>yarn.app.mapreduce.am.env</name>'  >> /opt/hadoop-3.3.5/etc/hadoop/mapred-site.xml\n",
        "  !echo '        <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>'  >> /opt/hadoop-3.3.5/etc/hadoop/mapred-site.xml\n",
        "  !echo '    </property>'  >> /opt/hadoop-3.3.5/etc/hadoop/mapred-site.xml\n",
        "  !echo '    <property>'  >> /opt/hadoop-3.3.5/etc/hadoop/mapred-site.xml\n",
        "  !echo '       <name>mapreduce.map.env</name>'  >> /opt/hadoop-3.3.5/etc/hadoop/mapred-site.xml\n",
        "  !echo '       <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>'  >> /opt/hadoop-3.3.5/etc/hadoop/mapred-site.xml\n",
        "  !echo '    </property>'  >> /opt/hadoop-3.3.5/etc/hadoop/mapred-site.xml\n",
        "  !echo '    <property>'  >> /opt/hadoop-3.3.5/etc/hadoop/mapred-site.xml\n",
        "  !echo '      <name>mapreduce.reduce.env</name>'  >> /opt/hadoop-3.3.5/etc/hadoop/mapred-site.xml\n",
        "  !echo '      <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>'  >> /opt/hadoop-3.3.5/etc/hadoop/mapred-site.xml\n",
        "  !echo '   </property>'  >> /opt/hadoop-3.3.5/etc/hadoop/mapred-site.xml\n",
        "  !echo '</configuration>'  >> /opt/hadoop-3.3.5/etc/hadoop/mapred-site.xml\n",
        "\n",
        "  print(\"\\n--16, Amended mapred-site.xml----\\n\")\n",
        "  !cat  /opt/hadoop-3.3.5/etc/hadoop/mapred-site.xml\n",
        "\n",
        "  print(\"\\n---17. hdfs-site.xml----\\n\")\n",
        "  !cat  /opt/hadoop-3.3.5/etc/hadoop/hdfs-site.xml\n",
        "  \n",
        "  print(\"\\n---18. Amend hdfs-site.xml----\\n\")\n",
        "  !echo  '<?xml version=\"1.0\" encoding=\"UTF-8\"?> '   > /opt/hadoop-3.3.5/etc/hadoop/hdfs-site.xml\n",
        "  !echo  '<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>' >> /opt/hadoop-3.3.5/etc/hadoop/hdfs-site.xml\n",
        "  !echo  '<configuration>'  >> /opt/hadoop-3.3.5/etc/hadoop/hdfs-site.xml\n",
        "  !echo  '    <property>'  >> /opt/hadoop-3.3.5/etc/hadoop/hdfs-site.xml\n",
        "  !echo  '        <name>dfs.replication</name>'  >> /opt/hadoop-3.3.5/etc/hadoop/hdfs-site.xml\n",
        "  !echo  '        <value>1</value>'  >> /opt/hadoop-3.3.5/etc/hadoop/hdfs-site.xml\n",
        "  !echo  '    </property>'  >> /opt/hadoop-3.3.5/etc/hadoop/hdfs-site.xml\n",
        "  !echo  '   <property>'   >> /opt/hadoop-3.3.5/etc/hadoop/hdfs-site.xml\n",
        "  !echo  '        <name>dfs.block.size</name>'  >> /opt/hadoop-3.3.5/etc/hadoop/hdfs-site.xml\n",
        "  !echo  '        <value>16777216</value>'  >> /opt/hadoop-3.3.5/etc/hadoop/hdfs-site.xml\n",
        "  !echo  '        <description>Block size</description>'  >> /opt/hadoop-3.3.5/etc/hadoop/hdfs-site.xml\n",
        "  !echo  '  </property>'  >> /opt/hadoop-3.3.5/etc/hadoop/hdfs-site.xml\n",
        "  !echo  '</configuration>'  >> /opt/hadoop-3.3.5/etc/hadoop/hdfs-site.xml\n",
        "\n",
        "  print(\"\\n---19. Amended hdfs-site.xml----\\n\")\n",
        "  !cat  /opt/hadoop-3.3.5/etc/hadoop/hdfs-site.xml\n",
        "\n",
        "  print(\"\\n---20. hadoop-env.sh----\\n\")\n",
        "  # https://stackoverflow.com/a/53140448\n",
        "  !cat /opt/hadoop-3.3.5/etc/hadoop/hadoop-env.sh\n",
        "  ! echo 'export JAVA_HOME=\"/usr/lib/jvm/java-8-openjdk-amd64\"' >> /opt/hadoop-3.3.5/etc/hadoop/hadoop-env.sh\n",
        "  ! echo 'export HDFS_NAMENODE_USER=\"root\"'  >> /opt/hadoop-3.3.5/etc/hadoop/hadoop-env.sh\n",
        "  ! echo 'export HDFS_DATANODE_USER=\"root\"'  >> /opt/hadoop-3.3.5/etc/hadoop/hadoop-env.sh\n",
        "  ! echo 'export HDFS_SECONDARYNAMENODE_USER=\"root\"'  >> /opt/hadoop-3.3.5/etc/hadoop/hadoop-env.sh\n",
        "  ! echo 'export YARN_RESOURCEMANAGER_USER=\"root\"'  >> /opt/hadoop-3.3.5/etc/hadoop/hadoop-env.sh\n",
        "  ! echo 'export YARN_NODEMANAGER_USER=\"root\"'  >> /opt/hadoop-3.3.5/etc/hadoop/hadoop-env.sh\n",
        "  \n",
        "  print(\"\\n---21. Amended hadoop-env.sh----\\n\")\n",
        "  !cat /opt/hadoop-3.3.5/etc/hadoop/hadoop-env.sh\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y159J8TDc3wS"
      },
      "source": [
        "#### ssh keys"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOFbfw7n0Pps"
      },
      "source": [
        "# 6.0 Function tp setup ssh passphrase\n",
        "def set_keys():\n",
        "  print(\"\\n---22. Generate SSH keys----\\n\")\n",
        "  ! cd ~ ; pwd \n",
        "  ! cd ~ ; ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa\n",
        "  ! cd ~ ; cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\n",
        "  ! cd ~ ; chmod 0600 ~/.ssh/authorized_keys\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M2kWg3dc6FT"
      },
      "source": [
        "#### Set environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRSn9XAV4rsR"
      },
      "source": [
        "# 7.0 Function to set up environmental variables\n",
        "def set_env():\n",
        "  print(\"\\n---23. Set Environment variables----\\n\")\n",
        "  # 'export' command does not work in colab\n",
        "  # https://stackoverflow.com/a/57240319\n",
        "  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"     #set environment variable\n",
        "  os.environ[\"JRE_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64/jre\"   \n",
        "  os.environ[\"HADOOP_HOME\"] = \"/opt/hadoop-3.3.5\"\n",
        "  os.environ[\"HADOOP_CONF_DIR\"] = \"/opt/hadoop-3.3.5/etc/hadoop\" \n",
        "  os.environ[\"LD_LIBRARY_PATH\"] += \":/opt/hadoop-3.3.5/lib/native\"\n",
        "  os.environ[\"PATH\"] += \":/opt/hadoop-3.3.5/bin:/opt/hadoop-3.3.5/sbin\""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WII-UNCzc9qJ"
      },
      "source": [
        "#### Install all function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oh1Mi0rHFpkU"
      },
      "source": [
        "# 8.0 Function to call all functions\n",
        "def install_hadoop():\n",
        "  print(\"\\n--Install java----\\n\")\n",
        "  ssh_install()\n",
        "  install_java()  \n",
        "  hadoop_install()\n",
        "  hadoop_config()\n",
        "  set_keys()\n",
        "  set_env()\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHJyHMhUdCRZ"
      },
      "source": [
        "### Begin install\n",
        "Start downloading, install and configure. Takes around 2 minutes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77YQikvsJiTm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1849b693-887f-4787-9497-a33101ca1ab9"
      },
      "source": [
        "y# 9.0 Start installation\n",
        "start = time.time()\n",
        "install_hadoop()\n",
        "end = time.time()\n",
        "print(\"\\n---Time taken----\\n\")\n",
        "print((end- start)/60)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--Install java----\n",
            "\n",
            "\n",
            "--1. Download and install ssh server----\n",
            "\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following packages were automatically installed and are no longer required:\n",
            "  libboost-atomic-dev libboost-atomic1.71-dev libboost-atomic1.71.0\n",
            "  libboost-chrono-dev libboost-chrono1.71-dev libboost-chrono1.71.0\n",
            "  libboost-container-dev libboost-container1.71-dev libboost-container1.71.0\n",
            "  libboost-context-dev libboost-context1.71-dev libboost-context1.71.0\n",
            "  libboost-coroutine-dev libboost-coroutine1.71-dev libboost-coroutine1.71.0\n",
            "  libboost-date-time-dev libboost-date-time1.71-dev libboost-date-time1.71.0\n",
            "  libboost-exception-dev libboost-exception1.71-dev libboost-fiber-dev\n",
            "  libboost-fiber1.71-dev libboost-fiber1.71.0 libboost-filesystem-dev\n",
            "  libboost-filesystem1.71-dev libboost-filesystem1.71.0 libboost-graph-dev\n",
            "  libboost-graph-parallel-dev libboost-graph-parallel1.71-dev\n",
            "  libboost-graph-parallel1.71.0 libboost-graph1.71-dev libboost-graph1.71.0\n",
            "  libboost-iostreams-dev libboost-iostreams1.71-dev libboost-locale-dev\n",
            "  libboost-locale1.71-dev libboost-locale1.71.0 libboost-log-dev\n",
            "  libboost-log1.71-dev libboost-log1.71.0 libboost-math-dev\n",
            "  libboost-math1.71-dev libboost-math1.71.0 libboost-mpi1.71.0\n",
            "  libboost-numpy-dev libboost-numpy1.71-dev libboost-numpy1.71.0\n",
            "  libboost-program-options-dev libboost-program-options1.71-dev\n",
            "  libboost-program-options1.71.0 libboost-python-dev libboost-python1.71-dev\n",
            "  libboost-python1.71.0 libboost-random-dev libboost-random1.71-dev\n",
            "  libboost-random1.71.0 libboost-regex-dev libboost-regex1.71-dev\n",
            "  libboost-regex1.71.0 libboost-serialization-dev\n",
            "  libboost-serialization1.71-dev libboost-serialization1.71.0\n",
            "  libboost-stacktrace-dev libboost-stacktrace1.71-dev\n",
            "  libboost-stacktrace1.71.0 libboost-system-dev libboost-system1.71-dev\n",
            "  libboost-system1.71.0 libboost-test-dev libboost-test1.71-dev\n",
            "  libboost-test1.71.0 libboost-thread-dev libboost-thread1.71-dev\n",
            "  libboost-timer-dev libboost-timer1.71-dev libboost-timer1.71.0\n",
            "  libboost-tools-dev libboost-type-erasure-dev libboost-type-erasure1.71-dev\n",
            "  libboost-type-erasure1.71.0 libboost-wave-dev libboost-wave1.71-dev\n",
            "  libboost-wave1.71.0 libboost1.71-tools-dev libcbor0.6 libevent-dev\n",
            "  libevent-extra-2.1-7 libevent-openssl-2.1-7 libfido2-1 libhwloc-dev\n",
            "  libibverbs-dev libnl-3-dev libnl-route-3-dev libnuma-dev ncurses-term\n",
            "  openmpi-common python3-dev python3-distro python3-distutils python3-lib2to3\n",
            "  python3.8-dev\n",
            "Use 'sudo apt autoremove' to remove them.\n",
            "The following packages will be REMOVED:\n",
            "  openssh-client openssh-server openssh-sftp-server ssh-import-id\n",
            "0 upgraded, 0 newly installed, 4 to remove and 23 not upgraded.\n",
            "After this operation, 5,942 kB disk space will be freed.\n",
            "(Reading database ... 130692 files and directories currently installed.)\n",
            "Removing openssh-server (1:8.2p1-4ubuntu0.5) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of stop.\n",
            "Removing openssh-sftp-server (1:8.2p1-4ubuntu0.5) ...\n",
            "Removing ssh-import-id (5.10-0ubuntu1) ...\n",
            "Removing openssh-client (1:8.2p1-4ubuntu0.5) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following packages were automatically installed and are no longer required:\n",
            "  libboost-atomic-dev libboost-atomic1.71-dev libboost-atomic1.71.0\n",
            "  libboost-chrono-dev libboost-chrono1.71-dev libboost-chrono1.71.0\n",
            "  libboost-container-dev libboost-container1.71-dev libboost-container1.71.0\n",
            "  libboost-context-dev libboost-context1.71-dev libboost-context1.71.0\n",
            "  libboost-coroutine-dev libboost-coroutine1.71-dev libboost-coroutine1.71.0\n",
            "  libboost-date-time-dev libboost-date-time1.71-dev libboost-date-time1.71.0\n",
            "  libboost-exception-dev libboost-exception1.71-dev libboost-fiber-dev\n",
            "  libboost-fiber1.71-dev libboost-fiber1.71.0 libboost-filesystem-dev\n",
            "  libboost-filesystem1.71-dev libboost-filesystem1.71.0 libboost-graph-dev\n",
            "  libboost-graph-parallel-dev libboost-graph-parallel1.71-dev\n",
            "  libboost-graph-parallel1.71.0 libboost-graph1.71-dev libboost-graph1.71.0\n",
            "  libboost-iostreams-dev libboost-iostreams1.71-dev libboost-locale-dev\n",
            "  libboost-locale1.71-dev libboost-locale1.71.0 libboost-log-dev\n",
            "  libboost-log1.71-dev libboost-log1.71.0 libboost-math-dev\n",
            "  libboost-math1.71-dev libboost-math1.71.0 libboost-mpi1.71.0\n",
            "  libboost-numpy-dev libboost-numpy1.71-dev libboost-numpy1.71.0\n",
            "  libboost-program-options-dev libboost-program-options1.71-dev\n",
            "  libboost-program-options1.71.0 libboost-python-dev libboost-python1.71-dev\n",
            "  libboost-python1.71.0 libboost-random-dev libboost-random1.71-dev\n",
            "  libboost-random1.71.0 libboost-regex-dev libboost-regex1.71-dev\n",
            "  libboost-regex1.71.0 libboost-serialization-dev\n",
            "  libboost-serialization1.71-dev libboost-serialization1.71.0\n",
            "  libboost-stacktrace-dev libboost-stacktrace1.71-dev\n",
            "  libboost-stacktrace1.71.0 libboost-system-dev libboost-system1.71-dev\n",
            "  libboost-system1.71.0 libboost-test-dev libboost-test1.71-dev\n",
            "  libboost-test1.71.0 libboost-thread-dev libboost-thread1.71-dev\n",
            "  libboost-timer-dev libboost-timer1.71-dev libboost-timer1.71.0\n",
            "  libboost-tools-dev libboost-type-erasure-dev libboost-type-erasure1.71-dev\n",
            "  libboost-type-erasure1.71.0 libboost-wave-dev libboost-wave1.71-dev\n",
            "  libboost-wave1.71.0 libboost1.71-tools-dev libevent-dev libevent-extra-2.1-7\n",
            "  libevent-openssl-2.1-7 libhwloc-dev libibverbs-dev libnl-3-dev\n",
            "  libnl-route-3-dev libnuma-dev openmpi-common python3-dev python3-distutils\n",
            "  python3-lib2to3 python3.8-dev\n",
            "Use 'sudo apt autoremove' to remove them.\n",
            "The following additional packages will be installed:\n",
            "  openssh-sftp-server ssh-import-id\n",
            "Suggested packages:\n",
            "  keychain libpam-ssh monkeysphere ssh-askpass molly-guard ufw\n",
            "The following NEW packages will be installed:\n",
            "  openssh-client openssh-server openssh-sftp-server ssh-import-id\n",
            "0 upgraded, 4 newly installed, 0 to remove and 23 not upgraded.\n",
            "Need to get 1,109 kB of archives.\n",
            "After this operation, 5,942 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 openssh-client amd64 1:8.2p1-4ubuntu0.5 [671 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 openssh-sftp-server amd64 1:8.2p1-4ubuntu0.5 [51.5 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 openssh-server amd64 1:8.2p1-4ubuntu0.5 [377 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu focal/main amd64 ssh-import-id all 5.10-0ubuntu1 [10.0 kB]\n",
            "Fetched 1,109 kB in 1s (843 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 4.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package openssh-client.\n",
            "(Reading database ... 130607 files and directories currently installed.)\n",
            "Preparing to unpack .../openssh-client_1%3a8.2p1-4ubuntu0.5_amd64.deb ...\n",
            "Unpacking openssh-client (1:8.2p1-4ubuntu0.5) ...\n",
            "Selecting previously unselected package openssh-sftp-server.\n",
            "Preparing to unpack .../openssh-sftp-server_1%3a8.2p1-4ubuntu0.5_amd64.deb ...\n",
            "Unpacking openssh-sftp-server (1:8.2p1-4ubuntu0.5) ...\n",
            "Selecting previously unselected package openssh-server.\n",
            "Preparing to unpack .../openssh-server_1%3a8.2p1-4ubuntu0.5_amd64.deb ...\n",
            "Unpacking openssh-server (1:8.2p1-4ubuntu0.5) ...\n",
            "Selecting previously unselected package ssh-import-id.\n",
            "Preparing to unpack .../ssh-import-id_5.10-0ubuntu1_all.deb ...\n",
            "Unpacking ssh-import-id (5.10-0ubuntu1) ...\n",
            "Setting up openssh-client (1:8.2p1-4ubuntu0.5) ...\n",
            "Setting up ssh-import-id (5.10-0ubuntu1) ...\n",
            "Setting up openssh-sftp-server (1:8.2p1-4ubuntu0.5) ...\n",
            "Setting up openssh-server (1:8.2p1-4ubuntu0.5) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of restart.\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Processing triggers for systemd (245.4-4ubuntu3.20) ...\n",
            "\n",
            "--2. Restart ssh server----\n",
            "\n",
            " * Restarting OpenBSD Secure Shell server sshd\n",
            "   ...done.\n",
            "\n",
            "--Download and install Java 8----\n",
            "\n",
            "openjdk version \"1.8.0_362\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09)\n",
            "OpenJDK 64-Bit Server VM (build 25.362-b09, mixed mode)\n",
            "javac 1.8.0_362\n",
            "\n",
            "--5. Download hadoop tar.gz----\n",
            "\n",
            "--2023-03-27 05:58:19--  https://dlcdn.apache.org/hadoop/common/hadoop-3.3.5/hadoop-3.3.5.tar.gz\n",
            "Resolving dlcdn.apache.org (dlcdn.apache.org)... 151.101.2.132, 2a04:4e42::644\n",
            "Connecting to dlcdn.apache.org (dlcdn.apache.org)|151.101.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 706533213 (674M) [application/x-gzip]\n",
            "Saving to: ‘hadoop-3.3.5.tar.gz’\n",
            "\n",
            "hadoop-3.3.5.tar.gz 100%[===================>] 673.80M   235MB/s    in 2.9s    \n",
            "\n",
            "2023-03-27 05:58:22 (235 MB/s) - ‘hadoop-3.3.5.tar.gz’ saved [706533213/706533213]\n",
            "\n",
            "\n",
            "--6. Transfer downloaded content and unzip tar.gz----\n",
            "\n",
            "\n",
            "--7. Create hadoop folder----\n",
            "\n",
            "\n",
            "--8. Check folder for files----\n",
            "\n",
            "total 690000\n",
            "drwxr-xr-x  1 root root      4096 Mar 27 05:58 .\n",
            "drwxr-xr-x  1 root root      4096 Mar 27 05:54 ..\n",
            "drwxr-xr-x  1 root root      4096 Mar 23 19:09 google\n",
            "drwxr-xr-x 10 2002 2002      4096 Mar 15 16:58 hadoop-3.3.5\n",
            "-rw-r--r--  1 root root 706533213 Mar 15 19:35 hadoop-3.3.5.tar.gz\n",
            "drwxr-xr-x  1 root root      4096 Mar 23 18:49 nvidia\n",
            "\n",
            "--Begin Configuring hadoop---\n",
            "\n",
            "\n",
            "=============================\n",
            "\n",
            "\n",
            "--9. core-site.xml----\n",
            "\n",
            "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
            "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
            "<!--\n",
            "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "  you may not use this file except in compliance with the License.\n",
            "  You may obtain a copy of the License at\n",
            "\n",
            "    http://www.apache.org/licenses/LICENSE-2.0\n",
            "\n",
            "  Unless required by applicable law or agreed to in writing, software\n",
            "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "  See the License for the specific language governing permissions and\n",
            "  limitations under the License. See accompanying LICENSE file.\n",
            "-->\n",
            "\n",
            "<!-- Put site-specific property overrides in this file. -->\n",
            "\n",
            "<configuration>\n",
            "</configuration>\n",
            "\n",
            "--10. Amend core-site.xml----\n",
            "\n",
            "\n",
            "--11. Amended core-site.xml----\n",
            "\n",
            "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
            "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
            " <configuration>\n",
            "    <property>\n",
            "        <name>fs.defaultFS</name>\n",
            "        <value>hdfs://localhost:9000</value>\n",
            "    </property>\n",
            "    <property>\n",
            "          <name>hadoop.tmp.dir</name>\n",
            "          <value>/app/hadoop/tmp</value>\n",
            "       <description>A base for other temporary directories.</description>\n",
            "     </property>\n",
            "     <property>\n",
            "      <name>dfs.safemode.threshold.pct</name>\n",
            "    <value>0</value>\n",
            "  </property>\n",
            "  </configuration>\n",
            "\n",
            "--12. yarn-site.xml----\n",
            "\n",
            "<?xml version=\"1.0\"?>\n",
            "<!--\n",
            "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "  you may not use this file except in compliance with the License.\n",
            "  You may obtain a copy of the License at\n",
            "\n",
            "    http://www.apache.org/licenses/LICENSE-2.0\n",
            "\n",
            "  Unless required by applicable law or agreed to in writing, software\n",
            "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "  See the License for the specific language governing permissions and\n",
            "  limitations under the License. See accompanying LICENSE file.\n",
            "-->\n",
            "<configuration>\n",
            "\n",
            "<!-- Site specific YARN configuration properties -->\n",
            "\n",
            "</configuration>\n",
            "\n",
            "--13. Amended yarn-site.xml----\n",
            "\n",
            "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
            "<configuration>\n",
            "    <property>\n",
            "        <name>yarn.nodemanager.aux-services</name>\n",
            "        <value>mapreduce_shuffle</value>\n",
            "    </property>\n",
            "    <property>\n",
            "       <name>yarn.nodemanager.vmem-check-enabled</name>\n",
            "       <value>false</value>\n",
            "    </property>\n",
            " </configuration>\n",
            "\n",
            "--14. mapred-site.xml----\n",
            "\n",
            "<?xml version=\"1.0\"?>\n",
            "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
            "<!--\n",
            "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "  you may not use this file except in compliance with the License.\n",
            "  You may obtain a copy of the License at\n",
            "\n",
            "    http://www.apache.org/licenses/LICENSE-2.0\n",
            "\n",
            "  Unless required by applicable law or agreed to in writing, software\n",
            "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "  See the License for the specific language governing permissions and\n",
            "  limitations under the License. See accompanying LICENSE file.\n",
            "-->\n",
            "\n",
            "<!-- Put site-specific property overrides in this file. -->\n",
            "\n",
            "<configuration>\n",
            "\n",
            "</configuration>\n",
            "\n",
            "--15. Amend mapred-site.xml----\n",
            "\n",
            "\n",
            "--16, Amended mapred-site.xml----\n",
            "\n",
            "<?xml version=\"1.0\"?>\n",
            "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
            "<configuration>\n",
            "    <property>\n",
            "       <name>mapreduce.framework.name</name>\n",
            "        <value>yarn</value>\n",
            "    </property>\n",
            "    <property>\n",
            "        <name>yarn.app.mapreduce.am.env</name>\n",
            "        <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>\n",
            "    </property>\n",
            "    <property>\n",
            "       <name>mapreduce.map.env</name>\n",
            "       <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>\n",
            "    </property>\n",
            "    <property>\n",
            "      <name>mapreduce.reduce.env</name>\n",
            "      <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>\n",
            "   </property>\n",
            "</configuration>\n",
            "\n",
            "---17. hdfs-site.xml----\n",
            "\n",
            "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
            "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
            "<!--\n",
            "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "  you may not use this file except in compliance with the License.\n",
            "  You may obtain a copy of the License at\n",
            "\n",
            "    http://www.apache.org/licenses/LICENSE-2.0\n",
            "\n",
            "  Unless required by applicable law or agreed to in writing, software\n",
            "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "  See the License for the specific language governing permissions and\n",
            "  limitations under the License. See accompanying LICENSE file.\n",
            "-->\n",
            "\n",
            "<!-- Put site-specific property overrides in this file. -->\n",
            "\n",
            "<configuration>\n",
            "\n",
            "</configuration>\n",
            "\n",
            "---18. Amend hdfs-site.xml----\n",
            "\n",
            "\n",
            "---19. Amended hdfs-site.xml----\n",
            "\n",
            "<?xml version=\"1.0\" encoding=\"UTF-8\"?> \n",
            "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
            "<configuration>\n",
            "    <property>\n",
            "        <name>dfs.replication</name>\n",
            "        <value>1</value>\n",
            "    </property>\n",
            "   <property>\n",
            "        <name>dfs.block.size</name>\n",
            "        <value>16777216</value>\n",
            "        <description>Block size</description>\n",
            "  </property>\n",
            "</configuration>\n",
            "\n",
            "---20. hadoop-env.sh----\n",
            "\n",
            "#\n",
            "# Licensed to the Apache Software Foundation (ASF) under one\n",
            "# or more contributor license agreements.  See the NOTICE file\n",
            "# distributed with this work for additional information\n",
            "# regarding copyright ownership.  The ASF licenses this file\n",
            "# to you under the Apache License, Version 2.0 (the\n",
            "# \"License\"); you may not use this file except in compliance\n",
            "# with the License.  You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "\n",
            "# Set Hadoop-specific environment variables here.\n",
            "\n",
            "##\n",
            "## THIS FILE ACTS AS THE MASTER FILE FOR ALL HADOOP PROJECTS.\n",
            "## SETTINGS HERE WILL BE READ BY ALL HADOOP COMMANDS.  THEREFORE,\n",
            "## ONE CAN USE THIS FILE TO SET YARN, HDFS, AND MAPREDUCE\n",
            "## CONFIGURATION OPTIONS INSTEAD OF xxx-env.sh.\n",
            "##\n",
            "## Precedence rules:\n",
            "##\n",
            "## {yarn-env.sh|hdfs-env.sh} > hadoop-env.sh > hard-coded defaults\n",
            "##\n",
            "## {YARN_xyz|HDFS_xyz} > HADOOP_xyz > hard-coded defaults\n",
            "##\n",
            "\n",
            "# Many of the options here are built from the perspective that users\n",
            "# may want to provide OVERWRITING values on the command line.\n",
            "# For example:\n",
            "#\n",
            "#  JAVA_HOME=/usr/java/testing hdfs dfs -ls\n",
            "#\n",
            "# Therefore, the vast majority (BUT NOT ALL!) of these defaults\n",
            "# are configured for substitution and not append.  If append\n",
            "# is preferable, modify this file accordingly.\n",
            "\n",
            "###\n",
            "# Generic settings for HADOOP\n",
            "###\n",
            "\n",
            "# Technically, the only required environment variable is JAVA_HOME.\n",
            "# All others are optional.  However, the defaults are probably not\n",
            "# preferred.  Many sites configure these options outside of Hadoop,\n",
            "# such as in /etc/profile.d\n",
            "\n",
            "# The java implementation to use. By default, this environment\n",
            "# variable is REQUIRED on ALL platforms except OS X!\n",
            "# export JAVA_HOME=\n",
            "\n",
            "# Location of Hadoop.  By default, Hadoop will attempt to determine\n",
            "# this location based upon its execution path.\n",
            "# export HADOOP_HOME=\n",
            "\n",
            "# Location of Hadoop's configuration information.  i.e., where this\n",
            "# file is living. If this is not defined, Hadoop will attempt to\n",
            "# locate it based upon its execution path.\n",
            "#\n",
            "# NOTE: It is recommend that this variable not be set here but in\n",
            "# /etc/profile.d or equivalent.  Some options (such as\n",
            "# --config) may react strangely otherwise.\n",
            "#\n",
            "# export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop\n",
            "\n",
            "# The maximum amount of heap to use (Java -Xmx).  If no unit\n",
            "# is provided, it will be converted to MB.  Daemons will\n",
            "# prefer any Xmx setting in their respective _OPT variable.\n",
            "# There is no default; the JVM will autoscale based upon machine\n",
            "# memory size.\n",
            "# export HADOOP_HEAPSIZE_MAX=\n",
            "\n",
            "# The minimum amount of heap to use (Java -Xms).  If no unit\n",
            "# is provided, it will be converted to MB.  Daemons will\n",
            "# prefer any Xms setting in their respective _OPT variable.\n",
            "# There is no default; the JVM will autoscale based upon machine\n",
            "# memory size.\n",
            "# export HADOOP_HEAPSIZE_MIN=\n",
            "\n",
            "# Enable extra debugging of Hadoop's JAAS binding, used to set up\n",
            "# Kerberos security.\n",
            "# export HADOOP_JAAS_DEBUG=true\n",
            "\n",
            "# Extra Java runtime options for all Hadoop commands. We don't support\n",
            "# IPv6 yet/still, so by default the preference is set to IPv4.\n",
            "# export HADOOP_OPTS=\"-Djava.net.preferIPv4Stack=true\"\n",
            "# For Kerberos debugging, an extended option set logs more information\n",
            "# export HADOOP_OPTS=\"-Djava.net.preferIPv4Stack=true -Dsun.security.krb5.debug=true -Dsun.security.spnego.debug\"\n",
            "\n",
            "# Some parts of the shell code may do special things dependent upon\n",
            "# the operating system.  We have to set this here. See the next\n",
            "# section as to why....\n",
            "export HADOOP_OS_TYPE=${HADOOP_OS_TYPE:-$(uname -s)}\n",
            "\n",
            "# Extra Java runtime options for some Hadoop commands\n",
            "# and clients (i.e., hdfs dfs -blah).  These get appended to HADOOP_OPTS for\n",
            "# such commands.  In most cases, # this should be left empty and\n",
            "# let users supply it on the command line.\n",
            "# export HADOOP_CLIENT_OPTS=\"\"\n",
            "\n",
            "#\n",
            "# A note about classpaths.\n",
            "#\n",
            "# By default, Apache Hadoop overrides Java's CLASSPATH\n",
            "# environment variable.  It is configured such\n",
            "# that it starts out blank with new entries added after passing\n",
            "# a series of checks (file/dir exists, not already listed aka\n",
            "# de-deduplication).  During de-deduplication, wildcards and/or\n",
            "# directories are *NOT* expanded to keep it simple. Therefore,\n",
            "# if the computed classpath has two specific mentions of\n",
            "# awesome-methods-1.0.jar, only the first one added will be seen.\n",
            "# If two directories are in the classpath that both contain\n",
            "# awesome-methods-1.0.jar, then Java will pick up both versions.\n",
            "\n",
            "# An additional, custom CLASSPATH. Site-wide configs should be\n",
            "# handled via the shellprofile functionality, utilizing the\n",
            "# hadoop_add_classpath function for greater control and much\n",
            "# harder for apps/end-users to accidentally override.\n",
            "# Similarly, end users should utilize ${HOME}/.hadooprc .\n",
            "# This variable should ideally only be used as a short-cut,\n",
            "# interactive way for temporary additions on the command line.\n",
            "# export HADOOP_CLASSPATH=\"/some/cool/path/on/your/machine\"\n",
            "\n",
            "# Should HADOOP_CLASSPATH be first in the official CLASSPATH?\n",
            "# export HADOOP_USER_CLASSPATH_FIRST=\"yes\"\n",
            "\n",
            "# If HADOOP_USE_CLIENT_CLASSLOADER is set, the classpath along\n",
            "# with the main jar are handled by a separate isolated\n",
            "# client classloader when 'hadoop jar', 'yarn jar', or 'mapred job'\n",
            "# is utilized. If it is set, HADOOP_CLASSPATH and\n",
            "# HADOOP_USER_CLASSPATH_FIRST are ignored.\n",
            "# export HADOOP_USE_CLIENT_CLASSLOADER=true\n",
            "\n",
            "# HADOOP_CLIENT_CLASSLOADER_SYSTEM_CLASSES overrides the default definition of\n",
            "# system classes for the client classloader when HADOOP_USE_CLIENT_CLASSLOADER\n",
            "# is enabled. Names ending in '.' (period) are treated as package names, and\n",
            "# names starting with a '-' are treated as negative matches. For example,\n",
            "# export HADOOP_CLIENT_CLASSLOADER_SYSTEM_CLASSES=\"-org.apache.hadoop.UserClass,java.,javax.,org.apache.hadoop.\"\n",
            "\n",
            "# Enable optional, bundled Hadoop features\n",
            "# This is a comma delimited list.  It may NOT be overridden via .hadooprc\n",
            "# Entries may be added/removed as needed.\n",
            "# export HADOOP_OPTIONAL_TOOLS=\"hadoop-kafka,hadoop-aliyun,hadoop-azure,hadoop-azure-datalake,hadoop-aws\"\n",
            "\n",
            "###\n",
            "# Options for remote shell connectivity\n",
            "###\n",
            "\n",
            "# There are some optional components of hadoop that allow for\n",
            "# command and control of remote hosts.  For example,\n",
            "# start-dfs.sh will attempt to bring up all NNs, DNS, etc.\n",
            "\n",
            "# Options to pass to SSH when one of the \"log into a host and\n",
            "# start/stop daemons\" scripts is executed\n",
            "# export HADOOP_SSH_OPTS=\"-o BatchMode=yes -o StrictHostKeyChecking=no -o ConnectTimeout=10s\"\n",
            "\n",
            "# The built-in ssh handler will limit itself to 10 simultaneous connections.\n",
            "# For pdsh users, this sets the fanout size ( -f )\n",
            "# Change this to increase/decrease as necessary.\n",
            "# export HADOOP_SSH_PARALLEL=10\n",
            "\n",
            "# Filename which contains all of the hosts for any remote execution\n",
            "# helper scripts # such as workers.sh, start-dfs.sh, etc.\n",
            "# export HADOOP_WORKERS=\"${HADOOP_CONF_DIR}/workers\"\n",
            "\n",
            "###\n",
            "# Options for all daemons\n",
            "###\n",
            "#\n",
            "\n",
            "#\n",
            "# Many options may also be specified as Java properties.  It is\n",
            "# very common, and in many cases, desirable, to hard-set these\n",
            "# in daemon _OPTS variables.  Where applicable, the appropriate\n",
            "# Java property is also identified.  Note that many are re-used\n",
            "# or set differently in certain contexts (e.g., secure vs\n",
            "# non-secure)\n",
            "#\n",
            "\n",
            "# Where (primarily) daemon log files are stored.\n",
            "# ${HADOOP_HOME}/logs by default.\n",
            "# Java property: hadoop.log.dir\n",
            "# export HADOOP_LOG_DIR=${HADOOP_HOME}/logs\n",
            "\n",
            "# A string representing this instance of hadoop. $USER by default.\n",
            "# This is used in writing log and pid files, so keep that in mind!\n",
            "# Java property: hadoop.id.str\n",
            "# export HADOOP_IDENT_STRING=$USER\n",
            "\n",
            "# How many seconds to pause after stopping a daemon\n",
            "# export HADOOP_STOP_TIMEOUT=5\n",
            "\n",
            "# Where pid files are stored.  /tmp by default.\n",
            "# export HADOOP_PID_DIR=/tmp\n",
            "\n",
            "# Default log4j setting for interactive commands\n",
            "# Java property: hadoop.root.logger\n",
            "# export HADOOP_ROOT_LOGGER=INFO,console\n",
            "\n",
            "# Default log4j setting for daemons spawned explicitly by\n",
            "# --daemon option of hadoop, hdfs, mapred and yarn command.\n",
            "# Java property: hadoop.root.logger\n",
            "# export HADOOP_DAEMON_ROOT_LOGGER=INFO,RFA\n",
            "\n",
            "# Default log level and output location for security-related messages.\n",
            "# You will almost certainly want to change this on a per-daemon basis via\n",
            "# the Java property (i.e., -Dhadoop.security.logger=foo). (Note that the\n",
            "# defaults for the NN and 2NN override this by default.)\n",
            "# Java property: hadoop.security.logger\n",
            "# export HADOOP_SECURITY_LOGGER=INFO,NullAppender\n",
            "\n",
            "# Default process priority level\n",
            "# Note that sub-processes will also run at this level!\n",
            "# export HADOOP_NICENESS=0\n",
            "\n",
            "# Default name for the service level authorization file\n",
            "# Java property: hadoop.policy.file\n",
            "# export HADOOP_POLICYFILE=\"hadoop-policy.xml\"\n",
            "\n",
            "#\n",
            "# NOTE: this is not used by default!  <-----\n",
            "# You can define variables right here and then re-use them later on.\n",
            "# For example, it is common to use the same garbage collection settings\n",
            "# for all the daemons.  So one could define:\n",
            "#\n",
            "# export HADOOP_GC_SETTINGS=\"-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps\"\n",
            "#\n",
            "# .. and then use it as per the b option under the namenode.\n",
            "\n",
            "###\n",
            "# Secure/privileged execution\n",
            "###\n",
            "\n",
            "#\n",
            "# Out of the box, Hadoop uses jsvc from Apache Commons to launch daemons\n",
            "# on privileged ports.  This functionality can be replaced by providing\n",
            "# custom functions.  See hadoop-functions.sh for more information.\n",
            "#\n",
            "\n",
            "# The jsvc implementation to use. Jsvc is required to run secure datanodes\n",
            "# that bind to privileged ports to provide authentication of data transfer\n",
            "# protocol.  Jsvc is not required if SASL is configured for authentication of\n",
            "# data transfer protocol using non-privileged ports.\n",
            "# export JSVC_HOME=/usr/bin\n",
            "\n",
            "#\n",
            "# This directory contains pids for secure and privileged processes.\n",
            "#export HADOOP_SECURE_PID_DIR=${HADOOP_PID_DIR}\n",
            "\n",
            "#\n",
            "# This directory contains the logs for secure and privileged processes.\n",
            "# Java property: hadoop.log.dir\n",
            "# export HADOOP_SECURE_LOG=${HADOOP_LOG_DIR}\n",
            "\n",
            "#\n",
            "# When running a secure daemon, the default value of HADOOP_IDENT_STRING\n",
            "# ends up being a bit bogus.  Therefore, by default, the code will\n",
            "# replace HADOOP_IDENT_STRING with HADOOP_xx_SECURE_USER.  If one wants\n",
            "# to keep HADOOP_IDENT_STRING untouched, then uncomment this line.\n",
            "# export HADOOP_SECURE_IDENT_PRESERVE=\"true\"\n",
            "\n",
            "###\n",
            "# NameNode specific parameters\n",
            "###\n",
            "\n",
            "# Default log level and output location for file system related change\n",
            "# messages. For non-namenode daemons, the Java property must be set in\n",
            "# the appropriate _OPTS if one wants something other than INFO,NullAppender\n",
            "# Java property: hdfs.audit.logger\n",
            "# export HDFS_AUDIT_LOGGER=INFO,NullAppender\n",
            "\n",
            "# Specify the JVM options to be used when starting the NameNode.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# a) Set JMX options\n",
            "# export HDFS_NAMENODE_OPTS=\"-Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=1026\"\n",
            "#\n",
            "# b) Set garbage collection logs\n",
            "# export HDFS_NAMENODE_OPTS=\"${HADOOP_GC_SETTINGS} -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')\"\n",
            "#\n",
            "# c) ... or set them directly\n",
            "# export HDFS_NAMENODE_OPTS=\"-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')\"\n",
            "\n",
            "# this is the default:\n",
            "# export HDFS_NAMENODE_OPTS=\"-Dhadoop.security.logger=INFO,RFAS\"\n",
            "\n",
            "###\n",
            "# SecondaryNameNode specific parameters\n",
            "###\n",
            "# Specify the JVM options to be used when starting the SecondaryNameNode.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# This is the default:\n",
            "# export HDFS_SECONDARYNAMENODE_OPTS=\"-Dhadoop.security.logger=INFO,RFAS\"\n",
            "\n",
            "###\n",
            "# DataNode specific parameters\n",
            "###\n",
            "# Specify the JVM options to be used when starting the DataNode.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# This is the default:\n",
            "# export HDFS_DATANODE_OPTS=\"-Dhadoop.security.logger=ERROR,RFAS\"\n",
            "\n",
            "# On secure datanodes, user to run the datanode as after dropping privileges.\n",
            "# This **MUST** be uncommented to enable secure HDFS if using privileged ports\n",
            "# to provide authentication of data transfer protocol.  This **MUST NOT** be\n",
            "# defined if SASL is configured for authentication of data transfer protocol\n",
            "# using non-privileged ports.\n",
            "# This will replace the hadoop.id.str Java property in secure mode.\n",
            "# export HDFS_DATANODE_SECURE_USER=hdfs\n",
            "\n",
            "# Supplemental options for secure datanodes\n",
            "# By default, Hadoop uses jsvc which needs to know to launch a\n",
            "# server jvm.\n",
            "# export HDFS_DATANODE_SECURE_EXTRA_OPTS=\"-jvm server\"\n",
            "\n",
            "###\n",
            "# NFS3 Gateway specific parameters\n",
            "###\n",
            "# Specify the JVM options to be used when starting the NFS3 Gateway.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# export HDFS_NFS3_OPTS=\"\"\n",
            "\n",
            "# Specify the JVM options to be used when starting the Hadoop portmapper.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# export HDFS_PORTMAP_OPTS=\"-Xmx512m\"\n",
            "\n",
            "# Supplemental options for priviliged gateways\n",
            "# By default, Hadoop uses jsvc which needs to know to launch a\n",
            "# server jvm.\n",
            "# export HDFS_NFS3_SECURE_EXTRA_OPTS=\"-jvm server\"\n",
            "\n",
            "# On privileged gateways, user to run the gateway as after dropping privileges\n",
            "# This will replace the hadoop.id.str Java property in secure mode.\n",
            "# export HDFS_NFS3_SECURE_USER=nfsserver\n",
            "\n",
            "###\n",
            "# ZKFailoverController specific parameters\n",
            "###\n",
            "# Specify the JVM options to be used when starting the ZKFailoverController.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# export HDFS_ZKFC_OPTS=\"\"\n",
            "\n",
            "###\n",
            "# QuorumJournalNode specific parameters\n",
            "###\n",
            "# Specify the JVM options to be used when starting the QuorumJournalNode.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# export HDFS_JOURNALNODE_OPTS=\"\"\n",
            "\n",
            "###\n",
            "# HDFS Balancer specific parameters\n",
            "###\n",
            "# Specify the JVM options to be used when starting the HDFS Balancer.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# export HDFS_BALANCER_OPTS=\"\"\n",
            "\n",
            "###\n",
            "# HDFS Mover specific parameters\n",
            "###\n",
            "# Specify the JVM options to be used when starting the HDFS Mover.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# export HDFS_MOVER_OPTS=\"\"\n",
            "\n",
            "###\n",
            "# Router-based HDFS Federation specific parameters\n",
            "# Specify the JVM options to be used when starting the RBF Routers.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# export HDFS_DFSROUTER_OPTS=\"\"\n",
            "\n",
            "###\n",
            "# HDFS StorageContainerManager specific parameters\n",
            "###\n",
            "# Specify the JVM options to be used when starting the HDFS Storage Container Manager.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# export HDFS_STORAGECONTAINERMANAGER_OPTS=\"\"\n",
            "\n",
            "###\n",
            "# Advanced Users Only!\n",
            "###\n",
            "\n",
            "#\n",
            "# When building Hadoop, one can add the class paths to the commands\n",
            "# via this special env var:\n",
            "# export HADOOP_ENABLE_BUILD_PATHS=\"true\"\n",
            "\n",
            "#\n",
            "# To prevent accidents, shell commands be (superficially) locked\n",
            "# to only allow certain users to execute certain subcommands.\n",
            "# It uses the format of (command)_(subcommand)_USER.\n",
            "#\n",
            "# For example, to limit who can execute the namenode command,\n",
            "# export HDFS_NAMENODE_USER=hdfs\n",
            "\n",
            "\n",
            "###\n",
            "# Registry DNS specific parameters\n",
            "###\n",
            "# For privileged registry DNS, user to run as after dropping privileges\n",
            "# This will replace the hadoop.id.str Java property in secure mode.\n",
            "# export HADOOP_REGISTRYDNS_SECURE_USER=yarn\n",
            "\n",
            "# Supplemental options for privileged registry DNS\n",
            "# By default, Hadoop uses jsvc which needs to know to launch a\n",
            "# server jvm.\n",
            "# export HADOOP_REGISTRYDNS_SECURE_EXTRA_OPTS=\"-jvm server\"\n",
            "\n",
            "---21. Amended hadoop-env.sh----\n",
            "\n",
            "#\n",
            "# Licensed to the Apache Software Foundation (ASF) under one\n",
            "# or more contributor license agreements.  See the NOTICE file\n",
            "# distributed with this work for additional information\n",
            "# regarding copyright ownership.  The ASF licenses this file\n",
            "# to you under the Apache License, Version 2.0 (the\n",
            "# \"License\"); you may not use this file except in compliance\n",
            "# with the License.  You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "\n",
            "# Set Hadoop-specific environment variables here.\n",
            "\n",
            "##\n",
            "## THIS FILE ACTS AS THE MASTER FILE FOR ALL HADOOP PROJECTS.\n",
            "## SETTINGS HERE WILL BE READ BY ALL HADOOP COMMANDS.  THEREFORE,\n",
            "## ONE CAN USE THIS FILE TO SET YARN, HDFS, AND MAPREDUCE\n",
            "## CONFIGURATION OPTIONS INSTEAD OF xxx-env.sh.\n",
            "##\n",
            "## Precedence rules:\n",
            "##\n",
            "## {yarn-env.sh|hdfs-env.sh} > hadoop-env.sh > hard-coded defaults\n",
            "##\n",
            "## {YARN_xyz|HDFS_xyz} > HADOOP_xyz > hard-coded defaults\n",
            "##\n",
            "\n",
            "# Many of the options here are built from the perspective that users\n",
            "# may want to provide OVERWRITING values on the command line.\n",
            "# For example:\n",
            "#\n",
            "#  JAVA_HOME=/usr/java/testing hdfs dfs -ls\n",
            "#\n",
            "# Therefore, the vast majority (BUT NOT ALL!) of these defaults\n",
            "# are configured for substitution and not append.  If append\n",
            "# is preferable, modify this file accordingly.\n",
            "\n",
            "###\n",
            "# Generic settings for HADOOP\n",
            "###\n",
            "\n",
            "# Technically, the only required environment variable is JAVA_HOME.\n",
            "# All others are optional.  However, the defaults are probably not\n",
            "# preferred.  Many sites configure these options outside of Hadoop,\n",
            "# such as in /etc/profile.d\n",
            "\n",
            "# The java implementation to use. By default, this environment\n",
            "# variable is REQUIRED on ALL platforms except OS X!\n",
            "# export JAVA_HOME=\n",
            "\n",
            "# Location of Hadoop.  By default, Hadoop will attempt to determine\n",
            "# this location based upon its execution path.\n",
            "# export HADOOP_HOME=\n",
            "\n",
            "# Location of Hadoop's configuration information.  i.e., where this\n",
            "# file is living. If this is not defined, Hadoop will attempt to\n",
            "# locate it based upon its execution path.\n",
            "#\n",
            "# NOTE: It is recommend that this variable not be set here but in\n",
            "# /etc/profile.d or equivalent.  Some options (such as\n",
            "# --config) may react strangely otherwise.\n",
            "#\n",
            "# export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop\n",
            "\n",
            "# The maximum amount of heap to use (Java -Xmx).  If no unit\n",
            "# is provided, it will be converted to MB.  Daemons will\n",
            "# prefer any Xmx setting in their respective _OPT variable.\n",
            "# There is no default; the JVM will autoscale based upon machine\n",
            "# memory size.\n",
            "# export HADOOP_HEAPSIZE_MAX=\n",
            "\n",
            "# The minimum amount of heap to use (Java -Xms).  If no unit\n",
            "# is provided, it will be converted to MB.  Daemons will\n",
            "# prefer any Xms setting in their respective _OPT variable.\n",
            "# There is no default; the JVM will autoscale based upon machine\n",
            "# memory size.\n",
            "# export HADOOP_HEAPSIZE_MIN=\n",
            "\n",
            "# Enable extra debugging of Hadoop's JAAS binding, used to set up\n",
            "# Kerberos security.\n",
            "# export HADOOP_JAAS_DEBUG=true\n",
            "\n",
            "# Extra Java runtime options for all Hadoop commands. We don't support\n",
            "# IPv6 yet/still, so by default the preference is set to IPv4.\n",
            "# export HADOOP_OPTS=\"-Djava.net.preferIPv4Stack=true\"\n",
            "# For Kerberos debugging, an extended option set logs more information\n",
            "# export HADOOP_OPTS=\"-Djava.net.preferIPv4Stack=true -Dsun.security.krb5.debug=true -Dsun.security.spnego.debug\"\n",
            "\n",
            "# Some parts of the shell code may do special things dependent upon\n",
            "# the operating system.  We have to set this here. See the next\n",
            "# section as to why....\n",
            "export HADOOP_OS_TYPE=${HADOOP_OS_TYPE:-$(uname -s)}\n",
            "\n",
            "# Extra Java runtime options for some Hadoop commands\n",
            "# and clients (i.e., hdfs dfs -blah).  These get appended to HADOOP_OPTS for\n",
            "# such commands.  In most cases, # this should be left empty and\n",
            "# let users supply it on the command line.\n",
            "# export HADOOP_CLIENT_OPTS=\"\"\n",
            "\n",
            "#\n",
            "# A note about classpaths.\n",
            "#\n",
            "# By default, Apache Hadoop overrides Java's CLASSPATH\n",
            "# environment variable.  It is configured such\n",
            "# that it starts out blank with new entries added after passing\n",
            "# a series of checks (file/dir exists, not already listed aka\n",
            "# de-deduplication).  During de-deduplication, wildcards and/or\n",
            "# directories are *NOT* expanded to keep it simple. Therefore,\n",
            "# if the computed classpath has two specific mentions of\n",
            "# awesome-methods-1.0.jar, only the first one added will be seen.\n",
            "# If two directories are in the classpath that both contain\n",
            "# awesome-methods-1.0.jar, then Java will pick up both versions.\n",
            "\n",
            "# An additional, custom CLASSPATH. Site-wide configs should be\n",
            "# handled via the shellprofile functionality, utilizing the\n",
            "# hadoop_add_classpath function for greater control and much\n",
            "# harder for apps/end-users to accidentally override.\n",
            "# Similarly, end users should utilize ${HOME}/.hadooprc .\n",
            "# This variable should ideally only be used as a short-cut,\n",
            "# interactive way for temporary additions on the command line.\n",
            "# export HADOOP_CLASSPATH=\"/some/cool/path/on/your/machine\"\n",
            "\n",
            "# Should HADOOP_CLASSPATH be first in the official CLASSPATH?\n",
            "# export HADOOP_USER_CLASSPATH_FIRST=\"yes\"\n",
            "\n",
            "# If HADOOP_USE_CLIENT_CLASSLOADER is set, the classpath along\n",
            "# with the main jar are handled by a separate isolated\n",
            "# client classloader when 'hadoop jar', 'yarn jar', or 'mapred job'\n",
            "# is utilized. If it is set, HADOOP_CLASSPATH and\n",
            "# HADOOP_USER_CLASSPATH_FIRST are ignored.\n",
            "# export HADOOP_USE_CLIENT_CLASSLOADER=true\n",
            "\n",
            "# HADOOP_CLIENT_CLASSLOADER_SYSTEM_CLASSES overrides the default definition of\n",
            "# system classes for the client classloader when HADOOP_USE_CLIENT_CLASSLOADER\n",
            "# is enabled. Names ending in '.' (period) are treated as package names, and\n",
            "# names starting with a '-' are treated as negative matches. For example,\n",
            "# export HADOOP_CLIENT_CLASSLOADER_SYSTEM_CLASSES=\"-org.apache.hadoop.UserClass,java.,javax.,org.apache.hadoop.\"\n",
            "\n",
            "# Enable optional, bundled Hadoop features\n",
            "# This is a comma delimited list.  It may NOT be overridden via .hadooprc\n",
            "# Entries may be added/removed as needed.\n",
            "# export HADOOP_OPTIONAL_TOOLS=\"hadoop-kafka,hadoop-aliyun,hadoop-azure,hadoop-azure-datalake,hadoop-aws\"\n",
            "\n",
            "###\n",
            "# Options for remote shell connectivity\n",
            "###\n",
            "\n",
            "# There are some optional components of hadoop that allow for\n",
            "# command and control of remote hosts.  For example,\n",
            "# start-dfs.sh will attempt to bring up all NNs, DNS, etc.\n",
            "\n",
            "# Options to pass to SSH when one of the \"log into a host and\n",
            "# start/stop daemons\" scripts is executed\n",
            "# export HADOOP_SSH_OPTS=\"-o BatchMode=yes -o StrictHostKeyChecking=no -o ConnectTimeout=10s\"\n",
            "\n",
            "# The built-in ssh handler will limit itself to 10 simultaneous connections.\n",
            "# For pdsh users, this sets the fanout size ( -f )\n",
            "# Change this to increase/decrease as necessary.\n",
            "# export HADOOP_SSH_PARALLEL=10\n",
            "\n",
            "# Filename which contains all of the hosts for any remote execution\n",
            "# helper scripts # such as workers.sh, start-dfs.sh, etc.\n",
            "# export HADOOP_WORKERS=\"${HADOOP_CONF_DIR}/workers\"\n",
            "\n",
            "###\n",
            "# Options for all daemons\n",
            "###\n",
            "#\n",
            "\n",
            "#\n",
            "# Many options may also be specified as Java properties.  It is\n",
            "# very common, and in many cases, desirable, to hard-set these\n",
            "# in daemon _OPTS variables.  Where applicable, the appropriate\n",
            "# Java property is also identified.  Note that many are re-used\n",
            "# or set differently in certain contexts (e.g., secure vs\n",
            "# non-secure)\n",
            "#\n",
            "\n",
            "# Where (primarily) daemon log files are stored.\n",
            "# ${HADOOP_HOME}/logs by default.\n",
            "# Java property: hadoop.log.dir\n",
            "# export HADOOP_LOG_DIR=${HADOOP_HOME}/logs\n",
            "\n",
            "# A string representing this instance of hadoop. $USER by default.\n",
            "# This is used in writing log and pid files, so keep that in mind!\n",
            "# Java property: hadoop.id.str\n",
            "# export HADOOP_IDENT_STRING=$USER\n",
            "\n",
            "# How many seconds to pause after stopping a daemon\n",
            "# export HADOOP_STOP_TIMEOUT=5\n",
            "\n",
            "# Where pid files are stored.  /tmp by default.\n",
            "# export HADOOP_PID_DIR=/tmp\n",
            "\n",
            "# Default log4j setting for interactive commands\n",
            "# Java property: hadoop.root.logger\n",
            "# export HADOOP_ROOT_LOGGER=INFO,console\n",
            "\n",
            "# Default log4j setting for daemons spawned explicitly by\n",
            "# --daemon option of hadoop, hdfs, mapred and yarn command.\n",
            "# Java property: hadoop.root.logger\n",
            "# export HADOOP_DAEMON_ROOT_LOGGER=INFO,RFA\n",
            "\n",
            "# Default log level and output location for security-related messages.\n",
            "# You will almost certainly want to change this on a per-daemon basis via\n",
            "# the Java property (i.e., -Dhadoop.security.logger=foo). (Note that the\n",
            "# defaults for the NN and 2NN override this by default.)\n",
            "# Java property: hadoop.security.logger\n",
            "# export HADOOP_SECURITY_LOGGER=INFO,NullAppender\n",
            "\n",
            "# Default process priority level\n",
            "# Note that sub-processes will also run at this level!\n",
            "# export HADOOP_NICENESS=0\n",
            "\n",
            "# Default name for the service level authorization file\n",
            "# Java property: hadoop.policy.file\n",
            "# export HADOOP_POLICYFILE=\"hadoop-policy.xml\"\n",
            "\n",
            "#\n",
            "# NOTE: this is not used by default!  <-----\n",
            "# You can define variables right here and then re-use them later on.\n",
            "# For example, it is common to use the same garbage collection settings\n",
            "# for all the daemons.  So one could define:\n",
            "#\n",
            "# export HADOOP_GC_SETTINGS=\"-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps\"\n",
            "#\n",
            "# .. and then use it as per the b option under the namenode.\n",
            "\n",
            "###\n",
            "# Secure/privileged execution\n",
            "###\n",
            "\n",
            "#\n",
            "# Out of the box, Hadoop uses jsvc from Apache Commons to launch daemons\n",
            "# on privileged ports.  This functionality can be replaced by providing\n",
            "# custom functions.  See hadoop-functions.sh for more information.\n",
            "#\n",
            "\n",
            "# The jsvc implementation to use. Jsvc is required to run secure datanodes\n",
            "# that bind to privileged ports to provide authentication of data transfer\n",
            "# protocol.  Jsvc is not required if SASL is configured for authentication of\n",
            "# data transfer protocol using non-privileged ports.\n",
            "# export JSVC_HOME=/usr/bin\n",
            "\n",
            "#\n",
            "# This directory contains pids for secure and privileged processes.\n",
            "#export HADOOP_SECURE_PID_DIR=${HADOOP_PID_DIR}\n",
            "\n",
            "#\n",
            "# This directory contains the logs for secure and privileged processes.\n",
            "# Java property: hadoop.log.dir\n",
            "# export HADOOP_SECURE_LOG=${HADOOP_LOG_DIR}\n",
            "\n",
            "#\n",
            "# When running a secure daemon, the default value of HADOOP_IDENT_STRING\n",
            "# ends up being a bit bogus.  Therefore, by default, the code will\n",
            "# replace HADOOP_IDENT_STRING with HADOOP_xx_SECURE_USER.  If one wants\n",
            "# to keep HADOOP_IDENT_STRING untouched, then uncomment this line.\n",
            "# export HADOOP_SECURE_IDENT_PRESERVE=\"true\"\n",
            "\n",
            "###\n",
            "# NameNode specific parameters\n",
            "###\n",
            "\n",
            "# Default log level and output location for file system related change\n",
            "# messages. For non-namenode daemons, the Java property must be set in\n",
            "# the appropriate _OPTS if one wants something other than INFO,NullAppender\n",
            "# Java property: hdfs.audit.logger\n",
            "# export HDFS_AUDIT_LOGGER=INFO,NullAppender\n",
            "\n",
            "# Specify the JVM options to be used when starting the NameNode.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# a) Set JMX options\n",
            "# export HDFS_NAMENODE_OPTS=\"-Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=1026\"\n",
            "#\n",
            "# b) Set garbage collection logs\n",
            "# export HDFS_NAMENODE_OPTS=\"${HADOOP_GC_SETTINGS} -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')\"\n",
            "#\n",
            "# c) ... or set them directly\n",
            "# export HDFS_NAMENODE_OPTS=\"-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')\"\n",
            "\n",
            "# this is the default:\n",
            "# export HDFS_NAMENODE_OPTS=\"-Dhadoop.security.logger=INFO,RFAS\"\n",
            "\n",
            "###\n",
            "# SecondaryNameNode specific parameters\n",
            "###\n",
            "# Specify the JVM options to be used when starting the SecondaryNameNode.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# This is the default:\n",
            "# export HDFS_SECONDARYNAMENODE_OPTS=\"-Dhadoop.security.logger=INFO,RFAS\"\n",
            "\n",
            "###\n",
            "# DataNode specific parameters\n",
            "###\n",
            "# Specify the JVM options to be used when starting the DataNode.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# This is the default:\n",
            "# export HDFS_DATANODE_OPTS=\"-Dhadoop.security.logger=ERROR,RFAS\"\n",
            "\n",
            "# On secure datanodes, user to run the datanode as after dropping privileges.\n",
            "# This **MUST** be uncommented to enable secure HDFS if using privileged ports\n",
            "# to provide authentication of data transfer protocol.  This **MUST NOT** be\n",
            "# defined if SASL is configured for authentication of data transfer protocol\n",
            "# using non-privileged ports.\n",
            "# This will replace the hadoop.id.str Java property in secure mode.\n",
            "# export HDFS_DATANODE_SECURE_USER=hdfs\n",
            "\n",
            "# Supplemental options for secure datanodes\n",
            "# By default, Hadoop uses jsvc which needs to know to launch a\n",
            "# server jvm.\n",
            "# export HDFS_DATANODE_SECURE_EXTRA_OPTS=\"-jvm server\"\n",
            "\n",
            "###\n",
            "# NFS3 Gateway specific parameters\n",
            "###\n",
            "# Specify the JVM options to be used when starting the NFS3 Gateway.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# export HDFS_NFS3_OPTS=\"\"\n",
            "\n",
            "# Specify the JVM options to be used when starting the Hadoop portmapper.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# export HDFS_PORTMAP_OPTS=\"-Xmx512m\"\n",
            "\n",
            "# Supplemental options for priviliged gateways\n",
            "# By default, Hadoop uses jsvc which needs to know to launch a\n",
            "# server jvm.\n",
            "# export HDFS_NFS3_SECURE_EXTRA_OPTS=\"-jvm server\"\n",
            "\n",
            "# On privileged gateways, user to run the gateway as after dropping privileges\n",
            "# This will replace the hadoop.id.str Java property in secure mode.\n",
            "# export HDFS_NFS3_SECURE_USER=nfsserver\n",
            "\n",
            "###\n",
            "# ZKFailoverController specific parameters\n",
            "###\n",
            "# Specify the JVM options to be used when starting the ZKFailoverController.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# export HDFS_ZKFC_OPTS=\"\"\n",
            "\n",
            "###\n",
            "# QuorumJournalNode specific parameters\n",
            "###\n",
            "# Specify the JVM options to be used when starting the QuorumJournalNode.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# export HDFS_JOURNALNODE_OPTS=\"\"\n",
            "\n",
            "###\n",
            "# HDFS Balancer specific parameters\n",
            "###\n",
            "# Specify the JVM options to be used when starting the HDFS Balancer.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# export HDFS_BALANCER_OPTS=\"\"\n",
            "\n",
            "###\n",
            "# HDFS Mover specific parameters\n",
            "###\n",
            "# Specify the JVM options to be used when starting the HDFS Mover.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# export HDFS_MOVER_OPTS=\"\"\n",
            "\n",
            "###\n",
            "# Router-based HDFS Federation specific parameters\n",
            "# Specify the JVM options to be used when starting the RBF Routers.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# export HDFS_DFSROUTER_OPTS=\"\"\n",
            "\n",
            "###\n",
            "# HDFS StorageContainerManager specific parameters\n",
            "###\n",
            "# Specify the JVM options to be used when starting the HDFS Storage Container Manager.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# export HDFS_STORAGECONTAINERMANAGER_OPTS=\"\"\n",
            "\n",
            "###\n",
            "# Advanced Users Only!\n",
            "###\n",
            "\n",
            "#\n",
            "# When building Hadoop, one can add the class paths to the commands\n",
            "# via this special env var:\n",
            "# export HADOOP_ENABLE_BUILD_PATHS=\"true\"\n",
            "\n",
            "#\n",
            "# To prevent accidents, shell commands be (superficially) locked\n",
            "# to only allow certain users to execute certain subcommands.\n",
            "# It uses the format of (command)_(subcommand)_USER.\n",
            "#\n",
            "# For example, to limit who can execute the namenode command,\n",
            "# export HDFS_NAMENODE_USER=hdfs\n",
            "\n",
            "\n",
            "###\n",
            "# Registry DNS specific parameters\n",
            "###\n",
            "# For privileged registry DNS, user to run as after dropping privileges\n",
            "# This will replace the hadoop.id.str Java property in secure mode.\n",
            "# export HADOOP_REGISTRYDNS_SECURE_USER=yarn\n",
            "\n",
            "# Supplemental options for privileged registry DNS\n",
            "# By default, Hadoop uses jsvc which needs to know to launch a\n",
            "# server jvm.\n",
            "# export HADOOP_REGISTRYDNS_SECURE_EXTRA_OPTS=\"-jvm server\"\n",
            "export JAVA_HOME=\"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
            "export HDFS_NAMENODE_USER=\"root\"\n",
            "export HDFS_DATANODE_USER=\"root\"\n",
            "export HDFS_SECONDARYNAMENODE_USER=\"root\"\n",
            "export YARN_RESOURCEMANAGER_USER=\"root\"\n",
            "export YARN_NODEMANAGER_USER=\"root\"\n",
            "\n",
            "---22. Generate SSH keys----\n",
            "\n",
            "/root\n",
            "Generating public/private rsa key pair.\n",
            "/root/.ssh/id_rsa already exists.\n",
            "Overwrite (y/n)? y\n",
            "Your identification has been saved in /root/.ssh/id_rsa\n",
            "Your public key has been saved in /root/.ssh/id_rsa.pub\n",
            "The key fingerprint is:\n",
            "SHA256:qcU32W31/e7nDmU1Zbp9sBXiN4XWTdBGCiA2netNvWU root@d89213ccffe7\n",
            "The key's randomart image is:\n",
            "+---[RSA 3072]----+\n",
            "|        +..o...OB|\n",
            "|       . oo ..++O|\n",
            "|           . ++=+|\n",
            "|       . ..o..oBE|\n",
            "|        S.+o. =+B|\n",
            "|       o ......oo|\n",
            "|      .       . .|\n",
            "|               o.|\n",
            "|               +*|\n",
            "+----[SHA256]-----+\n",
            "\n",
            "---23. Set Environment variables----\n",
            "\n",
            "\n",
            "---Time taken----\n",
            "\n",
            "0.9652640223503113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MW3SwbcLdGaY"
      },
      "source": [
        "### Format hadoop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhLO-fzv3EpE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3da09555-f523-403e-d57b-e20fadddc1d0"
      },
      "source": [
        "# 10.0 Format hadoop\n",
        "print(\"\\n---24. Format namenode----\\n\")\n",
        "!hdfs namenode  -format"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---24. Format namenode----\n",
            "\n",
            "WARNING: /opt/hadoop-3.3.5/logs does not exist. Creating.\n",
            "2023-03-27 05:59:11,305 INFO namenode.NameNode: STARTUP_MSG: \n",
            "/************************************************************\n",
            "STARTUP_MSG: Starting NameNode\n",
            "STARTUP_MSG:   host = d89213ccffe7/172.28.0.12\n",
            "STARTUP_MSG:   args = [-format]\n",
            "STARTUP_MSG:   version = 3.3.5\n",
            "STARTUP_MSG:   classpath = /opt/hadoop-3.3.5/etc/hadoop:/opt/hadoop-3.3.5/share/hadoop/common/lib/token-provider-1.0.1.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/jetty-webapp-9.4.48.v20220622.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/netty-codec-stomp-4.1.77.Final.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/curator-framework-4.2.0.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/jackson-databind-2.12.7.1.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/httpcore-4.4.13.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/netty-resolver-dns-4.1.77.Final.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/jackson-core-2.12.7.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/kerb-common-1.0.1.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/jsr305-3.0.2.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.77.Final-osx-aarch_64.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/asm-5.0.4.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/commons-configuration2-2.8.0.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/httpclient-4.5.13.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/json-smart-2.4.7.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/jetty-server-9.4.48.v20220622.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/gson-2.9.0.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/netty-codec-redis-4.1.77.Final.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/netty-handler-proxy-4.1.77.Final.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/kerb-core-1.0.1.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/commons-net-3.9.0.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/jsch-0.1.55.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/guava-27.0-jre.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/netty-transport-native-epoll-4.1.77.Final-linux-x86_64.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/failureaccess-1.0.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/netty-transport-native-epoll-4.1.77.Final-linux-aarch_64.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/dnsjava-2.1.7.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/netty-transport-native-unix-common-4.1.77.Final.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/netty-transport-rxtx-4.1.77.Final.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/jetty-xml-9.4.48.v20220622.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/jetty-http-9.4.48.v20220622.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/netty-codec-haproxy-4.1.77.Final.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/jetty-util-ajax-9.4.48.v20220622.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/reload4j-1.2.22.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/netty-transport-native-kqueue-4.1.77.Final-osx-aarch_64.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/jersey-server-1.19.4.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.77.Final-osx-x86_64.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/netty-transport-udt-4.1.77.Final.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/commons-beanutils-1.9.4.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/netty-transport-classes-kqueue-4.1.77.Final.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/commons-daemon-1.0.13.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/jersey-json-1.20.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/netty-codec-socks-4.1.77.Final.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/commons-text-1.10.0.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/curator-recipes-4.2.0.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/netty-codec-xml-4.1.77.Final.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/woodstox-core-5.4.0.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/curator-client-4.2.0.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/metrics-core-3.2.4.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/stax2-api-4.2.1.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/jersey-core-1.19.4.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/netty-codec-dns-4.1.77.Final.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/netty-codec-http-4.1.77.Final.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/kerb-util-1.0.1.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/commons-io-2.8.0.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/kerb-server-1.0.1.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/netty-codec-memcache-4.1.77.Final.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/commons-codec-1.15.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/netty-common-4.1.77.Final.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/checker-qual-2.5.2.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/jetty-servlet-9.4.48.v20220622.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/slf4j-api-1.7.36.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/jetty-util-9.4.48.v20220622.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/jetty-io-9.4.48.v20220622.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/kerb-client-1.0.1.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/commons-compress-1.21.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/netty-transport-native-kqueue-4.1.77.Final-osx-x86_64.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/kerby-config-1.0.1.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/netty-all-4.1.77.Final.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/jul-to-slf4j-1.7.36.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/jetty-security-9.4.48.v20220622.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/jackson-annotations-2.12.7.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/snappy-java-1.1.8.2.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/zookeeper-3.5.6.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/kerby-util-1.0.1.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/netty-transport-sctp-4.1.77.Final.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/netty-codec-smtp-4.1.77.Final.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/zookeeper-jute-3.5.6.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/netty-transport-classes-epoll-4.1.77.Final.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/netty-codec-mqtt-4.1.77.Final.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/re2j-1.1.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/avro-1.7.7.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/hadoop-auth-3.3.5.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/commons-lang3-3.12.0.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/netty-codec-http2-4.1.77.Final.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/jettison-1.5.3.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/netty-resolver-dns-classes-macos-4.1.77.Final.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/jersey-servlet-1.19.4.jar:/opt/hadoop-3.3.5/share/hadoop/common/lib/accessors-smart-2.4.7.jar:/opt/hadoop-3.3.5/share/hadoop/common/hadoop-common-3.3.5-tests.jar:/opt/hadoop-3.3.5/share/hadoop/common/hadoop-registry-3.3.5.jar:/opt/hadoop-3.3.5/share/hadoop/common/hadoop-common-3.3.5.jar:/opt/hadoop-3.3.5/share/hadoop/common/hadoop-nfs-3.3.5.jar:/opt/hadoop-3.3.5/share/hadoop/common/hadoop-kms-3.3.5.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/jetty-webapp-9.4.48.v20220622.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-codec-stomp-4.1.77.Final.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/curator-framework-4.2.0.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/jackson-databind-2.12.7.1.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/httpcore-4.4.13.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-resolver-dns-4.1.77.Final.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/jackson-core-2.12.7.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/jsr305-3.0.2.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.77.Final-osx-aarch_64.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/asm-5.0.4.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/commons-configuration2-2.8.0.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/httpclient-4.5.13.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/json-smart-2.4.7.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/jetty-server-9.4.48.v20220622.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/gson-2.9.0.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-codec-redis-4.1.77.Final.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-handler-proxy-4.1.77.Final.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/commons-net-3.9.0.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/jsch-0.1.55.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.77.Final-linux-x86_64.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.77.Final-linux-aarch_64.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-transport-native-unix-common-4.1.77.Final.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-transport-rxtx-4.1.77.Final.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/jetty-xml-9.4.48.v20220622.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/jetty-http-9.4.48.v20220622.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-codec-haproxy-4.1.77.Final.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/jetty-util-ajax-9.4.48.v20220622.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/reload4j-1.2.22.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.77.Final-osx-aarch_64.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/jersey-server-1.19.4.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.77.Final-osx-x86_64.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/paranamer-2.3.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-transport-udt-4.1.77.Final.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/commons-beanutils-1.9.4.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-3.10.6.Final.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-transport-classes-kqueue-4.1.77.Final.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/jersey-json-1.20.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-codec-socks-4.1.77.Final.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/commons-text-1.10.0.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/curator-recipes-4.2.0.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/kotlin-stdlib-1.4.10.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-codec-xml-4.1.77.Final.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/woodstox-core-5.4.0.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/curator-client-4.2.0.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/stax2-api-4.2.1.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/jersey-core-1.19.4.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-codec-dns-4.1.77.Final.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-codec-http-4.1.77.Final.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/commons-io-2.8.0.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-codec-memcache-4.1.77.Final.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/commons-codec-1.15.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-common-4.1.77.Final.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/jetty-servlet-9.4.48.v20220622.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/jetty-util-9.4.48.v20220622.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/jetty-io-9.4.48.v20220622.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/okio-2.8.0.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/commons-compress-1.21.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.77.Final-osx-x86_64.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-all-4.1.77.Final.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/jetty-security-9.4.48.v20220622.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/jackson-annotations-2.12.7.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/snappy-java-1.1.8.2.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/zookeeper-3.5.6.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-transport-sctp-4.1.77.Final.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-codec-smtp-4.1.77.Final.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/zookeeper-jute-3.5.6.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-transport-classes-epoll-4.1.77.Final.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-codec-mqtt-4.1.77.Final.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/re2j-1.1.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/avro-1.7.7.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/hadoop-auth-3.3.5.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/commons-lang3-3.12.0.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-codec-http2-4.1.77.Final.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/jettison-1.5.3.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-resolver-dns-classes-macos-4.1.77.Final.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/okhttp-4.9.3.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/jersey-servlet-1.19.4.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/lib/accessors-smart-2.4.7.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/hadoop-hdfs-client-3.3.5-tests.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/hadoop-hdfs-rbf-3.3.5.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/hadoop-hdfs-rbf-3.3.5-tests.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.3.5.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/hadoop-hdfs-native-client-3.3.5-tests.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/hadoop-hdfs-3.3.5.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/hadoop-hdfs-3.3.5-tests.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/hadoop-hdfs-native-client-3.3.5.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/hadoop-hdfs-nfs-3.3.5.jar:/opt/hadoop-3.3.5/share/hadoop/hdfs/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop-3.3.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.3.5.jar:/opt/hadoop-3.3.5/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.3.5.jar:/opt/hadoop-3.3.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.3.5.jar:/opt/hadoop-3.3.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.3.5.jar:/opt/hadoop-3.3.5/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.3.5.jar:/opt/hadoop-3.3.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.3.5.jar:/opt/hadoop-3.3.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.3.5.jar:/opt/hadoop-3.3.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.5.jar:/opt/hadoop-3.3.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.3.5.jar:/opt/hadoop-3.3.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.3.5-tests.jar:/opt/hadoop-3.3.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.3.5.jar:/opt/hadoop-3.3.5/share/hadoop/yarn:/opt/hadoop-3.3.5/share/hadoop/yarn/lib/asm-analysis-9.3.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.12.7.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/lib/javax-websocket-server-impl-9.4.48.v20220622.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/lib/asm-tree-9.3.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/lib/jetty-plus-9.4.48.v20220622.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/lib/jetty-client-9.4.48.v20220622.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/lib/jna-5.2.0.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/lib/websocket-server-9.4.48.v20220622.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.12.7.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/lib/jersey-guice-1.19.4.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/lib/websocket-client-9.4.48.v20220622.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/lib/guice-4.0.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/lib/asm-commons-9.3.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/lib/jline-3.9.0.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/lib/websocket-servlet-9.4.48.v20220622.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/lib/websocket-api-9.4.48.v20220622.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/lib/jackson-jaxrs-base-2.12.7.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/lib/jersey-client-1.19.4.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/lib/bcprov-jdk15on-1.68.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/lib/websocket-common-9.4.48.v20220622.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/lib/bcpkix-jdk15on-1.68.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/lib/objenesis-2.6.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/lib/javax.websocket-api-1.0.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/lib/javax.websocket-client-api-1.0.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/lib/javax-websocket-client-impl-9.4.48.v20220622.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/lib/snakeyaml-1.32.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/lib/jetty-annotations-9.4.48.v20220622.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/lib/jetty-jndi-9.4.48.v20220622.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/hadoop-yarn-services-api-3.3.5.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/hadoop-yarn-client-3.3.5.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.3.5.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.3.5.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/hadoop-yarn-applications-mawo-core-3.3.5.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/hadoop-yarn-server-common-3.3.5.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.3.5.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/hadoop-yarn-server-router-3.3.5.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/hadoop-yarn-services-core-3.3.5.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/hadoop-yarn-api-3.3.5.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/hadoop-yarn-server-tests-3.3.5.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.3.5.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/hadoop-yarn-registry-3.3.5.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.3.5.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/hadoop-yarn-common-3.3.5.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.3.5.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.3.5.jar:/opt/hadoop-3.3.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.3.5.jar\n",
            "STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r 706d88266abcee09ed78fbaa0ad5f74d818ab0e9; compiled by 'stevel' on 2023-03-15T15:56Z\n",
            "STARTUP_MSG:   java = 1.8.0_362\n",
            "************************************************************/\n",
            "2023-03-27 05:59:11,390 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\n",
            "2023-03-27 05:59:11,573 INFO namenode.NameNode: createNameNode [-format]\n",
            "2023-03-27 05:59:12,261 INFO namenode.NameNode: Formatting using clusterid: CID-ccaee854-dff1-4d12-960e-4a542d5d6d8f\n",
            "2023-03-27 05:59:12,325 INFO namenode.FSEditLog: Edit logging is async:true\n",
            "2023-03-27 05:59:12,362 INFO namenode.FSNamesystem: KeyProvider: null\n",
            "2023-03-27 05:59:12,365 INFO namenode.FSNamesystem: fsLock is fair: true\n",
            "2023-03-27 05:59:12,366 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\n",
            "2023-03-27 05:59:12,374 INFO namenode.FSNamesystem: fsOwner                = root (auth:SIMPLE)\n",
            "2023-03-27 05:59:12,374 INFO namenode.FSNamesystem: supergroup             = supergroup\n",
            "2023-03-27 05:59:12,374 INFO namenode.FSNamesystem: isPermissionEnabled    = true\n",
            "2023-03-27 05:59:12,374 INFO namenode.FSNamesystem: isStoragePolicyEnabled = true\n",
            "2023-03-27 05:59:12,374 INFO namenode.FSNamesystem: HA Enabled: false\n",
            "2023-03-27 05:59:12,431 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\n",
            "2023-03-27 05:59:12,598 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit : configured=1000, counted=60, effected=1000\n",
            "2023-03-27 05:59:12,598 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\n",
            "2023-03-27 05:59:12,604 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\n",
            "2023-03-27 05:59:12,604 INFO blockmanagement.BlockManager: The block deletion will start around 2023 Mar 27 05:59:12\n",
            "2023-03-27 05:59:12,606 INFO util.GSet: Computing capacity for map BlocksMap\n",
            "2023-03-27 05:59:12,606 INFO util.GSet: VM type       = 64-bit\n",
            "2023-03-27 05:59:12,608 INFO util.GSet: 2.0% max memory 2.8 GB = 57.8 MB\n",
            "2023-03-27 05:59:12,608 INFO util.GSet: capacity      = 2^23 = 8388608 entries\n",
            "2023-03-27 05:59:12,640 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled\n",
            "2023-03-27 05:59:12,640 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\n",
            "2023-03-27 05:59:12,662 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.999\n",
            "2023-03-27 05:59:12,662 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\n",
            "2023-03-27 05:59:12,662 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\n",
            "2023-03-27 05:59:12,663 INFO blockmanagement.BlockManager: defaultReplication         = 1\n",
            "2023-03-27 05:59:12,663 INFO blockmanagement.BlockManager: maxReplication             = 512\n",
            "2023-03-27 05:59:12,663 INFO blockmanagement.BlockManager: minReplication             = 1\n",
            "2023-03-27 05:59:12,663 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\n",
            "2023-03-27 05:59:12,663 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\n",
            "2023-03-27 05:59:12,663 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\n",
            "2023-03-27 05:59:12,663 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\n",
            "2023-03-27 05:59:12,714 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911\n",
            "2023-03-27 05:59:12,714 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215\n",
            "2023-03-27 05:59:12,714 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215\n",
            "2023-03-27 05:59:12,714 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215\n",
            "2023-03-27 05:59:12,740 INFO util.GSet: Computing capacity for map INodeMap\n",
            "2023-03-27 05:59:12,740 INFO util.GSet: VM type       = 64-bit\n",
            "2023-03-27 05:59:12,740 INFO util.GSet: 1.0% max memory 2.8 GB = 28.9 MB\n",
            "2023-03-27 05:59:12,740 INFO util.GSet: capacity      = 2^22 = 4194304 entries\n",
            "2023-03-27 05:59:12,744 INFO namenode.FSDirectory: ACLs enabled? true\n",
            "2023-03-27 05:59:12,744 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\n",
            "2023-03-27 05:59:12,744 INFO namenode.FSDirectory: XAttrs enabled? true\n",
            "2023-03-27 05:59:12,744 INFO namenode.NameNode: Caching file names occurring more than 10 times\n",
            "2023-03-27 05:59:12,752 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536\n",
            "2023-03-27 05:59:12,758 INFO snapshot.SnapshotManager: SkipList is disabled\n",
            "2023-03-27 05:59:12,764 INFO util.GSet: Computing capacity for map cachedBlocks\n",
            "2023-03-27 05:59:12,764 INFO util.GSet: VM type       = 64-bit\n",
            "2023-03-27 05:59:12,764 INFO util.GSet: 0.25% max memory 2.8 GB = 7.2 MB\n",
            "2023-03-27 05:59:12,764 INFO util.GSet: capacity      = 2^20 = 1048576 entries\n",
            "2023-03-27 05:59:12,782 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\n",
            "2023-03-27 05:59:12,782 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\n",
            "2023-03-27 05:59:12,782 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\n",
            "2023-03-27 05:59:12,790 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\n",
            "2023-03-27 05:59:12,791 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\n",
            "2023-03-27 05:59:12,793 INFO util.GSet: Computing capacity for map NameNodeRetryCache\n",
            "2023-03-27 05:59:12,793 INFO util.GSet: VM type       = 64-bit\n",
            "2023-03-27 05:59:12,793 INFO util.GSet: 0.029999999329447746% max memory 2.8 GB = 887.0 KB\n",
            "2023-03-27 05:59:12,793 INFO util.GSet: capacity      = 2^17 = 131072 entries\n",
            "2023-03-27 05:59:12,831 INFO namenode.FSImage: Allocated new BlockPoolId: BP-179146161-172.28.0.12-1679896752819\n",
            "2023-03-27 05:59:12,856 INFO common.Storage: Storage directory /app/hadoop/tmp/dfs/name has been successfully formatted.\n",
            "2023-03-27 05:59:13,011 INFO namenode.FSImageFormatProtobuf: Saving image file /app/hadoop/tmp/dfs/name/current/fsimage.ckpt_0000000000000000000 using no compression\n",
            "2023-03-27 05:59:13,148 INFO namenode.FSImageFormatProtobuf: Image file /app/hadoop/tmp/dfs/name/current/fsimage.ckpt_0000000000000000000 of size 399 bytes saved in 0 seconds .\n",
            "2023-03-27 05:59:13,167 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\n",
            "2023-03-27 05:59:13,214 INFO namenode.FSNamesystem: Stopping services started for active state\n",
            "2023-03-27 05:59:13,215 INFO namenode.FSNamesystem: Stopping services started for standby state\n",
            "2023-03-27 05:59:13,220 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.\n",
            "2023-03-27 05:59:13,220 INFO namenode.NameNode: SHUTDOWN_MSG: \n",
            "/************************************************************\n",
            "SHUTDOWN_MSG: Shutting down NameNode at d89213ccffe7/172.28.0.12\n",
            "************************************************************/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3iMrO3ldPsS"
      },
      "source": [
        "## Start and test hadoop\n",
        "If namenode is in safemode, use the command:   \n",
        "`!hdfs dfsadmin -safemode leave`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvfQ6c1KeqIh"
      },
      "source": [
        "#### Start hadoop\n",
        "If start fails with 'Connection refused', run `ssh_install()` once again"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlT89iZz_aeh",
        "outputId": "c3260f02-ed8c-4fda-c9a9-81c620e434c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 11.0 Start namenode\n",
        "#      If this fails, run\n",
        "#       ssh_install() below\n",
        "#        and start hadoop again:\n",
        "\n",
        "print(\"\\n---25. Start namenode----\\n\")\n",
        "! start-dfs.sh"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---25. Start namenode----\n",
            "\n",
            "Starting namenodes on [localhost]\n",
            "localhost: Warning: Permanently added 'localhost' (ECDSA) to the list of known hosts.\n",
            "Starting datanodes\n",
            "Starting secondary namenodes [d89213ccffe7]\n",
            "d89213ccffe7: Warning: Permanently added 'd89213ccffe7,172.28.0.12' (ECDSA) to the list of known hosts.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScVvdOPiuAaI"
      },
      "source": [
        "#ssh_install()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azR2g9vkes4K"
      },
      "source": [
        "#### Start yarn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8Qx_vc9PHZj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0426263c-8a4e-4973-e53b-32a53776228c"
      },
      "source": [
        "# 11.1 Start yarn\n",
        "! start-yarn.sh"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting resourcemanager\n",
            "Starting nodemanagers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UI-wzZD6ccBM"
      },
      "source": [
        "If `start-dfs.sh` fails, issue the following three commands, one after another:<br>  \n",
        "`! sudo apt-get remove openssh-client openssh-server`<br>\n",
        "`! sudo apt-get install openssh-client openssh-server`<br>\n",
        "`! service ssh restart`<br>\n",
        "\n",
        "And then try to start hadoop again, as: `start-dfs.sh`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJZLpxOYev7y"
      },
      "source": [
        "#### Test hadoop\n",
        "IF in safe mode, leave safe mode as:<br>\n",
        "`!hdfs dfsadmin -safemode leave`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMt89uLNCUVC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f167362-b654-4615-ec9c-9622f1ac37bd"
      },
      "source": [
        "# 11.1\n",
        "print(\"\\n---26. Make folders in hadoop----\\n\")\n",
        "! hdfs dfs -mkdir /user\n",
        "! hdfs dfs -mkdir /user/ashok"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---26. Make folders in hadoop----\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ez0FPw3CtOi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa43035c-085e-40cf-a101-d0a05bc6e0c1"
      },
      "source": [
        "# 11.2 Run hadoop commands\n",
        "! hdfs dfs -ls /\n",
        "! hdfs dfs -ls /user"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 items\n",
            "drwxr-xr-x   - root supergroup          0 2023-03-27 06:00 /user\n",
            "Found 1 items\n",
            "drwxr-xr-x   - root supergroup          0 2023-03-27 06:00 /user/ashok\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-fSbrEWH643"
      },
      "source": [
        "# 11.3 Stopping hadoop\n",
        "#      Gives some errors\n",
        "#      But hadoop stops\n",
        "#!stop-dfs.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwBD74dadYfz"
      },
      "source": [
        "Run the `ssh_install()` again if hadoop fails to start with `start-dfs.sh` and then try to start hadoop again."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBFVWWuafLBL"
      },
      "source": [
        "## Install spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyuLCFRJsvW3"
      },
      "source": [
        "### Define functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kk3BP0OfYpT1"
      },
      "source": [
        "`findspark`: PySpark isn't on `sys.path` by default, but that doesn't mean it can't be used as a regular library. You can address this by either symlinking pyspark into your site-packages, or adding `pyspark` to `sys.path` at runtime. `findspark` does the latter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opLGqtPRfM_5"
      },
      "source": [
        "# 1.0 Function to download and unzip spark\n",
        "def spark_koalas_install():\n",
        "  print(\"\\n--1.1 Install findspark----\\n\")\n",
        "  !pip install -q findspark\n",
        "\n",
        "  print(\"\\n--1.2 Install databricks Koalas----\\n\")\n",
        "  !pip install koalas\n",
        "\n",
        "  print(\"\\n--1.3 Download Apache tar.gz----\\n\")\n",
        "  ! wget -c https://mirrors.estointernet.in/apache/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "\n",
        "  print(\"\\n--1.4 Transfer downloaded content and unzip tar.gz----\\n\")\n",
        "  !  mv /content/spark*   /opt/\n",
        "  ! tar -xzf /opt/spark-3.1.1-bin-hadoop3.2.tgz  --directory /opt/\n",
        "\n",
        "  print(\"\\n--1.5 Check folder for files----\\n\")\n",
        "  ! ls -la /opt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebXfvQ1qiQHz"
      },
      "source": [
        "# 1.1 Function to set environment\n",
        "def set_spark_env():\n",
        "  print(\"\\n---2. Set Environment variables----\\n\")\n",
        "  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\" \n",
        "  os.environ[\"JRE_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64/jre\" \n",
        "  os.environ[\"SPARK_HOME\"] = \"/opt/spark-3.1.1-bin-hadoop3.2\"     \n",
        "  os.environ[\"LD_LIBRARY_PATH\"] += \":/opt/spark-3.1.1-bin-hadoop3.2/lib/native\"\n",
        "  os.environ[\"PATH\"] += \":/opt/spark-3.1.1-bin-hadoop3.2/bin:/opt/spark-3.1.1-bin-hadoop3.2/sbin\"\n",
        "  print(\"\\n---2.1. Check Environment variables----\\n\")\n",
        "  # Check\n",
        "  ! echo $PATH\n",
        "  ! echo $LD_LIBRARY_PATH"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUsZggDHj13U"
      },
      "source": [
        "# 1.2 Function to configure spark \n",
        "def spark_conf():\n",
        "  print(\"\\n---3. Configure spark to access hadoop----\\n\")\n",
        "  !mv /opt/spark-3.1.1-bin-hadoop3.2/conf/spark-env.sh.template  /opt/spark-3.1.1-bin-hadoop3.2/conf/spark-env.sh\n",
        "  !echo \"HADOOP_CONF_DIR=/opt/hadoop-3.3.5/etc/hadoop/\" >> /opt/spark-3.1.1-bin-hadoop3.2/conf/spark-env.sh\n",
        "  print(\"\\n---3.1 Check ----\\n\")\n",
        "  #!cat /opt/spark-3.1.1-bin-hadoop3.2/conf/spark-env.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7kLbAFLszN3"
      },
      "source": [
        "### Install spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_eaLhtPktHJ"
      },
      "source": [
        "# 2.0 Call all the three functions\n",
        "def install_spark():\n",
        "  spark_koalas_install()\n",
        "  set_spark_env()\n",
        "  spark_conf()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emaHs1XxRt5z",
        "outputId": "49cb774a-1472-4b97-9784-781d19ff38c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 2.1 \n",
        "install_spark()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "--1.1 Install findspark----\n",
            "\n",
            "\n",
            "--1.2 Install databricks Koalas----\n",
            "\n",
            "Collecting koalas\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/de/87c016a3e5055251ed117c86eb3b0de2381518c7acae54e115711ff30ceb/koalas-1.7.0-py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 5.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<1.20.0,>=1.14 in /usr/local/lib/python3.7/dist-packages (from koalas) (1.19.5)\n",
            "Requirement already satisfied: pyarrow>=0.10 in /usr/local/lib/python3.7/dist-packages (from koalas) (3.0.0)\n",
            "Requirement already satisfied: pandas<1.2.0,>=0.23.2 in /usr/local/lib/python3.7/dist-packages (from koalas) (1.1.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas<1.2.0,>=0.23.2->koalas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas<1.2.0,>=0.23.2->koalas) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas<1.2.0,>=0.23.2->koalas) (1.15.0)\n",
            "Installing collected packages: koalas\n",
            "Successfully installed koalas-1.7.0\n",
            "\n",
            "--1.3 Download Apache tar.gz----\n",
            "\n",
            "--2021-03-30 11:29:04--  https://mirrors.estointernet.in/apache/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
            "Resolving mirrors.estointernet.in (mirrors.estointernet.in)... 43.255.166.254, 2403:8940:3:1::f\n",
            "Connecting to mirrors.estointernet.in (mirrors.estointernet.in)|43.255.166.254|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228721937 (218M) [application/octet-stream]\n",
            "Saving to: ‘spark-3.1.1-bin-hadoop3.2.tgz’\n",
            "\n",
            "spark-3.1.1-bin-had 100%[===================>] 218.13M  11.9MB/s    in 22s     \n",
            "\n",
            "2021-03-30 11:29:27 (9.91 MB/s) - ‘spark-3.1.1-bin-hadoop3.2.tgz’ saved [228721937/228721937]\n",
            "\n",
            "\n",
            "--1.4 Transfer downloaded content and unzip tar.gz----\n",
            "\n",
            "\n",
            "--1.5 Check folder for files----\n",
            "\n",
            "total 609576\n",
            "drwxr-xr-x  1 root root      4096 Mar 30 11:29 .\n",
            "drwxr-xr-x  1 root root      4096 Mar 30 11:26 ..\n",
            "drwxr-xr-x  1 root root      4096 Mar 18 13:31 google\n",
            "drwxr-xr-x 10 1000 1000      4096 Mar 30 11:26 hadoop-3.2.2\n",
            "-rw-r--r--  1 root root 395448622 Jan 13 18:48 hadoop-3.2.2.tar.gz\n",
            "drwxr-xr-x  4 root root      4096 Mar 18 13:25 nvidia\n",
            "drwxr-xr-x 13 1000 1000      4096 Feb 22 02:11 spark-3.1.1-bin-hadoop3.2\n",
            "-rw-r--r--  1 root root 228721937 Feb 22 02:45 spark-3.1.1-bin-hadoop3.2.tgz\n",
            "\n",
            "---2. Set Environment variables----\n",
            "\n",
            "\n",
            "---2.1. Check Environment variables----\n",
            "\n",
            "/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin:/opt/bin:/opt/hadoop-3.2.2/bin:/opt/hadoop-3.2.2/sbin:/opt/spark-3.1.1-bin-hadoop3.2/bin:/opt/spark-3.1.1-bin-hadoop3.2/sbin\n",
            "/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/opt/hadoop-3.2.2/lib/native:/opt/spark-3.1.1-bin-hadoop3.2/lib/native\n",
            "\n",
            "---3. Configure spark to access hadoop----\n",
            "\n",
            "\n",
            "---3.1 Check ----\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6XYg1IGs17n"
      },
      "source": [
        "## Test spark\n",
        "Hadoop should have been started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZwLRxQWZryJ"
      },
      "source": [
        "Call some libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_1RXFGClfCy",
        "outputId": "db4670b6-53cf-4cda-e585-a1f200c000aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 3.0 Just call some libraries to test\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 3.1 Get spark in sys.path\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "# 3.2 Call other spark libraries\n",
        "#     Just to test\n",
        "from pyspark.sql import SparkSession\n",
        "import databricks.koalas as ks\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. Koalas will set it for you but it does not work if there is a Spark context already launched.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISNdxVmMmUVz"
      },
      "source": [
        "# 3.1 Build spark session\n",
        "spark = SparkSession. \\\n",
        "                    builder. \\\n",
        "                    master(\"local[*]\"). \\\n",
        "                    getOrCreate()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVlRiGQJmk58"
      },
      "source": [
        "# 4.0 Pandas DataFrame\n",
        "pdf = pd.DataFrame({\n",
        "        'x1': ['a','a','b','b', 'b', 'c', 'd','d'],\n",
        "        'x2': ['apple', 'orange', 'orange','orange', 'peach', 'peach','apple','orange'],\n",
        "        'x3': [1, 1, 2, 2, 2, 4, 1, 2],\n",
        "        'x4': [2.4, 2.5, 3.5, 1.4, 2.1,1.5, 3.0, 2.0],\n",
        "        'y1': [1, 0, 1, 0, 0, 1, 1, 0],\n",
        "        'y2': ['yes', 'no', 'no', 'yes', 'yes', 'yes', 'no', 'yes']\n",
        "    })\n",
        "\n",
        "# 4.1\n",
        "pdf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3pSQ7sDmnjt"
      },
      "source": [
        "# 4.2 Transform to Spark DataFrame\n",
        "df = spark.createDataFrame(pdf)\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzVBbgLkmsIR"
      },
      "source": [
        "# 4.3 Create a csv file \n",
        "#     and tranfer it to hdfs\n",
        "!echo \"a,b,c,d\"   > /content/airports.csv\n",
        "!echo \"5,4,6,7\"   >> /content/airports.csv\n",
        "!echo \"2,3,4,5\"   >> /content/airports.csv\n",
        "!echo \"8,9,0,1\"   >> /content/airports.csv\n",
        "!echo \"2,3,4,1\"   >> /content/airports.csv\n",
        "!echo \"1,2,2,1\"   >> /content/airports.csv\n",
        "!echo \"0,1,2,6\"   >> /content/airports.csv\n",
        "!echo \"9,3,1,8\"   >> /content/airports.csv\n",
        "!ls -la /content\n",
        "\n",
        "# 4.4\n",
        "!hdfs dfs -rm -f /user/ashok/airports.csv\n",
        "!hdfs dfs -put /content/airports.csv  /user/ashok/\n",
        "!hdfs dfs -ls /user/ashok"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fxfT0ysnmEK"
      },
      "source": [
        "# 5.0 Read file directly from hadoop\n",
        "airports_df = spark.read.csv( \n",
        "                              \"/user/ashok/airports.csv\",\n",
        "                              inferSchema = True,\n",
        "                              header = True\n",
        "                             )\n",
        "\n",
        "# 5.1 Show file\n",
        "airports_df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MihVRZhdxI4"
      },
      "source": [
        "## Test Koalas\n",
        "Hadoop should have been started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOGpE59BeHvk"
      },
      "source": [
        "Create a koalas dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPETd5DKtRem"
      },
      "source": [
        "# 6.0\n",
        "# If namenode is in safemode, first use:\n",
        "# hdfs dfsadmin -safemode leave\n",
        "kdf = ks.DataFrame(\n",
        "                   {\n",
        "                       'a': [1, 2, 3, 4, 5, 6],\n",
        "                       'b': [100, 200, 300, 400, 500, 600],\n",
        "                       'c': [\"one\", \"two\", \"three\", \"four\", \"five\", \"six\"]\n",
        "                    },\n",
        "                    index=[10, 20, 30, 40, 50, 60]\n",
        "                   )\n",
        "\n",
        "# 6.1 And show\n",
        "kdf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67nrVj9ctYwU"
      },
      "source": [
        "# 6.2 Pandas DataFrame\n",
        "pdf = pd.DataFrame({'x':range(3), 'y':['a','b','b'], 'z':['a','b','b']})\n",
        "\n",
        "# 6.2.1 Transform to koalas DataFrame\n",
        "df = ks.from_pandas(pdf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2U6CsdIftpEf"
      },
      "source": [
        "# 6.3 Rename koalas dataframe columns\n",
        "df.columns = ['x', 'y', 'z1']\n",
        "\n",
        "# 6.4 Do some operations on koalas DF, in place:\n",
        "df['x2'] = df.x * df.x\n",
        "\n",
        "# 6.6 Finally show koalas df\n",
        "df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eovb-i45tt4Z"
      },
      "source": [
        "# 6.7 Read csv file from hadoop\n",
        "#     and create koalas df\n",
        "ks.read_csv(\"/user/ashok/airports.csv\").head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_033UMyX7HJ"
      },
      "source": [
        "###################"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}